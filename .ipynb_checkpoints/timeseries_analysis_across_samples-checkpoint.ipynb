{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:27:53.745718Z",
     "start_time": "2019-07-03T16:27:53.738712Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import ast\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# To make the plot in the notebook and not in an extra window\n",
    "%matplotlib notebook \n",
    "\n",
    "# Implement error messages (Default should be True)\n",
    "error_on_missing_timestamps = False\n",
    "error_on_time_light_mismatch = False\n",
    "error_on_time_behavior_mismatch = False\n",
    "error_on_missing_behaviors = False\n",
    "error_on_invalid_behavior_range = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Generation of single data frame per sample, including behavior annotation, timestamp and dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:06:50.289153Z",
     "start_time": "2019-07-02T22:06:50.128929Z"
    }
   },
   "outputs": [],
   "source": [
    "#Open multiple .csv from single directory. Define existing behaviors. Define sample_ID and experiment_ID.\n",
    "# Chris's FRAN is zero based!!, whereas old annotations are 1 based\n",
    "#directory for behavior data\n",
    "\n",
    "behavior_directories = [r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv_cl_A4/',\n",
    "                        #r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv_cl_A9/',\n",
    "                        #r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv_ol/',\n",
    "                        #r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv_stim_artefact/'\n",
    "                       ] \n",
    "\n",
    "behavior_files = []\n",
    "for d in behavior_directories:\n",
    "    behavior_files.extend(\n",
    "        glob.glob(os.path.join(d, \"*.csv\"))) #join pathname with filename, \n",
    "\n",
    "# Behavior columns available in CSV files\n",
    "available_behaviors = ('fw', 'bw', 'stim', 'hunch', 'turn', 'other', 'HP', 'left turn', 'right turn')\n",
    "\n",
    "# Regular expression (define the expression filenames are searched for)\n",
    "# '.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "# '+' 1>> occurences, 'L' here character from filename\n",
    "# () outcome here: 2 groups, useful for extraction\n",
    "# [] optional list, eg 1 --> 1\n",
    "# ? character non or once \n",
    "\n",
    "# Behavior reg-ex (regular expression)\n",
    "behavior_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-behavior-(.+).csv')\n",
    "\n",
    "# Function: readall_behavior iterates through all csv (sorted) \n",
    "# and appends the files into the list (ls) and returns dictionary\n",
    "def readall_behavior(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample \n",
    "        # (cl (closeloop, RGECO/ Chronos), ol (openloop, RGECO/ Chronos), \n",
    "        # blocks (Raghav: GCaMP/Chrimson))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        # Look for filename_components, which are true for pattern\n",
    "        match = behavior_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        #define filename_components sample_id (first group), and exp_id (sec group)\n",
    "        part_sample_id, _, exp_id = filename_components         \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ';')\n",
    "        df.fillna(0, inplace=True) #replace NaN with zero\n",
    "        df['sample_id'] = sample_id  #add sample_id column\n",
    "        df['exp_id'] = exp_id #add exp_id column\n",
    "        data[sample_id] = df\n",
    "        #Count 'True' for each column ('behavior') in each single behavior.csv)\n",
    "        #print(filename, df[df == 1].count()) \n",
    "        #print(df)\n",
    "    return data\n",
    "\n",
    "behavior_data = readall_behavior(behavior_files)\n",
    "#print(behavior_data['17-11-06L2-cl'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:06:50.428007Z",
     "start_time": "2019-07-02T22:06:50.290749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START             8\n",
      "END               0\n",
      "fw             3644\n",
      "bw              711\n",
      "stim            201\n",
      "hunch           326\n",
      "turn           1508\n",
      "other           136\n",
      "HP              496\n",
      "left turn       769\n",
      "right turn      739\n",
      "sample_id         0\n",
      "exp_id            0\n",
      "Unnamed: 11       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency of each behavior in all imported behavior.csv by using the returned 'ls' from \n",
    "# the function readAll: concatenate the 'behavior_files' (global variable). 'True' for each \n",
    "# column ('behavior_type') in the concatenated file (df_behavior).\n",
    "# Sorting has to be = False (warning message without 'sort')\n",
    "df_behavior = pd.concat(behavior_data.values(), axis = 0, ignore_index = True, sort = False) #add sorting\n",
    "print(df_behavior[df_behavior == 1].count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:06:54.396557Z",
     "start_time": "2019-07-02T22:06:50.430251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import and merge fluorescence data: Several LM files for the same sample_id exists, but differ in cell_id).\n",
    "# List of LM data with two extra columns: sample_id and cell_id\n",
    "# Open LM files from different directories\n",
    "lightmicroscope_directories = [r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Basin_traces/', \n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Handle-like_Traces',\n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/a00c_traces',\n",
    "                               #r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/candidate_neuron_traces'\n",
    "                              ] \n",
    "\n",
    "# Iterate through LM data and extend files in a list from within and between directory and \n",
    "# build a list of files from all directories\n",
    "# (Note: append would 'extend' lists and not single files)\n",
    "lightmicroscope_files = []\n",
    "for d in lightmicroscope_directories:\n",
    "    lightmicroscope_files.extend(\n",
    "        glob.glob(os.path.join(d, \"*.csv\"))) #join pathname with filename\n",
    "\n",
    "# Lightmicroscopic data reg-ex (regular expression)\n",
    "lightmicroscope_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-(.*)-(.*).csv')\n",
    "\n",
    "# Function: readall_lm iterates through all LM_csv (sorted) \n",
    "# and returns a dictionary{key:value} \n",
    "# samples = {sample_id:cell-id}\n",
    "def readall_lm(all_files):\n",
    "    samples = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # Larva ID: YY-MM-DDLX, DETAIL = cell_id\n",
    "        # Look for filename_components, which are true for pattern\n",
    "        match = lightmicroscope_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, cell_id, exp_id = filename_components\n",
    "        \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        # Read LM.files \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ',')\n",
    "        # Replace NaN with zero\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cellname to each column as prefix\n",
    "        # lambda is a non defined function (longer version: def lambda(x):)\n",
    "        # Rename of columns after the format cell_id, name) eg: Basin A9\n",
    "        # inplace = True: column names are overwritten (if False: new dataframe)\n",
    "        df.rename(lambda x: '{}_{}'.format(cell_id, x), axis = 'columns', inplace = True)\n",
    "        # Get the sample_id (key) from the dictionary? to make a list [sample_cells] and \n",
    "        # if sample_id exists, append the list\n",
    "        # if sample_id does not exists, start a new list\n",
    "        # reminder: there can be several cell_id per sample_id\n",
    "        sample_cells = samples.get(sample_id)\n",
    "        if not sample_cells:\n",
    "            samples[sample_id] = sample_cells = {\n",
    "                'data': [],\n",
    "                'exp_id': exp_id,\n",
    "            }\n",
    "        sample_cells['data'].append(df)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "lm_samples = readall_lm(lightmicroscope_files)\n",
    "\n",
    "# New dictionary: lm_data{} to build a single dataframe with all cell_ids combined \n",
    "# for a single sample. Iterate over dict from same sample in one dataframe. \n",
    "# df.items iterate over pairs and build a list\n",
    "\n",
    "lm_data = {}\n",
    "\n",
    "# Iterate over all light samples and merge all found files\n",
    "# for each sample into a single data frame (per sample)\n",
    "for sample_id, sample_info in lm_samples.items():\n",
    "    cells_dataframes = sample_info['data']\n",
    "    #check if number of cells >= 1\n",
    "    if not cells_dataframes:\n",
    "        raise ValueError('No cells found for sample {}'.format(sample_id))\n",
    "    #first element in the list\n",
    "    lm_df = None\n",
    "\n",
    "    #iteration through other df\n",
    "    for cdf in cells_dataframes:\n",
    "        if lm_df is None:\n",
    "            lm_df = cdf\n",
    "        else:\n",
    "            if len(lm_df.index) != len(cdf.index):\n",
    "                raise ValueError('Data frame frame to merge has not same row count as target', sample_id)\n",
    "            lm_df = pd.merge(lm_df, cdf, left_index = True, right_index = True)\n",
    "            \n",
    "    lm_df['sample_id'] = sample_id  #add sample_id column\n",
    "    lm_df['exp_id'] = sample_info['exp_id']\n",
    "    lm_data[sample_id] = lm_df\n",
    "#print(list(lm_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:06:54.406419Z",
     "start_time": "2019-07-02T22:06:54.398701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import txt-files from of the absolute time/frame from the Ca-imaging (lm-data). \n",
    "# All txt-files have to be transposed, which is a memory intensive step. After the \n",
    "# data are complete, the transposed files should be exported (ToDo). Time-data are \n",
    "# combined with sample-ID and experiment-ID.\n",
    "\n",
    "timelapse_directory =(r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/timelapse/') \n",
    "timelapse_files = glob.glob(os.path.join(timelapse_directory, \"*.txt\")) #join pathname with filename\n",
    "\n",
    "# Behavior reg-ex (regular expression)\n",
    "time_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-time-(.+).txt')\n",
    "\n",
    "# Function: readall_timelapse iterates through all txt (sorted) and appends the \n",
    "# files into the dict (data) and returns ls\n",
    "def readall_time(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample (cl (closeloop), ol (openloop), blocks (Raghav))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        #look for filename_components, which are true for pattern\n",
    "        match = time_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, exp_id = filename_components #define filename_components sample_id (first group), and exp_id (sec group)  \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, header=1, index_col=None, delim_whitespace = True)\n",
    "        df = df.T #transposing because read_csv imports as row\n",
    "        df = df.reset_index() #transpose function sets data as index\n",
    "        df.rename(columns={'index':'time'}, inplace=True) #rename reset index column to time\n",
    "        df['time'] = df.time.astype(float)\n",
    "        data[sample_id] = df\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:12.815449Z",
     "start_time": "2019-07-02T22:06:54.407598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache file found, recomputing\n"
     ]
    }
   ],
   "source": [
    "# Keep in mind that some of the files has to be changed because of the discrepancy in timestamps\n",
    "timelapse_cache = 'timelapse.cache'\n",
    "\n",
    "try:\n",
    "    with open(timelapse_cache, 'r') as timelapse_cache_file:\n",
    "        # TODO\n",
    "        cache_data = timelapse_cache_file.read()\n",
    "        time_data = ast.literal_eval(cache_data)\n",
    "except FileNotFoundError as e:\n",
    "    print('No cache file found, recomputing')\n",
    "    # No cache file found, recompute\n",
    "    time_data = readall_time(timelapse_files)\n",
    "    # Write cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:20.471542Z",
     "start_time": "2019-07-02T22:09:12.816881Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17-08-24L5-cl: time data (6686 entries) doesn't match light data (6685 entries)\n",
      "17-08-26L1-cl: time data (6990 entries) doesn't match light data (6989 entries)\n",
      "17-08-26L3-cl: time data (2935 entries) doesn't match light data (4110 entries)\n",
      "17-08-27L2-cl: time data (6470 entries) doesn't match light data (6469 entries)\n",
      "17-08-28L3-cl: time data (6228 entries) doesn't match light data (6225 entries)\n",
      "17-08-29L2-cl: time data (6817 entries) doesn't match light data (6805 entries)\n",
      "17-11-03L7-cl: time data (1399 entries) doesn't match light data (2657 entries)\n",
      "17-11-04L1-cl: time data (6325 entries) doesn't match light data (6324 entries)\n",
      "17-11-06L1-cl: time data (1923 entries) doesn't match light data (6493 entries)\n",
      "17-11-08L3-cl: time data (3240 entries) doesn't match light data (6474 entries)\n",
      "17-11-26L1-cl: time data (6487 entries) doesn't match light data (6469 entries)\n",
      "17-11-29L3-cl: time data (6567 entries) doesn't match light data (6561 entries)\n",
      "17-11-30L2-cl: time data (6640 entries) doesn't match light data (6396 entries)\n",
      "Matched 77 light data sets with their respective time points\n",
      "max-diff 0.38299999999998136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_data = {}\n",
    "\n",
    "# Time data are merged into light data and checked if number length of lm = timestamp.  \n",
    "# Due to technical conditions, some time.txt-file have too many or not enough time data compared\n",
    "# to the corresponding LM data. The discrepancy is fixed by either dropping the extra timepoints or \n",
    "# by taking the average of the difference between each timepoint and extend the dataframe. \n",
    "# The first 10 timepoints are not included to account for instability of the microscope in \n",
    "# the beginning due to the moving parts. \n",
    "# Maximal difference between timepoints fyi.\n",
    "\n",
    "for sample_id, sample_df in lm_data.items():\n",
    "    # Add time stamps to data frame of current sample by merging\n",
    "    # The time data frame for the current sample, which is expected\n",
    "    # to match the light data (based on index).\n",
    "    timestamp_df = time_data.get(sample_id)\n",
    "    if timestamp_df is None:\n",
    "        msg = '{}: could not find timestamp data for sample'.format(sample_id)\n",
    "        if error_on_missing_timestamps:\n",
    "            raise ValueError(msg)\n",
    "        # Ignore, if missing data shouldn't cancel the whole process.\n",
    "        print(msg)\n",
    "        continue\n",
    "        \n",
    "    n_timestamps = len(timestamp_df)\n",
    "    n_lightdata = len(sample_df)\n",
    "    \n",
    "    # The timestamp and light recordings are done by different systems.\n",
    "    # This can cause the existence of additional time points/ or missing time points in a\n",
    "    # dataset, which will be filtered out in the merge operation below.\n",
    "    if n_lightdata != n_timestamps:\n",
    "        msg = '{}: time data ({} entries) doesn\\'t match light data ({} entries)'.format(\n",
    "                sample_id, n_timestamps, n_lightdata)\n",
    "        if error_on_time_light_mismatch:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        diffs = np.diff(timestamp_df['time'])[10:] #from 10th row onwards\n",
    "        diffs_avg = diffs.mean(axis=0)\n",
    "        #diff between timedata and lightdata\n",
    "        missing_data = len(sample_df) - len(timestamp_df)\n",
    "        \n",
    "        #add 'diffs_avg' to fill in missing_timedata\n",
    "        if missing_data > 0:\n",
    "            last_valid_index = len(timestamp_df) - 1\n",
    "            last_timestamp = timestamp_df.iloc[last_valid_index]['time']\n",
    "            if pd.isna(last_timestamp):\n",
    "                raise ValueError('Unexpected last valid timestamp for sample {} at index {}'.format(\n",
    "                        sample_id, last_valid_index))\n",
    "            for i in range(0, missing_data):\n",
    "                last_valid_index += 1\n",
    "                timestamp_df.loc[last_valid_index] = timestamp_df.iloc[last_valid_index - 1]['time'] + diffs_avg\n",
    "        elif missing_data < 0:\n",
    "            drop_start = len(timestamp_df) + missing_data\n",
    "            drop_end = len(timestamp_df)\n",
    "            timestamp_df.drop(list(range(drop_start, drop_end)))\n",
    "\n",
    "    # Merge timedata into light data\n",
    "    # Use an 'inner' join/merge to exclude time points that don't have matching light data.\n",
    "    new_sample_df = pd.merge(sample_df, timestamp_df, left_index = True, right_index = True, how='inner')\n",
    "    \n",
    "    # Store newly created data frame for sample (dictionary)\n",
    "    sample_data[sample_id] = new_sample_df\n",
    "    \n",
    "print('Matched {} light data sets with their respective time points'.format(len(sample_data)))\n",
    "\n",
    "# Max.diffs for timestamps\n",
    "# diffs defined earlier\n",
    "mx = diffs.max()\n",
    "print('max-diff', mx)\n",
    "\n",
    "plt.hist(diffs, bins=10, alpha=0.5)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:23.238054Z",
     "start_time": "2019-07-02T22:09:20.473749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find behavior data for sample \"17-11-02L3-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L1-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L2-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L3-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L5-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L6-1-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L6-2-cl\"\n",
      "Could not find behavior data for sample \"17-11-03L7-cl\"\n",
      "Could not find behavior data for sample \"17-11-04L1-cl\"\n",
      "Could not find behavior data for sample \"17-11-04L2-cl\"\n",
      "Could not find behavior data for sample \"17-11-04L3-cl\"\n",
      "Could not find behavior data for sample \"17-11-04L4-cl\"\n",
      "Could not find behavior data for sample \"17-11-05L6-cl\"\n",
      "Could not find behavior data for sample \"17-11-05L7-cl\"\n",
      "Could not find behavior data for sample \"17-11-06L1-cl\"\n",
      "Could not find behavior data for sample \"17-11-06L2-cl\"\n",
      "Could not find behavior data for sample \"17-11-06L3-cl\"\n",
      "Could not find behavior data for sample \"17-11-07L2-cl\"\n",
      "Could not find behavior data for sample \"17-11-07L3-cl\"\n",
      "Could not find behavior data for sample \"17-11-07L4-cl\"\n",
      "Could not find behavior data for sample \"17-11-07L5-cl\"\n",
      "Could not find behavior data for sample \"17-11-08L1-cl\"\n",
      "Could not find behavior data for sample \"17-11-08L2-cl\"\n",
      "Could not find behavior data for sample \"17-11-08L3-cl\"\n"
     ]
    }
   ],
   "source": [
    "# Combine behavior data with light data into a single data frame\n",
    "# per sample ID. To do so, add behavior data to light data frames,\n",
    "# because the light data is already organizes by frame. To accomodate\n",
    "# frame ranges without an behavior data, a column named \"quiet\" is\n",
    "# added which is True in these cases and False otherwise. Additionally,\n",
    "# for each behavior column, a behavior start and end column as well as\n",
    "# an overlap column is added so that parallel and successive behaviors\n",
    "# of the same type can be differentiated.\n",
    "\n",
    "for sample_id, sample_df in sample_data.items():\n",
    "    sample_behavior = behavior_data.get(sample_id)\n",
    "    if sample_behavior is None:\n",
    "        msg = 'Could not find behavior data for sample \"{}\"'.format(sample_id)\n",
    "        if error_on_missing_behaviors:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        continue\n",
    "\n",
    "    # Add extra columns for behavior\n",
    "    for behavior in available_behaviors:\n",
    "        sample_df[behavior] = False\n",
    "        sample_df['{}_start'.format(behavior)] = False\n",
    "        sample_df['{}_end'.format(behavior)] = False\n",
    "        sample_df['{}_overlap'.format(behavior)] = False\n",
    "    \n",
    "    # Add 'quiet' column. Set it initially to True and mark frames\n",
    "    # with actual behavior as quiet = False.\n",
    "    sample_df['quiet'] = True\n",
    "    \n",
    "    n_light_entries = len(sample_df)\n",
    "\n",
    "    # Iterate over behavior data and add data to target data frame\n",
    "    for i, row in sample_behavior.iterrows():\n",
    "        # Start and end are 1-based, make them 0-based\n",
    "        start = int(row['START'])\n",
    "        end = int(row['END'])\n",
    "        \n",
    "        if type(row['START']) == str:\n",
    "            print(sample_id)\n",
    "            print(start, end)\n",
    "        \n",
    "        if start >= end:\n",
    "            msg = \"{}: start ({}) needs to be strictly smaller than end ({})\".format(sample_id, start, end)\n",
    "            if error_on_invalid_behavior_range:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "        \n",
    "        # Make sure we capture start/end times that are a fractional number.\n",
    "        if row['START'] - start > 0 or row['END'] - end > 0:\n",
    "            raise ValueError('{}: start and end frame number can\\'t contain fractions'.format(sample_id))\n",
    "            \n",
    "        # Ignore behavior entries with an end frame higher than available light data.\n",
    "        # The behavior data is one-based, which is why a strict larger than test should\n",
    "        # be correct.\n",
    "        if end > n_light_entries:\n",
    "            msg = 'Sample: {} - Behavior row with range {}-{} exceeds light time points ({}): {}'.format(\n",
    "                sample_id, start, end, n_light_entries, row)\n",
    "            if error_on_time_behavior_mismatch:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "            \n",
    "        # Find behavior observed in row\n",
    "        observed_behaviors = []\n",
    "        for behavior in available_behaviors:\n",
    "            if row[behavior]:\n",
    "                observed_behaviors.append(behavior)\n",
    "        \n",
    "        # We assume that not more than two behaviors are observed at the same time\n",
    "        if len(observed_behaviors) > 2:\n",
    "            raise ValueError('Found multiple behaviors in row {} of sample {}'.format(i, sample_id))\n",
    "        \n",
    "        # Add observed behavior information to target data frames in all\n",
    "        # rows in behavior range.\n",
    "        for b in observed_behaviors:\n",
    "            # Iterate over frames valid for current behavior. Every valid\n",
    "            # frame is mapped into the canonical (light/cell) data frame,\n",
    "            # which is 0-indexed.\n",
    "            for j in range(start, end + 1):\n",
    "                # Behavior ranges are 1-indexed\n",
    "                current_frame = j - 1\n",
    "                # If the current behavior has already been observed at this frame,\n",
    "                # set overlap to True, because we are about to mark this behavior\n",
    "                # again as observed for this frame.\n",
    "                if sample_df.at[current_frame, b]:\n",
    "                    sample_df.at[current_frame, '{}_overlap'.format(b)] = True\n",
    "                else:\n",
    "                    sample_df.at[current_frame, b] = True\n",
    "                \n",
    "                # Mark this row as not quiet, because we observed\n",
    "                # a behavior in the current frame.\n",
    "                sample_df.at[current_frame, 'quiet'] = False\n",
    "\n",
    "            sample_df.at[start - 1, '{}_start'.format(b)] = True\n",
    "            sample_df.at[end - 1, '{}_end'.format(b)] = True\n",
    "            \n",
    "    # Mark quiet ranges with _start, _end and _overlap. By definion,\n",
    "    # quiet_overlap is always False.\n",
    "    sample_df['quiet_start'] = False\n",
    "    sample_df['quiet_end'] = False\n",
    "    sample_df['quiet_overlap'] = False\n",
    "    last_sample_idx = n_light_entries - 1\n",
    "    for i, row in sample_df.iterrows():\n",
    "        sample_df.at[i, 'quiet_start'] = row['quiet'] and (i == 0 or not sample_df.at[i - 1, 'quiet'])\n",
    "        sample_df.at[i, 'quiet_end'] = row['quiet'] and (i == last_sample_idx or not sample_df.at[i + 1, 'quiet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Data-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:23.256574Z",
     "start_time": "2019-07-02T22:09:23.239422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a class with sample_id, cell_type, event_name and filter_pattern\n",
    "\n",
    "class CellTraceConfig:\n",
    "    \n",
    "    def __init__(self, sample_id, cell_type, event_name, filter_pattern=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell_type = cell_type\n",
    "        self.event_name = event_name\n",
    "        self.filter_pattern = filter_pattern\n",
    "        \n",
    "    def get_filter_regex(self):\n",
    "        filter_regex = '^{}_'.format(self.cell_type)\n",
    "        if self.filter_pattern:\n",
    "            filter_regex += '.*{}.*'.format(self.filter_pattern)\n",
    "        return filter_regex\n",
    "    \n",
    "    def get_event_start_col(self):\n",
    "        return '{}_start'.format(self.event_name)\n",
    "\n",
    "    def add_event_time_points_to_plot(self, source_df, plot):\n",
    "        for idx, row in source_df.iterrows():\n",
    "            plot.annotate(self.event_name, xy=(row['time'], 1))\n",
    "            plt.axvline(row['time'], color='k', linestyle='-')  \n",
    "            \n",
    "# Define a class with sample_id, cell_type, event_time and filter_pattern (for behavioral_transitions)\n",
    "# Put '' [empty string] if you dont want any cell type\n",
    "\n",
    "class CellTransConfig:\n",
    "    \n",
    "    def __init__(self, sample_id, cell_type, event_time, filter_pattern=None, first_event=None, second_event=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell_type = cell_type\n",
    "        self.event_time = event_time\n",
    "        self.filter_pattern = filter_pattern\n",
    "        self.first_event = first_event\n",
    "        self.second_event = second_event\n",
    "        \n",
    "    def get_filter_regex(self):\n",
    "        if self.cell_type is None:\n",
    "            cell_str = r\"[a-zA-Z0-9]+\"\n",
    "        else:\n",
    "            cell_str = self.cell_type\n",
    "            \n",
    "        filter_regex = '^{}_'.format(cell_str)\n",
    "        if self.filter_pattern:\n",
    "            filter_regex += '.*{}.*'.format(self.filter_pattern)\n",
    "        return filter_regex\n",
    "    \n",
    "    \n",
    "    \n",
    "# Define a class for filtering after behavioral_transitions for either only cell_type or filter_pattern or both.\n",
    "# For example to average not only over all A00cs but all A00c_midL.\n",
    "\n",
    "class DataFilter():\n",
    "    def __init__(self, cell=None, pattern=None):\n",
    "        self.cell = cell if cell is not None else '.*' # Makes argument optional\n",
    "        self.pattern = pattern if pattern is not None else '.*' # Makes argument optional\n",
    "        \n",
    "    def get_cell_filter_regex(self):\n",
    "        filter_regex = '.*_{}_.*_.*'.format(self.cell)\n",
    "        return filter_regex\n",
    "    \n",
    "    def get_pattern_filter_regex(self):\n",
    "        filter_regex = '.*_.*_{}_.*'.format(self.pattern)\n",
    "        return filter_regex\n",
    "    \n",
    "    def get_cellpattern_filter_regex(self):\n",
    "        filter_regex = '.*_{}_{}_.*'.format(self.cell, self.pattern)\n",
    "        return filter_regex\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{}_{}\".format(self.cell, self.pattern)   \n",
    "    \n",
    "    \n",
    "# Define class to group the columns after cell_type/ pattern or both using the class Datafilter\n",
    "\n",
    "class TransitionGrouper:\n",
    "    def __init__(self, transitions_df):\n",
    "        self.transitions_df = transitions_df\n",
    "        \n",
    "        sample_ids, cells, patterns, *_ = zip(*[column.split(\"_\") for column in self.transitions_df.columns])\n",
    "        \n",
    "        self.sample_ids = sorted(set(sample_ids))\n",
    "        self.cells = sorted(set(cells))\n",
    "        self.patterns = sorted(set(patterns))\n",
    "        \n",
    "    def get_regex(self, cell_name=None, pattern=None):\n",
    "        data_filter = DataFilter(cell=cell_name, pattern=pattern)\n",
    "        if cell_name is not None and pattern is None:\n",
    "            return data_filter, data_filter.get_cell_filter_regex()\n",
    "        if cell_name is not None and pattern is not None:\n",
    "            return data_filter, data_filter.get_cellpattern_filter_regex()\n",
    "        if cell_name is None and pattern is not None:\n",
    "            return data_filter, data_filter.get_pattern_filter_regex()\n",
    "        raise ValueError(\"Both cell_name and pattern are None! :(\")\n",
    "    \n",
    "    def group_cells(self):\n",
    "        output = dict()\n",
    "        for cell_name in self.cells:\n",
    "            data_filter, regex = self.get_regex(cell_name)\n",
    "            cell_df = self.transitions_df.filter(regex=regex)\n",
    "            output[cell_name] = (str(data_filter), cell_df)\n",
    "        return output\n",
    "    \n",
    "    def group_patterns(self):\n",
    "        output = dict()\n",
    "        for pattern in self.patterns:\n",
    "            data_filter, regex = self.get_regex(pattern=pattern)\n",
    "            pattern_df = self.transitions_df.filter(regex=regex)\n",
    "            output[pattern] = (str(data_filter), pattern_df)\n",
    "        return output\n",
    "    \n",
    "    def group_cellpattern(self):\n",
    "        output = dict()\n",
    "        for cell_name, pattern in itertools.product(self.cells, self.patterns):\n",
    "            data_filter, regex = self.get_regex(cell_name, pattern)\n",
    "            cellpattern_df = self.transitions_df.filter(regex=regex)\n",
    "            output[(cell_name, pattern)] = (str(data_filter), cellpattern_df)\n",
    "        return output\n",
    "\n",
    "# Specific after Post-transitions for multiple transition kinds, used for plotting. For multiple transition \n",
    "# events, group after transition (first, or second event) <most useful> with option to group\n",
    "# after celltype, filterpattern, sample_id, observations.\n",
    "class TransitionType:\n",
    "    def __init__(self, sample_id=\".*\", cell=\".*\", filter_pattern=\".*\", n_obs=\".*\", first_event=\".*\", second_event=\".*\"):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell = cell\n",
    "        self.filter_pattern = filter_pattern\n",
    "        self.n_obs = n_obs\n",
    "        self.first_event = first_event\n",
    "        self.second_event = second_event\n",
    "        \n",
    "        self.pattern = \"{}_{}_{}_{}_{}_{}\"\n",
    "        \n",
    "    def get_filter_regex(self, use_all=False, use_cell=False, use_sample=False, use_filter_pattern=False, use_n_obs=False, use_first_event=False, use_second_event=False):\n",
    "        filter_regex = self.pattern.format(self.sample_id if use_sample or use_all else \".*\",\n",
    "                                          self.cell if use_cell or use_all else \".*\",\n",
    "                                          self.filter_pattern if use_filter_pattern or use_all else \".*\",\n",
    "                                          self.n_obs if use_n_obs or use_all else \".*\",\n",
    "                                          self.first_event if use_first_event or use_all else \".*\",\n",
    "                                          self.second_event if use_second_event or use_all else \".*\")\n",
    "        return filter_regex    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "2.1948051948051948\n"
     ]
    }
   ],
   "source": [
    "# Determine how many cells of the same cell type are in each (abdominal) hemisegment per sample\n",
    "# Basins:\n",
    "#cell_trace_configs = [\n",
    "#    CellTraceConfig('17-09-01L3-cl', 'basin', 'quiet', 'A2'),\n",
    "#    CellTraceConfig('17-08-26L6-cl', 'basin', 'quiet', 'A2')\n",
    "#]\n",
    "\n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig(name,'basin','quiet','A4L') for name in lm_data]    #if all samples should be included, you have \n",
    "                                                                        # to chose 'quiet' as event\n",
    "\n",
    "\n",
    "number_basins = []\n",
    "sample_ids = []\n",
    "for ctc in cell_trace_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells  \n",
    "    number = len(cell_subset_df.columns)\n",
    "    number_basins.append(number)\n",
    "    sample_ids.append(ctc.sample_id)\n",
    "avg = np.mean(number_basins)\n",
    "total = np.sum(number_basins)\n",
    "print(total)\n",
    "#print(len(number_basins))\n",
    "#print(list(zip(sample_ids, number_basins)))\n",
    "print(avg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:24.081692Z",
     "start_time": "2019-07-02T22:09:23.257916Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP of stim = 97.856 17-09-01L3-cl\n",
      "TP of stim = 104.709 17-08-26L5-cl\n",
      "TP of stim = 927.638 17-08-26L5-cl\n"
     ]
    }
   ],
   "source": [
    "# Load single samples with specific filter pattern. A certain event will be marked with vertical line. No \n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig('17-09-01L3-cl', 'basin', 'stim', 'A1L'),\n",
    "    CellTraceConfig('17-08-26L5-cl', 'A00c', 'stim', 'mid')\n",
    "]\n",
    "\n",
    "# Load all samples with specific filter pattern\n",
    "'''\n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig(name,'A00c', 'fw') for name in lm_data]\n",
    "'''\n",
    "\n",
    "all_events = [] #List of events, with raw dff data (no interpolation or other \n",
    "                #processing done at this point). Sample_id is added to the cell name. \n",
    "\n",
    "for ctc in cell_trace_configs:\n",
    "    #print(sample_df.keys)\n",
    "    #break\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(ctc.sample_id, cell_subset_df)   \n",
    "    # Get rows where current event starts.\n",
    "    event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()]]\n",
    "    # Gives the timestamp for the event_df (start)\n",
    "    for idx, row in event_df.iterrows():\n",
    "        #print('TP of {} ='.format(ctc.event_name), row['time'])\n",
    "        print('TP of {} ='.format(ctc.event_name), row['time'], '{}'.format(ctc.sample_id))\n",
    "        \n",
    "    # Extract for specific time window and align several events. \n",
    "    # Define timepoints pre and post an event (event_df). \n",
    "    # This works for single sample or multiple samples aligned \n",
    "    # Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "    # Add index and time = column\n",
    "\n",
    "    # Set the window range left and right from the event\n",
    "    left_half_window_size = 10.0 #in seconds\n",
    "    right_half_window_size = 50.0\n",
    "\n",
    "    # Event_df defined in pargraph before \n",
    "    windows = []\n",
    "    n_behavior = 0\n",
    "    for i,row in event_df.iterrows():\n",
    "        n_behavior += 1\n",
    "        window_start = row['time'] - left_half_window_size\n",
    "        window_end = row['time'] + right_half_window_size\n",
    "        \n",
    "        # Get subset of rows between window_start and window_end       \n",
    "        event = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "        # Normalizing the data to align on beginning of selected\n",
    "        # behavior (event_df = Zero) by substracting events in window\n",
    "        # around start of event of interest from start of event interest.\n",
    "        # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "        event.loc[:, 'time'] = event['time'] - row['time']\n",
    "\n",
    "        # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "        event.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), \n",
    "                     axis = 'columns', inplace = True) \n",
    "\n",
    "        # Rename time collum to time\n",
    "        event.rename(columns={ event.columns[0]: 'time' }, inplace = True) \n",
    "        all_events.append(event) # Append a list with all event\n",
    "        \n",
    "        #Round (NR)\n",
    "        #decimals = 1    \n",
    "        #event['time'] = event['time'].apply(lambda x: round(x, decimals))\n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_df = all_events.pop(0)\n",
    "for right_df in all_events:\n",
    "    all_df = pd.merge_ordered(all_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column (sollte spaeter kommen)\n",
    "all_df.index = all_df[\"time\"]\n",
    "del all_df[\"time\"]        \n",
    "#print(all_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_df = all_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:24.093653Z",
     "start_time": "2019-07-02T22:09:24.083017Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Single sample analysis\n",
    "    # For single sample over the whole experimental time\n",
    "    # Calculate min, max, avg, stddev, sem from cell_subset_df (defined earlier)\n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    del cell_subset_df['time'] # delete time_column\n",
    "    cell_subset_df.index.name = None # delete index name\n",
    "    cell_avg_df = cell_subset_df.mean(axis=1)\n",
    "    cell_min_df = cell_subset_df.min(axis=1)\n",
    "    cell_max_df = cell_subset_df.max(axis=1)\n",
    "    # Standard deviation (distribution)\n",
    "    cell_std_df = cell_subset_df.std(axis = 1)\n",
    "    #standard error of mean\n",
    "    cell_sem_df = cell_subset_df.sem(axis = 1)\n",
    "    #print(ctc.sample_id, cell_avg_df) \n",
    "\n",
    "\n",
    "# For single or multiple sample, aligned for certain event. Interpolated data used!\n",
    "# Averaged all events and all cells pro timepoint\n",
    "# Average for specific cell type filter-pattern (see below)\n",
    "all_cell_avg_df = int_all_df.mean(axis=1) \n",
    "all_cell_min_df = int_all_df.min(axis=1)\n",
    "all_cell_max_df = int_all_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_cell_std_df = int_all_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_cell_sem_df = int_all_df.sem(axis = 1)\n",
    "#print(all_cell_avg_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:24.486205Z",
     "start_time": "2019-07-02T22:09:24.094829Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting - single sample \n",
    "def layout_plot(plot, tick_spacing=100, fov=(0, 2400, 0, 1.2), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "# Get rows where current event is active and draw a vertical \n",
    "# line to indicate the event in the plot\n",
    "event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()] == 1]\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor(\"white\")\n",
    "\n",
    "# Plot all cells from cell_subset_df over entire time (specified in Cell_Trace_Config).\n",
    "sub1 = fig.add_subplot(211) #211\n",
    "cell_subset_df.plot(ax=sub1)\n",
    "ctc.add_event_time_points_to_plot(event_df, sub1)\n",
    "layout_plot(sub1)\n",
    "\n",
    "# Avg, min, max, std-dev for multiple cells in single sample over whole time\n",
    "sub2 = fig.add_subplot(212)#212\n",
    "ctc.add_event_time_points_to_plot(event_df, sub2)\n",
    "cell_avg_df.plot(ax=sub2, color = 'g', label = ctc.cell_type, linewidth=1)\n",
    "#cell_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#cell_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "cell_avg_df.plot.line(yerr=cell_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "#cell_avg_df.plot.line(yerr=cell_sem_df, ax=sub2, color = 'c', alpha = 0.1)\n",
    "layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:09:26.288636Z",
     "start_time": "2019-07-02T22:09:24.488422Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: HERE FOR PLOTTING THE ALIGNED EVENT, INDEPENDENT OF PRO AND / OR POST_EVENT\n",
    "# Plotting for multi-events (all_df) (raw_dff_data)\n",
    "# If a dataframe with NANs is plotted, use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=0.5, fov=(-20, 50, -0.05, 1.9), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "sub1 = fig.add_subplot(211)\n",
    "all_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(212)\n",
    "all_cell_avg_df.plot(ax=sub2, color = 'k', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_cell_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_cell_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "all_cell_avg_df.plot.line(yerr=all_cell_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "#all_cell_avg_df.plot.line(yerr=all_cell_sem_df, ax=sub2, color = 'c', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part extract the information about behavior transition under certain limitation:\n",
    "1) Find transition between two different! behaviors, within a max_delay.\n",
    "2) Find 2 transition between two different! behaviors with intersection: first event end = second event start, within a max_delay.\n",
    "3) Find transition between two identical! behaviors, within a max_delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:20:50.678148Z",
     "start_time": "2019-07-03T13:20:03.122113Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostBehaviorTransition:\n",
    "        \n",
    "    def __init__(self, sample_id, event, post_event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.post_event = post_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_before(sample_id, sample_df, first_event, second_event, \n",
    "                         max_delay=0,\n",
    "                         first_event_duration=None, \n",
    "                         second_event_duration=None):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that is followed by the event <second_event>,\n",
    "    separated by <max_delay> time. The end of <second_event> is expected\n",
    "    to happen strictly after the end of <first_event>. The start time\n",
    "    of <second_event> however can overlap with the end time of <first_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The start time of <second_event> can be before, at or after the\n",
    "    end of <first_event>.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "   \n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of second behavior and remember its time.\n",
    "        if row[second_event_start_col] and not row[second_event_overlap_col]:\n",
    "            #print(\"{} starts at {}\".format(second_event, row[\"time\"]))\n",
    "            second_event_start_time = row['time']\n",
    "        if row[first_event_end_col]:\n",
    "            #print(\"{} ends at {}\".format(first_event, row[\"time\"]))\n",
    "            first_event_end_time = row['time']\n",
    "        if row[first_event_start_col]:\n",
    "            #print(\"{} starts at {}\".format(first_event, row[\"time\"]))\n",
    "            first_event_start_time = row['time']\n",
    "        for column in sample_df.columns:\n",
    "            if (first_event_start_time is not None and\n",
    "                column.endswith(\"_start\") and\n",
    "                \"quiet\" not in column and\n",
    "                column != first_event_start_col and\n",
    "                column != second_event_start_col and\n",
    "                first_event not in column and\n",
    "                second_event not in column):\n",
    "                if row[column]:\n",
    "                    #print(\"{} ended at {}, but then found {} at {}\".format(first_event, first_event_end_time, column, row[\"time\"]))\n",
    "                    first_event_start_time = None\n",
    "                    first_event_end_time = None\n",
    "                    second_event_start_time = None\n",
    "                    second_event_end_time = None\n",
    "                    \n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time):\n",
    "            continue\n",
    "        \n",
    "        # Define rules for event_start_time and event_end_time\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        \n",
    "        # Test if first_event_start_time = second_event_start_time\n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "            \n",
    "        if second_event_end_time is None:\n",
    "            for j, row in sample_df.loc[i:, :].iterrows():\n",
    "                if row[second_event_end_col]:\n",
    "                    second_event_end_time = row[\"time\"]\n",
    "                    break\n",
    "        if second_event_end_time is None:\n",
    "            print(\"warning: end time not found for second event\")\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        # implement event duration (for quiet)\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            if first_event_duration is not None and first_event_end_time - first_event_start_time < first_event_duration:\n",
    "                continue\n",
    "            if second_event_duration is not None and second_event_end_time - second_event_start_time < second_event_duration:\n",
    "                continue\n",
    "            \n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time,\n",
    "                'first_event': first_event,\n",
    "                'second_event': second_event\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Open single samples \n",
    "\n",
    "behavior_transitions = [\n",
    "    PostBehaviorTransition('17-08-26L2-cl', 'fw', 'stim', 4.9)\n",
    "#    PostBehaviorTransition('17-08-26L6-cl', 'turn', 'bw', 4.9)\n",
    "]\n",
    "\n",
    "# Open all samples single Transitions\n",
    "#behavior_transitions = [\n",
    "#    PostBehaviorTransition(name, 'stim', 'fw', 4.9) for name in lm_data]\n",
    "\n",
    "'''\n",
    "# Open all samples multiple Transitions\n",
    "behavior_transitions = [\n",
    "    PostBehaviorTransition(name, 'fw', 'stim', 4.9) for name in lm_data] + [\n",
    "    PostBehaviorTransition(name, 'bw', 'stim', 4.9) for name in lm_data] + [\n",
    "    PostBehaviorTransition(name, 'turn', 'stim', 4.9) for name in lm_data] + [\n",
    "    PostBehaviorTransition(name, 'hunch', 'stim', 4.9) for name in lm_data] + [\n",
    "    PostBehaviorTransition(name, 'other', 'stim', 4.9) for name in lm_data]\n",
    "'''\n",
    "\n",
    "found_transitions = []\n",
    "for bt in tqdm(behavior_transitions):\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if not any([\"bw\" in column for column in sample_df.columns]):\n",
    "        continue\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_before(bt.sample_id, sample_df, bt.event, \n",
    "                                       bt.post_event, bt.max_delay, \n",
    "                                       first_event_duration = None, \n",
    "                                       second_event_duration = None) #For 'quiet' change *_event_duration. Defaul = None.\n",
    "    \n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "\n",
    "print(len(found_transitions)) # Number of data sets \n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions])) # Number of transitions/ not working!!\n",
    "#print(found_transitions) # Transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:01:06.326891Z",
     "start_time": "2019-07-03T13:00:18.335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duration between diff behavior from PostTransition (min, max, avg)\n",
    "# Note: only works for one type of transitions\n",
    "gap_Ptrans = []\n",
    "for sample in found_transitions:\n",
    "    for found_transition in sample:\n",
    "        gap_Ptrans.append((found_transition[\"second_event_start\"])-(found_transition[\"first_event_end\"]))\n",
    "        \n",
    "        # Test minus valus\n",
    "        if ((found_transition[\"second_event_start\"])-(found_transition[\"first_event_end\"])) < -4:\n",
    "            print(bt.sample_id, found_transition[\"first_event_end\"], found_transition[\"second_event_start\"]) \n",
    "#print(gap_Ptrans)\n",
    "\n",
    "avg_duration = np.mean(gap_Ptrans)\n",
    "max_duration = np.max(gap_Ptrans)\n",
    "min_duration = np.min(gap_Ptrans)\n",
    "\n",
    "print(avg_duration)\n",
    "print(max_duration)\n",
    "print(min_duration)\n",
    "\n",
    "# Histogram\n",
    "fig = plt.figure()\n",
    "plt.hist(gap_Ptrans, bins=100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:10:13.991373Z",
     "start_time": "2019-07-02T22:10:05.429676Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define celltype, filter-pattern for transitions. Optional do not run regex-filter. Aligning to first event start or \n",
    "# second event start: depends on stimulus. Output: a) aligned raw values, b) aloigned interpolated values.\n",
    "# tqdm = progress bar\n",
    "\n",
    "cell_Ptrans_configs = []\n",
    "all_Ptrans_events = []\n",
    "\n",
    "\n",
    "for sample in tqdm(found_transitions):\n",
    "    for found_transition in sample:\n",
    "        \n",
    "        # For all behavior except stimulus as first event \n",
    "        cell_Ptrans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"basin\",\n",
    "                                                    found_transition[\"second_event_start\"], \n",
    "                                                    filter_pattern = 'A1R', \n",
    "                                                    first_event=found_transition[\"first_event\"], \n",
    "                                                    second_event=found_transition[\"second_event\"]))\n",
    "        \n",
    "        # For stimulus as first_event\n",
    "        #cell_Ptrans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"basin\", \n",
    "        #                                           found_transition[\"first_event_start\"], 'A6', \n",
    "        #                                           first_event=found_transition[\"first_event\"], \n",
    "        #                                           second_event=found_transition[\"second_event\"]))\n",
    "\n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_df). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "# Add index and time = column\n",
    "\n",
    "# Set the window range left and right from the event (in seconds)\n",
    "left_half_window_size = 18.5 # If negative it goes further to right half (Good for skipping stimulus)\n",
    "right_half_window_size = 42.4\n",
    "\n",
    "# trans_df defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "\n",
    "for ctc in tqdm(cell_Ptrans_configs):\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "   \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    \n",
    "    # Don't apply filter regex, but take all cells from lm_data --> this does not work!\n",
    "    # ToDo\n",
    "    #cell_subset_df = lm_data.get(ctc.sample_id)#Get subset of cells \n",
    "    #cell_subset_df.reset_index(inplace = True, drop = True) # Add index and time = column\n",
    "    \n",
    "    n_behavior += 1\n",
    "    window_start = ctc.event_time - left_half_window_size\n",
    "    window_end = ctc.event_time + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end\n",
    "    # Including event_start\n",
    "    #trans = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "    # Excluding event start\n",
    "    trans = cell_subset_df[(cell_subset_df.time > window_start) & (cell_subset_df.time < window_end)]\n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    #trans.loc[:, 'time'] = trans['time'] - row['time']\n",
    "    trans.loc[:, 'time'] = trans['time'] - ctc.event_time\n",
    "    \n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    trans.rename(lambda x: '{}_{}_{}_{}_{}'.format(ctc.sample_id, x, n_behavior, ctc.first_event, ctc.second_event), axis = 'columns', inplace = True) \n",
    "\n",
    "    # Rename time collum to time\n",
    "    trans.rename(columns={ trans.columns[0]: 'time' }, inplace = True) \n",
    "    all_Ptrans_events.append(trans) # Append a list with all event\n",
    "#print(all_Ptrans_events)\n",
    "\n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_Ptrans_df = all_Ptrans_events.pop(0)\n",
    "for right_df in all_Ptrans_events:\n",
    "    all_Ptrans_df = pd.merge_ordered(all_Ptrans_df, right_df, on=\"time\", how=\"outer\")\n",
    "#print(all_Ptrans_df)\n",
    "\n",
    "# Resets the index as time and drops time column\n",
    "all_Ptrans_df.index = all_Ptrans_df[\"time\"]\n",
    "del all_Ptrans_df[\"time\"]        \n",
    "#print(all_Ptrans_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_Ptrans_df = all_Ptrans_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_Ptrans_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many cells of a determine type and filter pattern are involved in a specific transition\n",
    "def extract_parts(column:str, indicies):\n",
    "    parts = column.split(\"_\")\n",
    "    return tuple(parts[i] for i in indicies)\n",
    "# indicies: 0: sample_id, 1: cell, 2: filter_pattern, 3: n_obs, 4: first_event, 5:second_event\n",
    "parts = [extract_parts(column, [0,1,2]) for column in int_all_Ptrans_df.columns]\n",
    "print(len(parts))\n",
    "print(len(set(parts)))\n",
    "#print(set(parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple transition events, group after transition (first, or second event) <most useful> with option to group\n",
    "# after celltype, filterpattern, sample_id, observations, using class TransitionType\n",
    "\n",
    "transition_types = [\n",
    "    #TransitionType(first_event = 'fw'),\n",
    "    #TransitionType(first_event = 'bw'),\n",
    "    #TransitionType(first_event = 'turn'),\n",
    "    #TransitionType(first_event = 'hunch'),\n",
    "    #TransitionType(first_event = 'other'),\n",
    "    TransitionType(first_event = 'stim')\n",
    "]\n",
    "\n",
    "print(transition_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "# For multiple transition types!! \n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=5, fov=(-18.5, 42.4, 0.0, 1.0), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "colors = [\"cyan\", \"red\", \"violet\", \"red\", \"blue\"]\n",
    "# colors = {tt.get_filter_regex(use_cell=True, use_filter_pattern=True, use_first_event=True, use_second_event=True) : color for tt, color in zip(transition_types, colors)}\n",
    "    \n",
    "print(colors)\n",
    "    \n",
    "fig = plt.figure()\n",
    "sub2 = fig.add_subplot(111) \n",
    "for tt, color in zip(transition_types, colors):\n",
    "    int_some_Ptrans_df = int_all_Ptrans_df.filter(regex=tt.get_filter_regex(use_cell=True, use_filter_pattern=True, use_first_event=True, use_second_event=True))\n",
    "\n",
    "    # Average and stddev, min, max, sem for post_behavior_transition events\n",
    "    all_Ptrans_avg_df = int_some_Ptrans_df.mean(axis=1) # Interpolated data used\n",
    "    all_Ptrans_min_df = int_some_Ptrans_df.min(axis=1)\n",
    "    all_Ptrans_max_df = int_some_Ptrans_df.max(axis=1)\n",
    "    # Standard deviation (distribution)\n",
    "    all_Ptrans_std_df = int_some_Ptrans_df.std(axis = 1)\n",
    "    #standard error of mean\n",
    "    all_Ptrans_sem_df = int_some_Ptrans_df.sem(axis = 1)\n",
    "\n",
    "    all_Ptrans_avg_df.plot(ax=sub2, label = tt.get_filter_regex(use_all=True), color = color) #use interpolated df to calculate average...\n",
    "    #all_Ptrans_avg_df.plot(yerr=all_Ptrans_std_df, ax=sub2, label = tt.get_filter_regex(use_all=True), alpha = 0.005, color = color)\n",
    "    all_Ptrans_avg_df.plot.line(yerr=all_Ptrans_sem_df, ax=sub2, color = 'grey', alpha = 0.5)\n",
    "aligned_layout_plot(sub2, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:10:14.167222Z",
     "start_time": "2019-07-02T22:10:14.007350Z"
    }
   },
   "outputs": [],
   "source": [
    "# To make the plot in the notebook and not in an extra window\n",
    "%matplotlib notebook \n",
    "\n",
    "# Only for one type of transitions (I still need this!!)\n",
    "\n",
    "# Average and stddev, min, max, sem for post_behavior_transition events\n",
    "all_Ptrans_avg_df = int_all_Ptrans_df.mean(axis=1) # Interpolated data used\n",
    "all_Ptrans_min_df = int_all_Ptrans_df.min(axis=1)\n",
    "all_Ptrans_max_df = int_all_Ptrans_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_Ptrans_std_df = int_all_Ptrans_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_Ptrans_sem_df = int_all_Ptrans_df.sem(axis = 1)\n",
    "#wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "#and not avg over all cells per timepoint (refer to Data_filter or Grouper) \n",
    "\n",
    "# Plotting for multi-events (same_behavioral_transition)\n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=5, fov=(-18.5, 42.4, 0.0, 1.0), legend=True): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "sub1 = fig.add_subplot(111) #211\n",
    "all_Ptrans_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "#sub2 = fig.add_subplot(212) #212\n",
    "#all_Ptrans_avg_df.plot(ax=sub2, color = 'c', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_Ptrans_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Ptrans_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Ptrans_avg_df.plot.line(yerr=all_Ptrans_std_df, ax=sub2, color = 'lightgrey', alpha = 0.1)\n",
    "#all_Ptrans_avg_df.plot.line(yerr=all_Ptrans_sem_df, ax=sub2, color = 'grey', alpha = 0.1)\n",
    "#aligned_layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all all_Ptrans_df with left and right window in same data frame and aligned to second_event_start \n",
    "#(for stim first event on first_event_start_)\n",
    "# Average over the frames within the same sample (eg average over 1 sec)\n",
    "# extract all negative values for pre_event\n",
    "# extract positiv values for post event, but in case of the stimulus, the first two seconds must not be included\n",
    "\n",
    "\n",
    "pre_data = all_Ptrans_df[all_Ptrans_df.index < 0.0]\n",
    "post_data = all_Ptrans_df[all_Ptrans_df.index > 2.0] # for stim-behavior >2, otherwise >=0\n",
    "\n",
    "int_pre_data = pre_data.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "int_post_data = post_data.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "\n",
    "# Average over time for each cell type\n",
    "pre_data_avg = pre_data.mean(axis = 0)\n",
    "post_data_avg = post_data.mean(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold change (bar plot) in cell activity post - pre/pre (merged orderd,No interpolation, NO avg)\n",
    "# Fold change for all cells individually\n",
    "\n",
    "# fold changeI: post/pre\n",
    "#fold_change = (post_data_avg)/(pre_data_avg) \n",
    "\n",
    "# fold changeII: post-pre/pre\n",
    "fold_change = ((post_data_avg)-(pre_data_avg))/(pre_data_avg)\n",
    "#print(fold_change.index)\n",
    "\n",
    "# Transform Series to dataframe and name columns\n",
    "fold_change_df = fold_change.to_frame('transitions')\n",
    "# print(fold_change_df.index)\n",
    "# transitions = set([\"_\".join(sample_transition.split(\"_\")[-2:]) for sample_transition in fold_change_df.index])\n",
    "\n",
    "transitions = []\n",
    "past = None\n",
    "for sample_transition in fold_change_df.index:\n",
    "    current = \"_\".join(sample_transition.split(\"_\")[-2:])\n",
    "    if current != past:\n",
    "        transitions.append(current)\n",
    "        past = current\n",
    "print(transitions)\n",
    "\n",
    "print(transitions)\n",
    "transition_df_map = {transition: fold_change_df[fold_change_df.index.str.contains(transition)] for transition in transitions}\n",
    "print(list(transition_df_map.keys()))\n",
    "#print(transition_df_map[\"fw_stim\"].head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "# df_key = name, df-list = dataframe\n",
    "transition_df_list = list(transition_df_map.items())\n",
    "df_key, all_fold_change_df = transition_df_list.pop(0)\n",
    "for df_key, right_df in transition_df_list:\n",
    "    all_fold_change_df = pd.merge(\n",
    "        all_fold_change_df, right_df, left_index=True, right_index=True, how=\"outer\"\n",
    "    )\n",
    "\n",
    "#print(all_fold_change_df.to_string()) #print everything\n",
    "\n",
    "# Plot fold change\n",
    "ax = all_fold_change_df.plot.box() # its a series (diff for data frame)\n",
    "ax.set_title('')\n",
    "ax.set_xlabel('Transitions')\n",
    "ax.set_ylabel('Fold change')\n",
    "ax.set_xticklabels(list(transition_df_map.keys()))\n",
    "\n",
    "#ax = fold_change_df.plot.box() #single transition type\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mann-Witney U test (non-parametric pair wise test)\n",
    "\n",
    "# da ist ne Menge significant:()\n",
    "\n",
    "from scipy.stats import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "#mwu1 = mannwhitneyu(transition_df_map[\"fw_stim\"], transition_df_map[\"bw_stim\"])\n",
    "#mwu2 = mannwhitneyu(transition_df_map[\"fw_stim\"], transition_df_map[\"turn_stim\"])\n",
    "#mwu3 = mannwhitneyu(transition_df_map[\"fw_stim\"], transition_df_map[\"hunch_stim\"])\n",
    "#mwu4 = mannwhitneyu(transition_df_map[\"fw_stim\"], transition_df_map[\"other_stim\"])\n",
    "#mwu5 = mannwhitneyu(transition_df_map[\"bw_stim\"], transition_df_map[\"turn_stim\"])\n",
    "#mwu6 = mannwhitneyu(transition_df_map[\"bw_stim\"], transition_df_map[\"bw_stim\"])\n",
    "\n",
    "#print(mwu1, mwu2, mwu3, mwu4, mwu5, mwu6)\n",
    "#print(mwu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.815183Z",
     "start_time": "2019-07-02T22:10:14.168479Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Triple transition\n",
    "# Find first and  second transition between two different! behaviors with intersection: \n",
    "# first event end = second event start, within a max_delay.\n",
    " \n",
    "# Open single samples \n",
    "'''\n",
    "first_transitions = [\n",
    "    PostBehaviorTransition('17-08-26L6-cl', 'fw', 'stim', 3)\n",
    "]\n",
    "second_transitions = [\n",
    "    PostBehaviorTransition('17-08-26L6-cl', 'stim', 'fw', 3)\n",
    "]\n",
    "'''\n",
    "# Open all samples\n",
    "\n",
    "first_transitions = [\n",
    "    PostBehaviorTransition(name,'bw', 'stim', 3) for name in lm_data]\n",
    "    \n",
    "second_transitions = [\n",
    "    PostBehaviorTransition(name,'stim', 'quiet', 3) for name in lm_data]    \n",
    "\n",
    "\n",
    "found_transitions = []\n",
    "for first_bt, second_bt in tqdm(zip(first_transitions, second_transitions)):\n",
    "    transitions=[]\n",
    "    assert first_bt.sample_id == second_bt.sample_id, \"{} does not match {}\".format(first_bt.sample_id, second_bt.sample_id)\n",
    "    sample_df = sample_data.get(first_bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    if not any([\"bw\" in column for column in sample_df.columns]):\n",
    "        continue\n",
    "    first_transition_duration = None\n",
    "    second_transition_duration = None\n",
    "    third_transition_duration = 2\n",
    "    first_transitions = find_behavior_before(first_bt.sample_id, sample_df, first_bt.event, first_bt.post_event, first_bt.max_delay, first_event_duration=first_transition_duration, \n",
    "                         second_event_duration=second_transition_duration)\n",
    "    second_transitions = find_behavior_before(second_bt.sample_id, sample_df, second_bt.event, second_bt.post_event, second_bt.max_delay, first_event_duration=second_transition_duration, \n",
    "                         second_event_duration=third_transition_duration)\n",
    "    #print(\"{} transitions from {} to {}\".format(len(first_transitions), first_bt.event, first_bt.post_event))\n",
    "    #print(\"{} transitions from {} to {}\".format(len(second_transitions), second_bt.event, second_bt.post_event))\n",
    "    \n",
    "    for ft in first_transitions:\n",
    "        for st in second_transitions:\n",
    "            if abs(ft[\"second_event_start\"] - st[\"first_event_start\"]) < 0.00001:\n",
    "                transitions.append({\n",
    "                    \"sample_id\":ft[\"sample_id\"], \"first_event_start\":ft[\"first_event_start\"], \"first_event_end\":ft[\"first_event_end\"],\n",
    "                    \"second_event_start\": st[\"first_event_start\"], \"second_event_end\": st[\"first_event_end\"],\n",
    "                    \"third_event_start\": st[\"second_event_start\"]\n",
    "                })\n",
    "    if transitions:\n",
    "        print(\"{} transition triples found\".format(len(transitions)))\n",
    "        found_transitions.append(transitions)\n",
    "    \n",
    "\n",
    "\n",
    "print(len(found_transitions)) #number of data sets not the actual stim\n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.897605Z",
     "start_time": "2019-07-02T22:06:49.737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the predefined CellTransConfig to filter by celltype and pattern.\n",
    "# The results are merged_ordered and interpolated.\n",
    "cell_Ttrans_configs = []\n",
    "all_Ttrans_events = []\n",
    "\n",
    "for sample in tqdm(found_transitions):\n",
    "    sample_ls_trans = []\n",
    "    for found_transition in sample:\n",
    "        \n",
    "        # For all behavior\n",
    "        sample_ls_trans.append(found_transition[\"second_event_start\"]) \n",
    "        cell_Ttrans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"A00c\", \n",
    "                                                  found_transition[\"second_event_start\"]))\n",
    "\n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_df). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "# Add index and time = column\n",
    "\n",
    "# Set the window range left and right from the event (in seconds)\n",
    "left_half_window_size = 100.0 \n",
    "right_half_window_size = 200.0\n",
    "\n",
    "# trans_df defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "\n",
    "for ctc in tqdm(cell_Ttrans_configs):\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)\n",
    "    \n",
    "    # Don't apply filter regex, but take all cells from lm_data\n",
    "    #cell_subset_df =  lm_data.get(ctc.sample_id)#Get subset of cells \n",
    "    #cell_subset_df.reset_index(inplace = True, drop = True) # Add index and time = column\n",
    "    #print(cell_subset_df)\n",
    "    \n",
    "    n_behavior += 1\n",
    "    window_start = ctc.event_time - left_half_window_size\n",
    "    window_end = ctc.event_time + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end       \n",
    "    trans = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    #trans.loc[:, 'time'] = trans['time'] - row['time']\n",
    "    trans.loc[:, 'time'] = trans['time'] - ctc.event_time\n",
    "    \n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    trans.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), axis = 'columns', inplace = True) \n",
    "\n",
    "    # Rename time collum to time\n",
    "    trans.rename(columns={ trans.columns[0]: 'time' }, inplace = True) \n",
    "    all_Ttrans_events.append(trans) # Append a list with all event\n",
    "      \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_Ttrans_df = all_Ttrans_events.pop(0)\n",
    "for right_df in all_Ttrans_events:\n",
    "    all_Ttrans_df = pd.merge_ordered(all_Ttrans_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column\n",
    "all_Ttrans_df.index = all_Ttrans_df[\"time\"]\n",
    "del all_Ttrans_df[\"time\"]        \n",
    "#print(all_Ttrans_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_Ttrans_df = all_Ttrans_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_Ttrans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.898443Z",
     "start_time": "2019-07-02T22:06:49.741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average and stddev, min, max, sem for post_behavior_transition events\n",
    "all_Ttrans_avg_df = int_all_Ttrans_df.mean(axis=1) # Interpolated data used\n",
    "all_Ttrans_min_df = int_all_Ttrans_df.min(axis=1)\n",
    "all_Ttrans_max_df = int_all_Ttrans_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_Ttrans_std_df = int_all_Ttrans_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_Ttrans_sem_df = int_all_Ttrans_df.sem(axis = 1)\n",
    "#wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "#and not avg over all cells per timepoint\n",
    "\n",
    "# Plotting for multi-events (same_behavioral_transition)\n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=10, fov=(-20, 50, 0.0, 1.0), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "#sub1 = fig.add_subplot(211)\n",
    "#all_Ttrans_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "#aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(111) #212\n",
    "all_Ttrans_avg_df.plot(ax=sub2, color = 'r', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_Ttrans_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Ttrans_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "all_Ttrans_avg_df.plot.line(yerr=all_Ttrans_std_df, ax=sub2, color = 'lightgrey', alpha = 0.1)\n",
    "#all_Ttrans_avg_df.plot.line(yerr=all_Ttrans_sem_df, ax=sub2, color = 'grey', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:28:40.144304Z",
     "start_time": "2019-07-03T16:28:40.067035Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Behavior_transitions for identical behavior type\n",
    "\n",
    "class SamePairBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, max_delay=0, max_ignored_quiet_time=math.inf):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "        self.max_ignored_quiet_time = max_ignored_quiet_time\n",
    "\n",
    "print(\"finding same behaviors only\")\n",
    "def find_behavior_next(sample_id, sample_df, first_event, second_event, max_delay=0,\n",
    "                       max_ignored_quiet_time=math.inf):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that will be followed by the same event <second_event>,\n",
    "    separated by <max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>.\n",
    "    \n",
    "    If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account differently to match start and end times\n",
    "    to the correct event. During iteration for one loop, we have to exclude the \n",
    "    fact that the first_event == second_event.\n",
    "    \n",
    "    Assumption: No overlapp\n",
    "    Not in data sets identified\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event)\n",
    "    \n",
    "    # Construct a list without quiet and passed in behaviors\n",
    "    other_behavior_cols = get_unrelated_nonquiet_behaviors((first_event, second_event))\n",
    "    other_behavior_start_cols = ['{}_start'.format(c) for c in other_behavior_cols]\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    quiet_event_start_time = None\n",
    "    quiet_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    # Check for overlap between the same behaviors (print index, where 'True') and use\n",
    "    # it as a check that there is not this error in the behavior data\n",
    "    ##print(sample_id, sample_df.index[sample_df['bw_overlap']].tolist())\n",
    "    \n",
    "    # Note: The overlap statement was removed. This part has to be \n",
    "    # checked if overlapping events are found in the data\n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of first behavior and remember its time.\n",
    "        if row[first_event_start_col]and first_event_start_time is None:\n",
    "            first_event_start_time = row['time']\n",
    "        if row[first_event_end_col] and first_event_end_time is None: \n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col] and first_event_start_time is not None:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[second_event_end_col] and first_event_end_time is not None:\n",
    "            second_event_end_time = row['time']\n",
    " \n",
    "        if first_event_start_time:\n",
    "            if row['quiet_start']:\n",
    "                quiet_event_start_time = row['time']\n",
    "            elif quiet_event_start_time is not None and row['quiet_end']:\n",
    "                quiet_event_end_time = row['time']\n",
    "    \n",
    "            reset = False\n",
    "        \n",
    "            # If we found a quiet start and end time *after* the start of\n",
    "            # the first event has been found, reset pattern search if the\n",
    "            # quiet period is larger than max_ignored_quiet_time.\n",
    "            if quiet_event_start_time is not None and \\\n",
    "                    quiet_event_end_time is not None:\n",
    "                quiet_duration = quiet_event_end_time - quiet_event_start_time\n",
    "                if quiet_duration > max_ignored_quiet_time:\n",
    "                    reset = True\n",
    "                \n",
    "            # If any non-quiet and non-passed-in behavior is start\n",
    "            if any(row[c] for c in other_behavior_start_cols):\n",
    "                reset = True\n",
    "                \n",
    "            if reset:\n",
    "                first_event_start_time = None\n",
    "                first_event_end_time = None\n",
    "                quiet_event_start_time = None\n",
    "                quiet_event_end_time = None\n",
    "                second_event_start_time = None\n",
    "                second_event_end_time = None\n",
    "            \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time, second_event_end_time):\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if first_event_start_time == second_event_start_time:\n",
    "            continue\n",
    "        if first_event_end_time == second_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed. During iteration the first_event == second_event. \n",
    "        if (second_event_start_time == first_event_end_time): #NR\n",
    "            continue\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = second_event_start_time\n",
    "        first_event_end_time = second_event_end_time\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "        quiet_event_start_time = None\n",
    "        quiet_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_unrelated_nonquiet_behaviors(excluding=[]):\n",
    "    if 'turn' in excluding:\n",
    "        turn_behaviors = ['left turn', 'right turn']\n",
    "        turn_behaviors.extend(excluding)\n",
    "        excluding = turn_behaviors\n",
    "    return [ab for ab in available_behaviors if ab not in excluding]    \n",
    "\n",
    "# Open single sample\n",
    "\n",
    "#behavior_transitions = [\n",
    "#    SamePairBehaviorTransition('17-08-26L1-cl', 'turn', 'turn', max_delay=10, max_ignored_quiet_time = math.inf)\n",
    "    #SamePairBehaviorTransition('17-08-24L1-cl', 'fw', 'fw', max_delay=10, max_ignored_quiet_time = math.inf)\n",
    "    #SamePairBehaviorTransition('17-11-06L3-cl', 'bw', 'bw', 3),\n",
    "    #SamePairBehaviorTransition('17-11-29L1-cl', 'bw', 'bw', 3)\n",
    "#]\n",
    "\n",
    "\n",
    "# Open all samples \n",
    "behavior_transitions = [\n",
    "    SamePairBehaviorTransition(name, 'turn', 'turn', max_delay=10, max_ignored_quiet_time = math.inf) for name in lm_data]\n",
    "\n",
    "\n",
    "found_transitions = []\n",
    "for bt in tqdm(behavior_transitions):\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    \n",
    "    if not any([\"bw\" in column for column in sample_df.columns]):\n",
    "        continue\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_next(bt.sample_id, sample_df, bt.pre_event,\n",
    "                                     bt.event, bt.max_delay, bt.max_ignored_quiet_time)\n",
    "\n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "#print(len(transitions))\n",
    "print(len(found_transitions))\n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))\n",
    "#print(found_transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T14:30:01.813730Z",
     "start_time": "2019-07-03T14:30:01.756366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duration between behavior from SameTransition (min, max, avg)\n",
    "gap_Strans = []\n",
    "for sample in tqdm(found_transitions):\n",
    "    for found_transition in sample:\n",
    "        gap_Strans.append((found_transition[\"second_event_start\"])-(found_transition[\"first_event_end\"]))\n",
    "        \n",
    "        if ((found_transition[\"second_event_start\"])-(found_transition[\"first_event_end\"])) < -5:\n",
    "            print(bt.sample_id, found_transition[\"first_event_end\"], found_transition[\"second_event_start\"]) \n",
    "#print(gap_Strans)\n",
    "\n",
    "avg_duration = np.mean(gap_Strans)\n",
    "max_duration = np.max(gap_Strans)\n",
    "min_duration = np.min(gap_Strans)\n",
    "\n",
    "print(avg_duration)\n",
    "print(max_duration)\n",
    "print(min_duration)\n",
    "\n",
    "# Histogram (doesnt show everything)\n",
    "plt.hist(gap_Strans, bins=100, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "###there is always the same timestamp and -7.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.901778Z",
     "start_time": "2019-07-02T22:06:49.753Z"
    }
   },
   "outputs": [],
   "source": [
    "# For same_behavior_transition events get event_time and use cell_trace_config to filter by celltype and pattern.\n",
    "# The results are merged and interpolated.\n",
    "cell_Strans_configs = []\n",
    "all_Strans_events = []\n",
    "\n",
    "for sample in tqdm(found_transitions):\n",
    "    sample_ls_trans = []\n",
    "    for found_transition in sample:\n",
    "        #print(found_transition[\"sample_id\"], found_transition[\"second_event_start\"])\n",
    "        #print(found_transition[\"first_event_end\"])\n",
    "        # Sustitute cell_type with None if you don\"t want to sort\n",
    "        sample_ls_trans.append(found_transition[\"second_event_start\"])\n",
    "        cell_Strans_configs.append(CellTransConfig(found_transition[\"sample_id\"], 'A00c', \n",
    "                                                  found_transition[\"second_event_start\"], 'mid' ))\n",
    "\n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_df). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "# Add index and time = column\n",
    "\n",
    "# Set the window range left and right from the event\n",
    "left_half_window_size = 20.0 #in seconds\n",
    "right_half_window_size = 50.0\n",
    "\n",
    "# trans_df defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "\n",
    "for ctc in tqdm(cell_Strans_configs):\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    # Apply filter regex \n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)\n",
    "    \n",
    "    # Don't apply filter regex, but take all cells from lm_data\n",
    "    #cell_subset_df =  lm_data.get(ctc.sample_id)#Get subset of cells  (includes column with sample_id and exp_id)\n",
    "    #cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    #cell_subset_df.reset_index(inplace = True, drop = True) # Add index and time = column\n",
    "    #if 'time' not in cell_subset_df:\n",
    "    #    print(\"ignoring \" + ctc.sample_id)\n",
    "    #    continue\n",
    "    #else:\n",
    "    #    print(\"processing \" + ctc.sample_id)\n",
    "    #print(cell_subset_df['time'])\n",
    "    \n",
    "    n_behavior += 1\n",
    "    window_start = ctc.event_time - left_half_window_size\n",
    "    window_end = ctc.event_time + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end       \n",
    "    trans = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "    #print(trans)\n",
    "    #print(trans['time'])\n",
    "    #print(trans.loc[:,'time'])\n",
    "    #print(trans['time']- ctc.event_time)\n",
    "    \n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    #trans.loc[:, 'time'] = trans['time'] - row['time']\n",
    "    #trans.loc[:, 'time'] = trans['time'] - ctc.event_time # Works after applying filer\n",
    "    trans.loc[:, 'time'] = trans['time'] - ctc.event_time\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    trans.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), axis = 'columns', inplace = True) \n",
    "\n",
    "    # Rename time collum to time\n",
    "    trans.rename(columns={ trans.columns[0]: 'time' }, inplace = True) \n",
    "    all_Strans_events.append(trans) # Append a list with all event\n",
    "    #print(trans)    \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_Strans_df = all_Strans_events.pop(0)\n",
    "for right_df in all_Strans_events:\n",
    "    all_Strans_df = pd.merge_ordered(all_Strans_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column (sollte spaeter kommen)\n",
    "all_Strans_df.index = all_Strans_df[\"time\"]\n",
    "del all_Strans_df[\"time\"]        \n",
    "#print(all_Strans_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_Strans_df = all_Strans_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_Strans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.902726Z",
     "start_time": "2019-07-02T22:06:49.758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average and stddev, min, max, sem for same_behavior_transition events\n",
    "all_Strans_avg_df = int_all_Strans_df.mean(axis=1) # Interpolated data used\n",
    "all_Strans_min_df = int_all_Strans_df.min(axis=1)\n",
    "all_Strans_max_df = int_all_Strans_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_Strans_std_df = int_all_Strans_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_Strans_sem_df = int_all_Strans_df.sem(axis = 1)\n",
    "#wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "#and not avg over all cells per timepoint --> should work with transition-grouper\n",
    "\n",
    "# Plotting for multi-events (same_behavioral_transition)\n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=1, fov=(-10, 10, 0.0, 0.2), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "#sub1 = fig.add_subplot(111) #211\n",
    "#all_trans_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "#aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(111) #212\n",
    "all_Strans_avg_df.plot(ax=sub2, color = 'r', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_Strans_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Strans_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "all_Strans_avg_df.plot.line(yerr=all_Strans_std_df, ax=sub2, color = 'lightgrey', alpha = 0.5)\n",
    "#all_Strans_avg_df.plot.line(yerr=all_Strans_sem_df, ax=sub2, color = 'grey', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.903505Z",
     "start_time": "2019-07-02T22:06:49.761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using class Transitiongrouper to group cell_type/pattern or both\n",
    "# This is crucial if statistical methods are applied for sub groups\n",
    "# Input dataframe from behavioral transitions (Post-, Same-, Triple-Transition)\n",
    "\n",
    "#grouper = TransitionGrouper(int_all_Ptrans_df)\n",
    "grouper = TransitionGrouper(int_post_data)\n",
    "\n",
    "# Function to calculate the average cell/pattern_groups depending on regex\n",
    "def average_grouping(grouping):\n",
    "    df = grouping[list(grouping)[0]][1]\n",
    "    average_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for group_pattern, df in grouping.values():\n",
    "        average_col = df.mean(axis=1)\n",
    "        average_df[group_pattern] = average_col\n",
    "\n",
    "    return average_df\n",
    "\n",
    "# Function to calculate the std cell/pattern_groups depending on regex\n",
    "def std_grouping(grouping):\n",
    "    df = grouping[list(grouping)[0]][1]\n",
    "    std_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for group_pattern, df in grouping.values():\n",
    "        std_col = df.std(axis=1)\n",
    "        std_df[group_pattern] = std_col\n",
    "\n",
    "    return std_df\n",
    "\n",
    "# Function to calculate the sem cell/pattern_groups depending on regex\n",
    "def sem_grouping(grouping):\n",
    "    df = grouping[list(grouping)[0]][1]\n",
    "    sem_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for group_pattern, df in grouping.values():\n",
    "        sem_col = df.sem(axis=1)\n",
    "        sem_df[group_pattern] = sem_col\n",
    "\n",
    "    return sem_df\n",
    "    \n",
    "    \n",
    "cell_groups = grouper.group_cells()\n",
    "cell_averages = average_grouping(cell_groups)\n",
    "cell_std = std_grouping(cell_groups)\n",
    "cell_sem = sem_grouping(cell_groups)\n",
    "#print(cell_averages.head())\n",
    "\n",
    "pattern_groups = grouper.group_patterns()\n",
    "pattern_averages = average_grouping(pattern_groups) \n",
    "pattern_std = std_grouping(pattern_groups)\n",
    "pattern_sem = sem_grouping(pattern_groups)\n",
    "#print(pattern_averages.head())\n",
    "\n",
    "cellpattern_groups = grouper.group_cellpattern()\n",
    "cellpattern_averages = average_grouping(cellpattern_groups) \n",
    "cellpattern_std = std_grouping(cellpattern_groups)\n",
    "cellpattern_sem = sem_grouping(cellpattern_groups)\n",
    "#print(pattern_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.904374Z",
     "start_time": "2019-07-02T22:06:49.765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot average, std-dev from Transition grouper\n",
    "\n",
    "def aligned_layout_plot(plot, tick_spacing=1, fov=(-10, 10, 0.0, 0.9), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "sub1 = fig.add_subplot(211)\n",
    "pattern_averages.plot(ax=sub1)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(212)\n",
    "cell_averages.plot(ax=sub2, color = 'r')\n",
    "#cell_averages.plot(ax=sub2, yerr=cell_std, color = 'lightgrey', alpha = 0.1)\n",
    "cell_averages.plot(ax=sub2, yerr=cell_sem, color = 'grey', alpha = 0.5)\n",
    "aligned_layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.905360Z",
     "start_time": "2019-07-02T22:06:49.772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duration of each behavior (min, max, avg)\n",
    "\n",
    "desired_behavior = \"hunch\"\n",
    "all_durations = []\n",
    "sample_ids = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "for sample_id, sample_df in tqdm(sample_data.items()):\n",
    "    if any([\"bw\" in column for column in sample_df.columns]):\n",
    "        for i, row in sample_df.iterrows():\n",
    "            if row[\"{}_start\".format(desired_behavior)]:  \n",
    "                start_times.append(row[\"time\"])\n",
    "                sample_ids.append(sample_id)\n",
    "            if row[\"{}_end\".format(desired_behavior)]: \n",
    "                end_time = row[\"time\"]\n",
    "                end_times.append(row[\"time\"])\n",
    "        assert len(end_times) == len(start_times), \"{} start times vs {} end times on sample {}\".format(len(start_times), len(end_times), sample_id)\n",
    "all_durations = [end - start for start, end in zip(start_times, end_times)]\n",
    "durations_with_sample_id = list(zip(sample_ids, all_durations))\n",
    "\n",
    "# Test if behavior.csv is correct\n",
    "for t, sample_id, start_time, end_time in zip(all_durations, sample_ids, start_times, end_times):\n",
    "    if t > 200:\n",
    "        print(sample_id, start_time, end_time)\n",
    "\n",
    "#print(durations_with_sample_id[0:10])\n",
    "\n",
    "#print(all_durations)       \n",
    "#print(len(all_durations))\n",
    "avg_duration = np.mean(all_durations)\n",
    "max_duration = np.max(all_durations)\n",
    "min_duration = np.min(all_durations)\n",
    "\n",
    "print(avg_duration)\n",
    "#print(max_duration)\n",
    "#print(min_duration)\n",
    "\n",
    "# Histogram\n",
    "fig = plt.figure()\n",
    "plt.hist(all_durations, bins=5, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "#print(list(filter(lambda x: x> 10, all_durations)))  # not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.907908Z",
     "start_time": "2019-07-02T22:06:49.779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neuronal activity of a certain behavior (all behavior from the same type, independent on pre- and post-event)\n",
    "\n",
    "# For all behavior events (same type) get event_time and use cell_trace_config to filter by celltype and pattern.\n",
    "\n",
    "# The results are merged and interpolated.\n",
    "\n",
    "cell = \"A00c\" \n",
    "#pattern = 'mid'\n",
    "desired_behavior = \"fw\"\n",
    "align_to_start = True\n",
    "\n",
    "sample_ids = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "desired_traces = []\n",
    "\n",
    "# Checkpoint if not all behavior data are used for analysis, but dff and timestamps are available\n",
    "for sample_id, sample_df in tqdm(sample_data.items()):\n",
    "    if any([\"bw\" in column for column in sample_df.columns]):\n",
    "        for i, row in sample_df.iterrows():\n",
    "            if row[\"{}_start\".format(desired_behavior)]:  \n",
    "                start_times.append(row[\"time\"])\n",
    "                sample_ids.append(sample_id)\n",
    "            if row[\"{}_end\".format(desired_behavior)]: \n",
    "                end_time = row[\"time\"]\n",
    "                end_times.append(row[\"time\"])\n",
    "        assert len(end_times) == len(start_times), \"{} start times vs {} end times on sample {}\".format(len(start_times), len(end_times), sample_id)\n",
    "\n",
    "for sample_id, start_time, end_time in zip(sample_ids, start_times, end_times):\n",
    "    if align_to_start:\n",
    "        desired_traces.append(CellTransConfig(sample_id, cell,\n",
    "                                              start_time))\n",
    "    else:\n",
    "        desired_traces.append(CellTransConfig(sample_id, cell, \n",
    "                                              end_time))\n",
    "        \n",
    "print(len(desired_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing if samples are different from each other, extract per sample the peak of each behavior wave (eg fw),\n",
    "# for each A00c, each hemisegment for Basin and each segment for handles\n",
    "\n",
    "cell = \"A00c\" #Note: for each cell type we need a specific grouping because of missing identity\n",
    "pattern = 'postR'\n",
    "desired_behavior = \"fw\"\n",
    "\n",
    "sample_ids = []\n",
    "peak_values = []\n",
    "start_times = []\n",
    "start_index = []\n",
    "end_times = []\n",
    "end_index = []\n",
    "\n",
    "# Checkpoint if not all behavior data are used for analysis, but dff and timestamps are available\n",
    "for sample_id, sample_df in tqdm(sample_data.items()):\n",
    "    if any([\"bw\" in column for column in sample_df.columns]):\n",
    "        for i, row in sample_df.iterrows():\n",
    "            if row[\"{}_start\".format(desired_behavior)]:  \n",
    "                start_times.append(row[\"time\"])\n",
    "                start_index.append(i)\n",
    "                sample_ids.append(sample_id)\n",
    "            if row[\"{}_end\".format(desired_behavior)]: \n",
    "                end_time = row[\"time\"]\n",
    "                end_index.append(i)\n",
    "                end_times.append(row[\"time\"])\n",
    "        assert len(end_times) == len(start_times), \"{} start times vs {} end times on sample {}\".format(len(start_times), len(end_times), sample_id)\n",
    "        \n",
    "        \n",
    "max_values = {}\n",
    "for sample_id, start_time, end_time in zip(sample_ids, start_index, end_index):\n",
    "    max_values.setdefault(sample_id, [])\n",
    "    regex_pattern = \"{}_{}\".format(cell, pattern)\n",
    "    sample_df = sample_data.get(sample_id)\n",
    "    max_values[sample_id].append(sample_df.filter(like=regex_pattern, axis=1).loc[start_time:end_time].max().mean())\n",
    "    \n",
    "print({key: len(values) for key, values in max_values.items()})\n",
    "print(max_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vnc_stats_python.hypothesis_tests.permutation_tests import all_pairs_perm_tests\n",
    "\n",
    "# Pull out max values for each sample, taking care to throw out any nan values\n",
    "smp_labels = list(max_values.keys())\n",
    "n_smps = len(smp_labels)\n",
    "smp_max_vls = [None]*n_smps\n",
    "for i, s_l in enumerate(smp_labels):\n",
    "    vls = np.asarray(max_values[s_l])\n",
    "    vls = vls[np.logical_not(np.isnan(vls))] # Get rid of nan values\n",
    "    smp_max_vls[i] = vls\n",
    "    \n",
    "    \n",
    "# Now we want to get rid of any samples where we had *no* data (after removing nan values)\n",
    "good_inds = [len(v) != 0 for v in smp_max_vls]\n",
    "smp_labels = [s_l for i, sl in enumerate(smp_labels) if good_inds[i]]\n",
    "smp_max_vls = [vl for i, vl in enumerate(smp_max_vls) if good_inds[i]]\n",
    "    \n",
    "p_vls = all_pairs_perm_tests(smp_max_vls, test_opts={'n_perms': 10000}, update_int=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(p_vls < .01)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_vls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.909047Z",
     "start_time": "2019-07-02T22:06:49.783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix: cell/cell before and cell/cell after transition\n",
    "\n",
    "# Extract for data int_all_Ptrans where second_event_start - second event ends\n",
    "#I cheat with window size (its actually a good idea:)\n",
    "#legend would be nice\n",
    "# header of matrix \n",
    "# muss average pro cell_id haben sonst plottet er fur jedes sample/transition extra\n",
    "# average over cells not time should work\n",
    "\n",
    "# TEST\n",
    "#print(int_pre_data)\n",
    "#print(int_post_data)\n",
    "\n",
    "# Using TransitionGrouper \n",
    "# Be careful where you generate the dataframe \n",
    "# TODO after grouper I lose pre and post window\n",
    "#grouper = [TransitionGrouper(int_pre_data),\n",
    "#           #TransitionGrouper(int_post_data)\n",
    "#          ] NOT WORKING + OVERWRITES WITH THE PREVIOUS ONE\n",
    "\n",
    "\n",
    "#cell_groups = grouper.group_cells()\n",
    "#cell_averages = average_grouping(cell_groups)\n",
    "#print(cell_averages)\n",
    "\n",
    "#pattern_groups = grouper.group_patterns()\n",
    "\n",
    "#cellpattern_groups = grouper.group_cellpattern()\n",
    "#cellpattern_averages = average_grouping(cellpattern_groups) \n",
    "#print(pattern_averages)\n",
    "print(cellpattern_averages)\n",
    "\n",
    "# Define time window for pre-transition_start\n",
    "# pre_data defined before\n",
    "# Define time window for post-transition_start\n",
    "# post_data defined before \n",
    "\n",
    "plt.matshow(cellpattern_averages.corr())\n",
    "#plt.matshow(int_post_data.corr())\n",
    "#plt.matshow(int_post_data.corr()-int_pre_data.corr())\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "plt.title('Correlation Matrix', fontsize=16);\n",
    "plt.show()\n",
    "correlation = np.corrcoef(cellpattern_averages) #data(cell,time)\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = cellpattern_averages.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "\n",
    "#size(c1, c2) ~ abs(corr(c1, c2)) # should make the visualisation better (not tested yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.910756Z",
     "start_time": "2019-07-02T22:06:49.787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test if sum of left and right turns = turns\n",
    "#for sample_id, sample_df in tqdm(sample_data.items()):\n",
    "    #print(sample_id, sum(sample_df['turn_start']), sum(sample_df['left turn_start']), sum(sample_df['right turn_start']))\n",
    "#    if sum(sample_df['turn_start']) != (sum(sample_df['left turn_start']) + sum(sample_df['right turn_start'])):\n",
    "#        print(sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.912275Z",
     "start_time": "2019-07-02T22:06:49.790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test number of behavior\n",
    "#alle = []\n",
    "#for sample_id, sample_df in tqdm(sample_data.items()):\n",
    "    #print(sum(sample_df['turn_start']))\n",
    "#    alle.append(sum(sample_df['HP_start']))\n",
    "    \n",
    "#print(sum(alle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.913774Z",
     "start_time": "2019-07-02T22:06:49.797Z"
    }
   },
   "outputs": [],
   "source": [
    "# ToDo \n",
    "\n",
    "# Will/ Tom \n",
    "# For cell pattern basin -hemisegment grouping!!\n",
    "# For permutation test, does not work if several cells\n",
    "# FOR CORRELATION MATRIX 2 groups (pre and post) using Transition grouper is not possible\n",
    "# also for Correlation do multiple transition eg compare post_data fw with post_data bw\n",
    " # For correlation matrix update grouper! the cellpattern does not work as I wanted\n",
    "\n",
    " # For Posttransition: Don't apply filter regex, but take all cells from lm_data --> this does not work!\n",
    "    # ToDo\n",
    "    #cell_subset_df = lm_data.get(ctc.sample_id)#Get subset of cells \n",
    "    #cell_subset_df.reset_index(inplace = True, drop = True) # Add index and time = column\n",
    "    \n",
    "\n",
    "# Group after observations (eg Basin per hemisegment)\n",
    "# NR\n",
    "\n",
    "#raw data stim substraction (image calculation) or Albert:(\n",
    "# Correlation matrix\n",
    "# cell activity during a certain behavior\n",
    "# cross-correlation\n",
    "# correlation matrix cell-cell before and after \n",
    "# non-negative matrix factorization (for dynamic and weighting) ?? ich weiss ja nicht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.914873Z",
     "start_time": "2019-07-02T22:06:49.801Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.915964Z",
     "start_time": "2019-07-02T22:06:49.806Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list(event_df)) #prints header\n",
    "#rounded time is only visual, I still get several 'same' tp\n",
    "#sample_df.round({'time' : 1})\n",
    "   # Round time on 1 or 2nd decimal\n",
    "    # the df.round({'time' : 1}) doesn't work if to many decimals\n",
    "    #decimals = 2    \n",
    "    #timestamp_df['time'] = timestamp_df['time'].apply(lambda x: round(x, decimals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.918002Z",
     "start_time": "2019-07-02T22:06:49.813Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Test\n",
    "def test_find_behavior_before():\n",
    "    data_columns = ['time', 'bw_start', 'bw_end', 'bw_overlap', 'fw_start', 'fw_end', 'fw_overlap', 'turn_start', 'turn_end', 'turn_overlap']\n",
    "    data = [\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [2, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [3, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [4, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [5, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    ]\n",
    "    toy_df = pd.DataFrame(data, columns = data_columns)\n",
    "    \n",
    "    behavior_transitions = [\n",
    "        #PostBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "        #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 3),\n",
    "        #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 3),\n",
    "        PostBehaviorTransition('na', 'fw', 'bw', 5)\n",
    "    ]\n",
    "    \n",
    "    found_transitions = []\n",
    "    for bt in behavior_transitions:\n",
    "        sample_df = toy_df\n",
    "        if sample_df is None:\n",
    "            raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "        transitions = find_behavior_before(bt.sample_id, sample_df, bt.event, bt.post_event, bt.max_delay)\n",
    "\n",
    "        if transitions:\n",
    "            found_transitions.append(transitions)\n",
    "\n",
    "\n",
    "    print(len(found_transitions)) #number of data sets not the actual stim\n",
    "    print(len(transitions)) \n",
    "    print(found_transitions)\n",
    "    \n",
    "test_find_behavior_before()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.918956Z",
     "start_time": "2019-07-02T22:06:49.818Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {'A' : [3.2, np.nan, np.nan, 5, np.nan, np.nan],\n",
    "     'B' : [np.nan, 4.1, np.nan, np.nan, 6.2, np.nan], \n",
    "     'C' : [np.nan, np.nan, 1.1, np.nan, np.nan, 2.5]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "\n",
    "#int_df_linear = df.interpolate(method = 'linear', inplace = False)\n",
    "#print(int_df_linear)\n",
    "\n",
    "#int_df_index = df.interpolate(method='index', inplace=False)\n",
    "#print(int_df_index)\n",
    "#u = df['T'].unique\n",
    "\n",
    "# Resets the index as time and drops time column\n",
    "#df.index = df[\"T\"]\n",
    "#del df[\"T\"] \n",
    "\n",
    "#df.fillna(value=None, method='ffill', axis=None, inplace=False, limit=None, downcast=None) # not what I want\n",
    "#df.interpolate(method='linear', axis=0, limit=None,\n",
    "#                      inplace=False, limit_direction='forward', limit_area=None, downcast=None) \n",
    "\n",
    "\n",
    "# print out only rows where in'T' are duplicates\n",
    "#df[df.duplicated(subset= ['T'], keep=False)]\n",
    "\n",
    "    \n",
    "    \n",
    "#np.nanmean(j, axis=0)\n",
    "#    for j in df.loc[df[\"T\"]== value]:\n",
    "#        np.nanmean(j, axis=0)\n",
    "\n",
    "\n",
    "#df.mean(axis=1, skipna=None)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.919974Z",
     "start_time": "2019-07-02T22:06:49.823Z"
    }
   },
   "outputs": [],
   "source": [
    "t = {'T' : [1,2, 3, 4, 5, 6, 7, 8], 'A' : [3.2, 5, 5.5, 5.3, 9, 8, 8, 3],\n",
    "     'B' : [4.1, 6.2, 6.0, 6.2, 8, 1, 1.5, 3.7], \n",
    "     'C' : [1.1, 2.5, 2.3, 1.2, 0.9, 1.1, 1.8, 1.7]}\n",
    "df1 = pd.DataFrame(data=t)\n",
    "df1\n",
    "\n",
    "d = {'A' : [3.2, 5, 5.5, 5.3, 9, 8, 8, 3],\n",
    "     'B' : [4.1, 6.2, 6.0, 6.2, 8, 1, 1.5, 3.7], \n",
    "     'C' : [1.1, 2.5, 2.3, 1.2, 0.9, 1.1, 1.8, 1.7]\n",
    "    }\n",
    "df2 = pd.DataFrame(data=d)\n",
    "print(df2)\n",
    "subset = df2.loc[[2] ,['B']]\n",
    "subset1 = df2.loc[[2] ,:]\n",
    "print(subset1)\n",
    "#fig = plt.figure()\n",
    "#ax = fold_change_avg.plot.bar()\n",
    "#ax = df2.boxplot()\n",
    "\n",
    "\n",
    "#DataFrame.boxplot(column=None, by=None, \n",
    "#                  ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None,\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.921406Z",
     "start_time": "2019-07-02T22:06:49.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Interpolation \n",
    "# inplace  = False, since we want to keep the data sets with raw data as well\n",
    "\n",
    "# Linear Interpolation: According to documentation, \n",
    "# because it assums the index is equally spaced.\n",
    "# ‘Index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "int_df2 = df2.interpolate(method = 'index', inplace = False)\n",
    "print(int_df2)\n",
    "\n",
    "#Note: First 5 values = NAN!!??!! (Method?)\n",
    "\n",
    "#intpol_all_df = all_df.interpolate(method='index', inplace=False)\n",
    "#print(intpol_all_df)\n",
    "ls = [2, 1.2, 3, 8.2]\n",
    "#for i in ls:\n",
    "    #print(i)\n",
    "a =int_df2.loc[int_df2['C'].isin(ls)]\n",
    "print(a)\n",
    "# find row where C = 1.2 #isin\n",
    "#a =int_df2.loc[df['C'].isin(1.2)]\n",
    "#a =int_df2.loc[int_df2['C'] == 1.2]\n",
    "\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.922529Z",
     "start_time": "2019-07-02T22:06:49.831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data anlysis - TODO\n",
    "\n",
    "# Dataprocessing\n",
    "# For the next step, we try two methods two normalize the data and get the timestamps \n",
    "# in synchrony between the different samples/events (1.Interpolation of some kind, 2. Binning,\n",
    "# 3) fitting curve)\n",
    "               \n",
    "# Interpolation \n",
    "# inplace  = False, since we want to keep the data sets with raw data as well\n",
    "\n",
    "# Linear Interpolation: According to documentation it is not correct to use, \n",
    "# because it assums the index is equally spaced.\n",
    "\n",
    "# ‘Index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "#Note: First 5 values = NAN!!??!!\n",
    "\n",
    "#intpol_all_df = all_df.interpolate(method='index', inplace=False)\n",
    "#print(intpol_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.923696Z",
     "start_time": "2019-07-02T22:06:49.835Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#test\n",
    "def test_find_behavior_next():\n",
    "    data_columns = ['time', 'bw_start', 'bw_end', 'bw_overlap', 'fw_start', 'fw_end', 'fw_overlap', 'turn_start', 'turn_end', 'turn_overlap']\n",
    "    data = [\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [2, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [3, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [4, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [5, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [7, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [8, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [9, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [10, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [11, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [12, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [13, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [14, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    "    toy_df = pd.DataFrame(data, columns = data_columns)\n",
    "\n",
    "    behavior_transitions = [\n",
    "    SamePairBehaviorTransition('na', 'bw', 'bw', 12)]\n",
    "\n",
    "\n",
    "    found_transitions = []\n",
    "    for bt in behavior_transitions:\n",
    "        sample_df = toy_df\n",
    "        if sample_df is None:\n",
    "            raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "        transitions = find_behavior_next(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "\n",
    "        if transitions:\n",
    "            found_transitions.append(transitions)\n",
    "\n",
    "    print(len(transitions))\n",
    "    print(len(found_transitions))\n",
    "    print(found_transitions)\n",
    "    \n",
    "test_find_behavior_next()\n",
    "'''\n",
    "\n",
    "\n",
    "# Merge\n",
    "#all_fold_change_df = pd.merge(X_X_fold_change_df, Y_Y_fold_change_df, left_index = True, right_index = True, how='outer')\n",
    "#print(all_fold_change_df.to_string()) #print everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T22:11:00.924917Z",
     "start_time": "2019-07-02T22:06:49.839Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "a,b,c,*d = x\n",
    "print(d)\n",
    "\n",
    "\n",
    "\n",
    "x = {1:2, 2:3, 3:4}\n",
    "print(x[1] == 2) # retrieve value associated with key 1\n",
    "print(x.get(1, -1) == 2) # retrieve value associated with key 1 if key 1 exists else return -1\n",
    "print(x.get(5, -1) == -1) # retrieve value associated with key 5 if key 5 exists else return -1\n",
    "print(x[5] == -1)\n",
    "\n",
    "# same as x.get(5, -1)\n",
    "try:\n",
    "    print(x[5])\n",
    "except KeyError:\n",
    "    print(-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "from numpy.linalg import norm as get_vector_magnitude\n",
    "x = np.array([1,2,3,4,5])\n",
    "print(np.linalg.norm(x))\n",
    "print(get_vector_magnitude(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(all_Ptrans_df)\n",
    "#print(len(all_Ptrans_df))\n",
    "\n",
    "#print(all_Ptrans_df.mean(axis=0))\n",
    "#for index, row in all_Ptrans_df.iterrows():\n",
    "\n",
    "#for index in range(0, len(all_Ptrans_df.index)):\n",
    "#    if all_Ptrans_df.index[index] < 0: \n",
    "        #print('pre', all_Ptrans_df.index[index]) #OK print time\n",
    "#        print(all_Ptrans_df.iloc[index,:])\n",
    "#        ## no loop for mean!\n",
    "        \n",
    "        #print('mean', pre_event)\n",
    "        \n",
    "        #pre_event_df.append(pre_event)\n",
    "        #print('pre_event', pre_event)\n",
    "    \n",
    "    #if all_Ptrans_df.index[index] >= 0:\n",
    "        #print('post', all_Ptrans_df.index[index])\n",
    "        #print(all_Ptrans_df.iloc[index,:])\n",
    "    #    post_event = all_Ptrans_df.mean(axis=0)\n",
    "    #    post_event_df.append(post_event)\n",
    "        #print('post_event', post_event)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "509px",
    "left": "1323px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
