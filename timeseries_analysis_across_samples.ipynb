{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: behavior annotation with Chris's fran code are zero-based!!!\n",
    "\n",
    "addition to the code: \n",
    "\n",
    "1) read in a single/ few data sets and read all datasets (for cellConfig and behavior_transition)\n",
    "\n",
    "2) check if I really have solved the timestamp issue\n",
    "\n",
    "3) check if all csv files exists\n",
    "\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was build for analysis of neuronal avtivity data in combination with behavioral annotations. \n",
    "\n",
    "Multiple csv-files (behavior, dff, time_stamp, neuronal activity (lm-data)) are combined in a single dataframe per experiment (sample_df). The identity of the sample is kept by introducing a sample-ID (date, number of sample) and an experiment-ID (imaging acquisition type <close and open loop>). Kind of behavior is predefined and extended with 'quiet'. Data of behavior, as well time-data are combined with all lm data. To define the start end end, as well accoiunt for overlapping events of the same behavior, a start, end, and overlap column is added for each behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries and implement error messages (Default should be True)\n",
    "# To make the plot in the notebook and not in an extra window\n",
    "%matplotlib notebook \n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import re\n",
    "\n",
    "error_on_missing_timestamps = False\n",
    "error_on_time_light_mismatch = False\n",
    "error_on_time_behavior_mismatch = False\n",
    "error_on_missing_behaviors = False\n",
    "error_on_invalid_behavior_range = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open multiple .csv from single directory. Define existing behaviors. Define sample_ID and experiment_ID.\n",
    "\n",
    "#directory for behavior data\n",
    "behavior_directory = r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv/' # directory for behavioral annotations\n",
    "behavior_files = glob.glob(os.path.join(behavior_directory, \"*.csv\")) #join pathname with filename\n",
    "\n",
    "# Behavior columns available in CSV files\n",
    "available_behaviors = ('fw', 'bw', 'stim', 'hunch', 'turn', 'other', 'HP', 'left turn', 'right turn')\n",
    "\n",
    "#Regular expression (define the expression filenames are searched for)\n",
    "#'.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "#'+' 1>> occurences, 'L' here character from filename\n",
    "#() outcome here: 2 groups, useful for extraction\n",
    "#[] optional list, eg 1 --> 1\n",
    "#? character non or once \n",
    "\n",
    "#Behavior reg-ex (regular expression)\n",
    "behavior_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-behavior-(.+).csv')\n",
    "\n",
    "# Function: readall_behavior iterates through all csv (sorted) \n",
    "# and appends the files into the list (ls) and returns dictionary\n",
    "def readall_behavior(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample \n",
    "        # (cl (closeloop, RGECO/ Chronos), ol (openloop, RGECO/ Chronos), \n",
    "        # blocks (Raghav: GCaMP/Chrimson))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        # Look for filename_components, which are true for pattern\n",
    "        match = behavior_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        #define filename_components sample_id (first group), and exp_id (sec group)\n",
    "        part_sample_id, _, exp_id = filename_components         \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ';')\n",
    "        df.fillna(0, inplace=True) #replace NaN with zero\n",
    "        df['sample_id'] = sample_id  #add sample_id column\n",
    "        df['exp_id'] = exp_id #add exp_id column\n",
    "        data[sample_id] = df\n",
    "        #Count 'True' for each column ('behavior') in each single behavior.csv)\n",
    "        #print(filename, df[df == 1].count()) \n",
    "        #print(df)\n",
    "    return data\n",
    "\n",
    "behavior_data = readall_behavior(behavior_files)\n",
    "#print(behavior_data['17-08-26L3-cl'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START           11\n",
      "END              0\n",
      "fw            5128\n",
      "bw             926\n",
      "stim           353\n",
      "hunch          444\n",
      "turn          2196\n",
      "other          183\n",
      "HP             790\n",
      "left turn     1104\n",
      "right turn    1093\n",
      "sample_id        0\n",
      "exp_id           0\n",
      "stimulus         4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency of each behavior in all imported behavior.csv by using the returned 'ls' from \n",
    "# the function readAll: concatenate the 'behavior_files' (global variable). 'True' for each \n",
    "# column ('behavior_type') in the concatenated file (df_behavior).\n",
    "# Sorting has to be = False (warning message without 'sort')\n",
    "df_behavior = pd.concat(behavior_data.values(), axis = 0, ignore_index = True, sort = False) #add sorting\n",
    "print(df_behavior[df_behavior == 1].count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and merge fluorescence data: Several LM files for the same sample_id exists, but differ in cell_id).\n",
    "# List of LM data with two extra columns: sample_id and cell_id\n",
    "\n",
    "    #Mapping of sampleID vs lists of LM dataframes\n",
    "    #Convert list to a single dataframe\n",
    "    #Map with df_behavior (later done)\n",
    "\n",
    "# Open LM files from different directories\n",
    "lightmicroscope_directories = [r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Basin_traces/', \n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Handle-like_Traces',\n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/a00c_traces'\n",
    "                              ] \n",
    "\n",
    "# Iterate through LM data and extend files in a list from within and between directory and \n",
    "# build a list of files from all directories\n",
    "# (Note: append would 'extend' lists and not single files)\n",
    "lightmicroscope_files = []\n",
    "for d in lightmicroscope_directories:\n",
    "    lightmicroscope_files.extend(\n",
    "        glob.glob(os.path.join(d, \"*.csv\"))) #join pathname with filename, \n",
    "                                                                       \n",
    "# Regular expression (define the expression filenames are searched for)\n",
    "# '.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "# '+' 1>> occurences, 'L' here character from filename\n",
    "# () outcome here: 2 groups, useful for extraction\n",
    "\n",
    "# Lightmicroscopic data reg-ex (regular expression)\n",
    "lightmicroscope_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-(.*)-(.*).csv')\n",
    "\n",
    "# Function: readall_lm iterates through all LM_csv (sorted) \n",
    "# and returns a dictionary{key:value} \n",
    "# samples = {sample_id:cell-id}\n",
    "def readall_lm(all_files):\n",
    "    samples = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # Larva ID: YY-MM-DDLX, DETAIL = cell_id\n",
    "        #look for filename_components, which are true for pattern\n",
    "        match = lightmicroscope_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, cell_id, exp_id = filename_components\n",
    "        \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        # Read LM.files \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ',')\n",
    "        # Replace NaN with zero\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cellname to each column as prefix\n",
    "        # lambda is a non defined function (longer version: def lambda(x):)\n",
    "        # Rename of columns after the format cell_id, name) eg: Basin A9\n",
    "        # inplace = True: column names are overwritten (if False: new dataframe)\n",
    "        df.rename(lambda x: '{}_{}'.format(cell_id, x), axis = 'columns', inplace = True)\n",
    "        \n",
    "        # Get the sample_id (key) from the dictionary? to make a list [sample_cells] and \n",
    "        # if sample_id exists, append the list\n",
    "        # if sample_id does not exists, start a new list\n",
    "        # reminder: there can be several cell_id per sample_id\n",
    "        sample_cells = samples.get(sample_id)\n",
    "        if not sample_cells:\n",
    "            samples[sample_id] = sample_cells = {\n",
    "                'data': [],\n",
    "                'exp_id': exp_id,\n",
    "            }\n",
    "        sample_cells['data'].append(df)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "lm_samples = readall_lm(lightmicroscope_files)\n",
    "\n",
    "# New dictionary: lm_data{} to build a single dataframe with all cell_ids combined \n",
    "# for a single sample. Iterate over dict (samples)? and data from same sample in \n",
    "# one dataframe. \n",
    "# df.items iterate over pairs and build a list\n",
    "\n",
    "lm_data = {}\n",
    "\n",
    "# Iterate over all light samples and merge all found files\n",
    "# for each sample into a single data frame (per sample)\n",
    "for sample_id, sample_info in lm_samples.items():\n",
    "    cells_dataframes = sample_info['data']\n",
    "    #check if number of cells >= 1\n",
    "    if not cells_dataframes:\n",
    "        raise ValueError('No cells found for sample {}'.format(sample_id))\n",
    "    #first element in the list\n",
    "    lm_df = None\n",
    "\n",
    "    #iteration through other df\n",
    "    for cdf in cells_dataframes:\n",
    "        if lm_df is None:\n",
    "            lm_df = cdf\n",
    "        else:\n",
    "            if len(lm_df.index) != len(cdf.index):\n",
    "                raise ValueError('Data frame frame to merge has not same row count as target', sample_id)\n",
    "            lm_df = pd.merge(lm_df, cdf, left_index = True, right_index = True)\n",
    "            \n",
    "    lm_df['sample_id'] = sample_id  #add sample_id column\n",
    "    lm_df['exp_id'] = sample_info['exp_id']\n",
    "    lm_data[sample_id] = lm_df\n",
    "#print(list(lm_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import txt-files from of the absolute time/frame from the Ca-imaging (lm-data). \n",
    "# All txt-files have to be transposed, which is a memory intensive step. After the \n",
    "# data are complete, the transposed files should be exported (ToDo). Time-data are \n",
    "# combined with sample-ID and experiment-ID.\n",
    "\n",
    "timelapse_directory =(r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/timelapse/') \n",
    "timelapse_files = glob.glob(os.path.join(timelapse_directory, \"*.txt\")) #join pathname with filename\n",
    "\n",
    "# Regular expression (define the expression filenames are searched for)\n",
    "# '.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "# '+' 1>> occurences, 'L' here character from filename\n",
    "# () outcome here: 2 groups, useful for extraction\n",
    "# [] optional list, eg 1 --> 1\n",
    "# ? character non or once \n",
    "\n",
    "# Behavior reg-ex (regular expression)\n",
    "time_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-time-(.+).txt')\n",
    "\n",
    "# Function: readall_timelapse iterates through all txt (sorted) and appends the \n",
    "# files into the dict (data) and returns ls\n",
    "def readall_time(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample (cl (closeloop), ol (openloop), blocks (Raghav))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        #look for filename_components, which are true for pattern\n",
    "        match = time_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, exp_id = filename_components #define filename_components sample_id (first group), and exp_id (sec group)  \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, header=1, index_col=None, delim_whitespace = True)\n",
    "        df = df.T #transposing because read_csv imports as row\n",
    "        df = df.reset_index() #transpose function sets data as index\n",
    "        df.rename(columns={'index':'time'}, inplace=True) #rename reset index column to time\n",
    "        df['time'] = df.time.astype(float)\n",
    "        data[sample_id] = df\n",
    "        \n",
    "    return data\n",
    "\n",
    "time_data = readall_time(timelapse_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17-08-24L5-cl: time data (6686 entries) doesn't match light data (6685 entries)\n",
      "17-08-26L1-cl: time data (6990 entries) doesn't match light data (6989 entries)\n",
      "17-08-27L2-cl: time data (6470 entries) doesn't match light data (6469 entries)\n",
      "17-08-28L3-cl: time data (6228 entries) doesn't match light data (6225 entries)\n",
      "17-08-29L2-cl: time data (6817 entries) doesn't match light data (6805 entries)\n",
      "17-11-03L7-cl: time data (1399 entries) doesn't match light data (2657 entries)\n",
      "17-11-04L1-cl: time data (6325 entries) doesn't match light data (6324 entries)\n",
      "17-11-06L1-cl: could not find timestamp data for sample\n",
      "17-11-08L3-cl: time data (3240 entries) doesn't match light data (6474 entries)\n",
      "17-11-26L1-cl: time data (6487 entries) doesn't match light data (6469 entries)\n",
      "17-11-29L3-cl: time data (6567 entries) doesn't match light data (6561 entries)\n",
      "17-11-30L2-cl: time data (6640 entries) doesn't match light data (6396 entries)\n",
      "Matched 76 light data sets with their respective time points\n"
     ]
    }
   ],
   "source": [
    "sample_data = {}\n",
    "\n",
    "# Time data are merged into light data and checked if number length of lm = timestamp.  \n",
    "# Due to technical conditions, some time.txt-file have too many or not enough time data compared\n",
    "# to the corresponding LM data. The discrepancy is fixed by either dropping the extra timepoints or \n",
    "# by taking the average of the difference between each timepoint and extend the dataframe. \n",
    "# The first 10 timepoints are not included to account for instability of the microscope in \n",
    "# the beginning due to the moving parts. \n",
    "# Maximal difference between timepoints fyi.\n",
    "\n",
    "for sample_id, sample_df in lm_data.items():\n",
    "    # Add time stamps to data frame of current sample by merging\n",
    "    # The time data frame for the current sample, which is expected\n",
    "    # to match the light data (based on index).\n",
    "    timestamp_df = time_data.get(sample_id)\n",
    "    if timestamp_df is None:\n",
    "        msg = '{}: could not find timestamp data for sample'.format(sample_id)\n",
    "        if error_on_missing_timestamps:\n",
    "            raise ValueError(msg)\n",
    "        # Ignore, if missing data shouldn't cancel the whole process.\n",
    "        print(msg)\n",
    "        continue\n",
    "        \n",
    "    n_timestamps = len(timestamp_df)\n",
    "    n_lightdata = len(sample_df)\n",
    "    \n",
    "    # The timestamp and light recordings are done by different systems.\n",
    "    # This can cause the existence of additional time points/ or missing time points in a\n",
    "    # dataset, which will be filtered out in the merge operation below.\n",
    "    if n_lightdata != n_timestamps:\n",
    "        msg = '{}: time data ({} entries) doesn\\'t match light data ({} entries)'.format(\n",
    "                sample_id, n_timestamps, n_lightdata)\n",
    "        if error_on_time_light_mismatch:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        diffs = np.diff(timestamp_df['time'])[10:] #from 10th row onwards\n",
    "        diffs_avg = diffs.mean(axis=0)\n",
    "        #diff between timedata and lightdata\n",
    "        missing_data = len(sample_df) - len(timestamp_df)\n",
    "        \n",
    "        #add 'diffs_avg' to fill in missing_timedata\n",
    "        if missing_data > 0:\n",
    "            last_valid_index = len(timestamp_df) - 1\n",
    "            last_timestamp = timestamp_df.iloc[last_valid_index]['time']\n",
    "            if pd.isna(last_timestamp):\n",
    "                raise ValueError('Unexpected last valid timestamp for sample {} at index {}'.format(\n",
    "                        sample_id, last_valid_index))\n",
    "            for i in range(0, missing_data):\n",
    "                last_valid_index += 1\n",
    "                timestamp_df.loc[last_valid_index] = timestamp_df.iloc[last_valid_index - 1]['time'] + diffs_avg\n",
    "        elif missing_data < 0:\n",
    "            drop_start = len(timestamp_df) + missing_data\n",
    "            drop_end = len(timestamp_df)\n",
    "            timestamp_df.drop(list(range(drop_start, drop_end)))\n",
    "\n",
    "    # Merge timedata into light data\n",
    "    # Use an 'inner' join/merge to exclude time points that don't have matching light data.\n",
    "    new_sample_df = pd.merge(sample_df, timestamp_df, left_index = True, right_index = True, how='inner')\n",
    "    \n",
    "    # Store newly created data frame for sample (dictionary)\n",
    "    sample_data[sample_id] = new_sample_df\n",
    "    \n",
    "print('Matched {} light data sets with their respective time points'.format(len(sample_data)))\n",
    "\n",
    "# Max.diffs for timestamps\n",
    "diffs = np.diff(timestamp_df['time'])[10:] #from 10th row onwards\n",
    "mx = diffs.max()\n",
    "#print(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-84c42e552256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Make sure we capture start/end times that are a fractional number.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'START'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'END'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}: start and end frame number can\\'t contain fractions'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# Combine behavior data with light data into a single data frame\n",
    "# per sample ID. To do so, add behavior data to light data frames,\n",
    "# because the light data is already organizes by frame. To accomodate\n",
    "# frame ranges without an behavior data, a column named \"quiet\" is\n",
    "# added which is True in these cases and False otherwise. Additionally,\n",
    "# for each behavior column, a behavior start and end column as well as\n",
    "# an overlap column is added so that parallel and successive behaviors\n",
    "# of the same type can be differentiated.\n",
    "\n",
    "for sample_id, sample_df in sample_data.items():\n",
    "    sample_behavior = behavior_data.get(sample_id)\n",
    "    if sample_behavior is None:\n",
    "        msg = 'Could not find behavior data for sample \"{}\"'.format(sample_id)\n",
    "        if error_on_missing_behaviors:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        continue\n",
    "\n",
    "    # Add extra columns for behavior\n",
    "    for behavior in available_behaviors:\n",
    "        sample_df[behavior] = False\n",
    "        sample_df['{}_start'.format(behavior)] = False\n",
    "        sample_df['{}_end'.format(behavior)] = False\n",
    "        sample_df['{}_overlap'.format(behavior)] = False\n",
    "    \n",
    "    # Add 'quiet' column. Set it initially to True and mark frames\n",
    "    # with actual behavior as quiet = False.\n",
    "    sample_df['quiet'] = True\n",
    "    \n",
    "    n_light_entries = len(sample_df)\n",
    "\n",
    "    # Iterate over behavior data and add data to target data frame\n",
    "    for i, row in sample_behavior.iterrows():\n",
    "        # Start ane end are 1-based, make them 0-based\n",
    "        start = int(row['START'])\n",
    "        end = int(row['END'])\n",
    "        \n",
    "        if start >= end:\n",
    "            msg = \"{}: start ({}) needs to be strictly smaller than end ({})\".format(sample_id, start, end)\n",
    "            if error_on_invalid_behavior_range:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "        \n",
    "        # Make sure we capture start/end times that are a fractional number.\n",
    "        if row['START'] - start > 0 or row['END'] - end > 0:\n",
    "            raise ValueError('{}: start and end frame number can\\'t contain fractions'.format(sample_id))\n",
    "            \n",
    "        # Ignore behavior entries with an end frame higher than available light data.\n",
    "        # The behavior data is one-based, which is why a strict larger than test should\n",
    "        # be correct.\n",
    "        if end > n_light_entries:\n",
    "            msg = 'Sample: {} - Behavior row with range {}-{} exceeds light time points ({}): {}'.format(\n",
    "                sample_id, start, end, n_light_entries, row)\n",
    "            if error_on_time_behavior_mismatch:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "            \n",
    "        # Find behavior observed in row\n",
    "        observed_behaviors = []\n",
    "        for behavior in available_behaviors:\n",
    "            if row[behavior]:\n",
    "                observed_behaviors.append(behavior)\n",
    "        \n",
    "        # We assume that not more than two behaviors are observed at the same time\n",
    "        if len(observed_behaviors) > 2:\n",
    "            raise ValueError('Found multiple behaviors in row {} of sample {}'.format(i, sample_id))\n",
    "        \n",
    "        # Add observed behavior information to target data frames in all\n",
    "        # rows in behavior range.\n",
    "        for b in observed_behaviors:\n",
    "            # Iterate over frames valid for current behavior. Every valid\n",
    "            # frame is mapped into the canonical (light/cell) data frame,\n",
    "            # which is 0-indexed.\n",
    "            for j in range(start, end + 1):\n",
    "                # Behavior ranges are 1-indexed\n",
    "                current_frame = j - 1\n",
    "                # If the current behavior has already been observed at this frame,\n",
    "                # set overlap to True, because we are about to mark this behavior\n",
    "                # again as observed for this frame.\n",
    "                if sample_df.at[current_frame, b]:\n",
    "                    sample_df.at[current_frame, '{}_overlap'.format(b)] = True\n",
    "                else:\n",
    "                    sample_df.at[current_frame, b] = True\n",
    "                \n",
    "                # Mark this row as not quiet, because we observed\n",
    "                # a behavior in the current frame.\n",
    "                sample_df.at[current_frame, 'quiet'] = False\n",
    "\n",
    "            sample_df.at[start - 1, '{}_start'.format(b)] = True\n",
    "            sample_df.at[end - 1, '{}_end'.format(b)] = True\n",
    "            \n",
    "    # Mark quiet ranges with _start, _end and _overlap. By definion,\n",
    "    # quiet_overlap is always False.\n",
    "    sample_df['quiet_start'] = False\n",
    "    sample_df['quiet_end'] = False\n",
    "    sample_df['quiet_overlap'] = False\n",
    "    last_sample_idx = n_light_entries - 1\n",
    "    for i, row in sample_df.iterrows():\n",
    "        sample_df.at[i, 'quiet_start'] = row['quiet'] and (i == 0 or not sample_df.at[i - 1, 'quiet'])\n",
    "        sample_df.at[i, 'quiet_end'] = row['quiet'] and (i == last_sample_idx or not sample_df.at[i + 1, 'quiet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above generated dataframe per sample (sample_df) including lm data, behavior, time and sample_id/exp_id, and the extended cell-names and extra behavioral columns can be analysed in the following section.\n",
    "\n",
    "Class is defined, including sample_id, cell_type, event (type), and filter pattern. This \n",
    "allows to extract information throughout all samples about activity pattern (lm-data) \n",
    "of a specific celltype (including sub-type) and event_start (=behavior). \n",
    "\n",
    "For single sample: cell_subset_df \n",
    "        allows visualisation of whole experimantal time, and extract/ visualized information around a specific\n",
    "        event, which are aligned and normalized (event_start = zero). An adjustable time-window around the \n",
    "        event_start of interested is included. \n",
    "    - avg, max, min, stdev and sem can be direct calculated from the data (for whole timeseries)\n",
    "    - But if events are aligned, same problem as with multiple samples\n",
    "        \n",
    "For multiple combined samples (no additional processing): all_events\n",
    "        extract/ visualized information around a specific\n",
    "        event, which are aligned and normalized (event_start = zero). An adjustable time-window around the\n",
    "        event_start of interested is included. \n",
    "\n",
    "    The samples are imaged (Ca-imaging) with different imaging speed, and therefore a direct comparison between the \n",
    "    samples is not possible. Following things have to be considered:\n",
    "    \n",
    "                    ToDo!!!\n",
    "    \n",
    "        - If the data are analysed as raw data (all_events) there are many NaN in the data set, and also the same \n",
    "        timestamp is duplicated for each individual sample\n",
    "        - avg, max, min, stdev and sem can not be direct calculated from the data, because(?) vergessen:(\n",
    "        \n",
    "        - For some analysis (), the samples should be aligned: For now we use interpolation after index. \n",
    "        THAT HAS TO BE CHECKED!!!  \n",
    "        \n",
    "        - alternatives: underlying fitting curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define a class with sample_id, cell_type, event_name and filter_pattern\n",
    "class CellTraceConfig:\n",
    "    \n",
    "    def __init__(self, sample_id, cell_type, event_name, filter_pattern=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell_type = cell_type\n",
    "        self.event_name = event_name\n",
    "        self.filter_pattern = filter_pattern\n",
    "        \n",
    "    def get_filter_regex(self):\n",
    "        filter_regex = '^{}_'.format(self.cell_type)\n",
    "        if self.filter_pattern:\n",
    "            filter_regex += '.*{}.*'.format(self.filter_pattern)\n",
    "        return filter_regex\n",
    "    \n",
    "    def get_event_start_col(self):\n",
    "        return '{}_start'.format(self.event_name)\n",
    "\n",
    "    def add_event_time_points_to_plot(self, source_df, plot):\n",
    "        for idx, row in source_df.iterrows():\n",
    "            plot.annotate(self.event_name, xy=(row['time'], 1))\n",
    "            plt.axvline(row['time'], color='k', linestyle='-')  \n",
    "'''    \n",
    "# Allows to load specific samples (single samples) with specific filter pattern\n",
    "cell_trace_configs = [\n",
    "    #CellTraceConfig('17-08-26L6-cl', 'basin', 'stim'),\n",
    "    CellTraceConfig('17-08-26L1-cl', 'A00c', 'stim', 'mid'),\n",
    "    CellTraceConfig('17-08-26L2-cl', 'A00c', 'stim'),\n",
    "    CellTraceConfig('17-08-26L5-cl', 'A00c', 'stim'),\n",
    "    CellTraceConfig('17-08-26L6-cl', 'A00c', 'stim', 'mid'),\n",
    "    CellTraceConfig('17-08-24L2-1-cl', 'A00c', 'stim'),\n",
    "    CellTraceConfig('17-08-24L2-2-cl', 'A00c', 'stim', 'mid'),\n",
    "    CellTraceConfig('17-08-24L5-cl', 'A00c', 'stim', 'mid')\n",
    "]\n",
    "'''\n",
    "# Allows to load all samples with specific filter pattern\n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig(name,'A00c', 'stim', 'mid') for name in lm_data]\n",
    "\n",
    "# Allow to load all samples with specific filter pattern\n",
    "# ToDo\n",
    "\n",
    "all_events = [] #List of events, with raw dff data (no interpolation or other \n",
    "                #processing done at this point). Sample_id is added to the cell name. \n",
    "\n",
    "for ctc in cell_trace_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue\n",
    "        \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)   \n",
    "    # Get rows where current event starts.\n",
    "    event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()]]\n",
    "    \n",
    "    # Gives the timestamp for the event_df (start)\n",
    "    #for idx, row in event_df.iterrows():\n",
    "    #    print('TP of {} ='.format(ctc.event_name), row['time'])\n",
    "        \n",
    "    # Extract for specific time window and align several events. \n",
    "    # Define timepoints pre and post an event (event_df). \n",
    "    # This works for single sample or multiple samples aligned \n",
    "    # Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "    # Add index and time = column\n",
    "\n",
    "    # Set the window range left and right from the event\n",
    "    left_half_window_size = 20.0 #in seconds\n",
    "    right_half_window_size = 100.0\n",
    "\n",
    "    # Event_df defined in pargraph before \n",
    "    windows = []\n",
    "    n_behavior = 0\n",
    "    for i,row in event_df.iterrows():\n",
    "        n_behavior += 1\n",
    "        window_start = row['time'] - left_half_window_size\n",
    "        window_end = row['time'] + right_half_window_size\n",
    "        \n",
    "        # Get subset of rows between window_start and window_end       \n",
    "        event = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "        # Normalizing the data to align on beginning of selected\n",
    "        # behavior (event_df = Zero) by substracting events in window\n",
    "        # around start of event of interest from start of event interest.\n",
    "        # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "        event.loc[:, 'time'] = event['time'] - row['time']\n",
    "\n",
    "        # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "        event.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), \n",
    "                     axis = 'columns', inplace = True) \n",
    "\n",
    "        # Rename time collum to time\n",
    "        event.rename(columns={ event.columns[0]: 'time' }, inplace = True)\n",
    "        all_events.append(event) # Append a list with all event\n",
    "        \n",
    "        #Round (NR)\n",
    "        #decimals = 1    \n",
    "        #event['time'] = event['time'].apply(lambda x: round(x, decimals))\n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_df = all_events.pop(0)\n",
    "for right_df in all_events:\n",
    "    all_df = pd.merge_ordered(all_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column\n",
    "all_df.index = all_df[\"time\"]\n",
    "del all_df[\"time\"]   \n",
    "#print(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single sample, over whole experimental time\n",
    "- does not have much meaningfulness\n",
    "\n",
    "single sample, aligned for events\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Single sample -analysis\n",
    "# For single sample over the whole experimental time\n",
    "# Note: multiple sample-comparison need more pre-processing(see below)\n",
    "# Calculate min, max, avg, stddev, sem from cell_subset_df (defined earlier)\n",
    "cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "del cell_subset_df['time'] # delete time_column\n",
    "cell_subset_df.index.name = None # delete index name\n",
    "cell_avg_df = cell_subset_df.mean(axis=1)\n",
    "cell_min_df = cell_subset_df.min(axis=1)\n",
    "cell_max_df = cell_subset_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "cell_std_df = cell_subset_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "cell_sem_df = cell_subset_df.sem(axis = 1)\n",
    "#print(cell_avg_df) #OK\n",
    "\n",
    "\n",
    "#Average is wrongly applied, because it avg all events and all cells pro tp\n",
    "# Good! NaN are ignored and the correct avg is calculated\n",
    "# For single sample, aligned for certain event\n",
    "all_cell_avg_df = all_df.mean(axis=1)\n",
    "all_cell_min_df = all_df.min(axis=1)\n",
    "all_cell_max_df = all_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_cell_std_df = all_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_cell_sem_df = all_df.sem(axis = 1)\n",
    "#print(all_cell_avg_df) #wrong zur haelfte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting - single sample\n",
    "\n",
    "def layout_plot(plot, tick_spacing=10, fov=(-2, 2500, -0.05, 1.0), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "# Get rows where current event is active and draw a vertical \n",
    "# line to indicate the event in the plot\n",
    "event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()] == 1]\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from cell_subset_df over entire time (specified in Cell_Trace_Config).\n",
    "sub1 = fig.add_subplot(211)\n",
    "cell_subset_df.plot(ax=sub1)\n",
    "ctc.add_event_time_points_to_plot(event_df, sub1)\n",
    "layout_plot(sub1)\n",
    "\n",
    "# Avg, min, max, std-dev\n",
    "sub2 = fig.add_subplot(212)\n",
    "ctc.add_event_time_points_to_plot(event_df, sub2)\n",
    "cell_avg_df.plot(ax=sub2, color = 'k', label = ctc.cell_type, linewidth=2)\n",
    "#cell_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#cell_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "cell_avg_df.plot.line(yerr=cell_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "#cell_avg_df.plot.line(yerr=cell_sem_df, ax=sub2, color = 'c', alpha = 0.1)\n",
    "layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: HERE FOR PLOTTING THE ALIGNED EVENT, INDEPENDENT OF PRO AND / OR POST_EVENT\n",
    "# (should be after transition_event)\n",
    "\n",
    "# Plotting for multi-events (all_df) (raw_dff_data)\n",
    "# If a dataframe with NANs is plotted, use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=0.5, fov=(-20, 100, -0.05, 1.7), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "sub1 = fig.add_subplot(211)\n",
    "all_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(212)\n",
    "all_cell_avg_df.plot(ax=sub2, color = 'k', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data anlysis - TODO\n",
    "\n",
    "\n",
    "# Dataprocessing\n",
    "# For the next step, we try two methods two normalize the data and get the timestamps \n",
    "# in synchrony between the different samples/events (1.Interpolation of some kind, 2. Binning,\n",
    "# 3) fitting curve)\n",
    "               \n",
    "# Interpolation \n",
    "# inplace  = False, since we want to keep the data sets with raw data as well\n",
    "\n",
    "# Linear Interpolation: According to documentation it is not correct to use, \n",
    "# because it assums the index is equally spaced.\n",
    "\n",
    "# ‘Index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "#Note: First 5 values = NAN!!??!!\n",
    "\n",
    "#intpol_all_df = all_df.interpolate(method='index', inplace=False)\n",
    "#print(intpol_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part extract the information about behavior transition under certain limitation:\n",
    "1) Find second_behavior, and extract information if the defined first_behavior happens within a max_delay.\n",
    "2) Find first_behavior, and extract information if the defined second_behavior happens within a max_delay.\n",
    "3) Find second_behavior, and extract information if the defined first_behavior and third_behavior \n",
    "   happens within a max_delays.\n",
    "4) Find first_behavior, and extract information if the same second_behavior happens within a max_delay. Note: So far no overlap cases detected. Code for overlap cases could not be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_after(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <second_event> that follow the event <first_event>,\n",
    "    separated by <max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>.\n",
    "    \n",
    "    TODO: If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account to match start and end times\n",
    "    to the correct event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for end of first behavior and remember its time.\n",
    "        if row[first_event_start_col]:\n",
    "            first_event_start_time = row['time']\n",
    "        if row[first_event_end_col] and not row[first_event_overlap_col]:\n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col]:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[second_event_end_col]:\n",
    "            second_event_end_time = row['time']\n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time, second_event_end_time):\n",
    "            continue\n",
    "            \n",
    "        #NR\n",
    "        # Define rules for event_start_time and event_end_time\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > second_event_end_time:\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "behavior_transitions = [\n",
    "    PreBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 3),\n",
    "    #PreBehaviorTransition('17-08-26L6-cl', 'turn', 'bw', 3)\n",
    "]\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_after(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "    found_transitions.append(transitions)\n",
    "\n",
    "print(len(found_transitions))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, event, post_event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.post_event = post_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_before(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that is followed by the event <second_event>,\n",
    "    separated by <max_delay> time. The end of <second_event> is expected\n",
    "    to happen strictly after the end of <first_event>. The start time\n",
    "    of <second_event> however can overlap with the end time of <first_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The start time of <second_event> can be before, at or after the\n",
    "    end of <first_event>.\n",
    "    \n",
    "    TODO: If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account to match start and end times\n",
    "    to the correct event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of second behavior and remember its time.\n",
    "        if row[second_event_start_col] and not row[second_event_overlap_col]:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[first_event_end_col]:\n",
    "            first_event_end_time = row['time']\n",
    "        if row[first_event_start_col]:\n",
    "            first_event_start_time = row['time']\n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time):\n",
    "            continue\n",
    "        \n",
    "        #NR\n",
    "        # Define rules for event_start_time and event_end_time\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        \n",
    "        # Test if first_event_start_time = second_event_start_time\n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Open single samples (!see CellConfig!) >ToDo\n",
    "#behavior_transitions = [\n",
    "    #PostBehaviorTransition('17-08-26L1-cl', 'stim', 'fw', 5),\n",
    "    #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 5),\n",
    "    #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 5),\n",
    "    #PostBehaviorTransition('17-08-26L6-cl', 'stim', 'fw', 5)\n",
    "#]\n",
    "\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    PostBehaviorTransition(name,'stim', 'fw', 3) for name in lm_data]\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_before(bt.sample_id, sample_df, bt.event, bt.post_event, bt.max_delay)\n",
    "    found_transitions.append(transitions)\n",
    "\n",
    "#print(len(found_transitions)) #number of data sets not the actual stim\n",
    "#print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, post_event,\n",
    "                 pre_max_delay=0, post_max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.post_event = post_event\n",
    "        self.pre_max_delay = pre_max_delay\n",
    "        self.post_max_delay = post_max_delay\n",
    "\n",
    "def find_behavior_between(sample_id, sample_df, first_event, second_event,\n",
    "                          third_event, pre_max_delay=0, post_max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <second_event> that a) follows the event <first_event>,\n",
    "    separated by <pre_max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <pre_max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>. And b) the behavior <second_event> is followed by the\n",
    "    event <third_event>, separated by <post_max_delay> time. The end of\n",
    "    <third_event> is expected to happen strictly after the end of <second_event>.\n",
    "    The start time of <third_event> however can overlap with the end time of\n",
    "    <second_event>. In this case, the time difference is negative, and still\n",
    "    smaller than <post_max_delay>. The start time of <third_event> can be before,\n",
    "    at or after the end of <second_event>.\n",
    "    \n",
    "    The start of <first_event> is expected to happen strictly before the start\n",
    "    of <third_event>. Apart from this, both <first_event> and <third_event> can\n",
    "    overlap.\n",
    "    \n",
    "    TODO: handle overlaps when behavior types are the same.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    third_event_start_col = '{}_start'.format(third_event)\n",
    "    third_event_end_col = '{}_end'.format(third_event)\n",
    "    third_event_overlap_col = '{}_overlap'.format(third_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    third_event_start_time = None\n",
    "    third_event_end_time = None\n",
    "\n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for behaviors and remember its time.\n",
    "        if row[first_event_start_col] and first_event_start_time is None:\n",
    "            first_event_start_time = row['time']\n",
    "            first_event_end_time = None\n",
    "            continue\n",
    "        if row[first_event_end_col] and not row[first_event_overlap_col]:\n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col]:\n",
    "            second_event_start_time = row['time']\n",
    "            second_event_end_time = None\n",
    "        if row[second_event_end_col]:\n",
    "            second_event_end_time = row['time']\n",
    "        if row[third_event_start_col] and not row[third_event_overlap_col]:\n",
    "            third_event_start_time = row['time']\n",
    "            \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_end_time, second_event_start_time,\n",
    "                    second_event_end_time, third_event_start_time):\n",
    "            continue\n",
    "            \n",
    "        #NR\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > second_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > third_event_start_time:\n",
    "            continue\n",
    "            \n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "        if abs(second_event_start_time - third_event_start_time) < 0.00001:\n",
    "            print('{}: start time (second) event {} and start time of (third) event {} are the same: {}'.format(\n",
    "                sample_id, second_event, third_event, second_event_start_time))\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= pre_max_delay \\\n",
    "                and (third_event_start_time - second_event_end_time) <= post_max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time,\n",
    "                'third_event_start': third_event_start_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "        third_event_start_time = None\n",
    "        third_event_end_time = None\n",
    "\n",
    "    return results\n",
    "'''\n",
    "# Open single sample (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    BehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 'hunch', 3, 3),\n",
    "]\n",
    "'''\n",
    "\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    BehaviorTransition(name, 'fw', 'stim', 'fw', 3) for name in lm_data]\n",
    "\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_between(bt.sample_id, sample_df, bt.pre_event,\n",
    "                                        bt.event, bt.post_event, bt.pre_max_delay,\n",
    "                                        bt.post_max_delay)\n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "print(len(found_transitions))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[{'sample_id': '17-08-26L1-cl', 'first_event_start': 112.297, 'first_event_end': 114.702, 'second_event_start': 116.761, 'second_event_end': 120.539}, {'sample_id': '17-08-26L1-cl', 'first_event_start': 980.434, 'first_event_end': 981.808, 'second_event_start': 982.495, 'second_event_end': 985.586}, {'sample_id': '17-08-26L1-cl', 'first_event_start': 995.201, 'first_event_end': 997.948, 'second_event_start': 999.665, 'second_event_end': 1002.069}, {'sample_id': '17-08-26L1-cl', 'first_event_start': 1007.907, 'first_event_end': 1009.967, 'second_event_start': 1012.371, 'second_event_end': 1014.775}, {'sample_id': '17-08-26L1-cl', 'first_event_start': 1139.432, 'first_event_end': 1140.462, 'second_event_start': 1142.523, 'second_event_end': 1144.24}, {'sample_id': '17-08-26L1-cl', 'first_event_start': 2123.641, 'first_event_end': 2125.014, 'second_event_start': 2127.419, 'second_event_end': 2128.792}]]\n"
     ]
    }
   ],
   "source": [
    "# All behavior_transitions, which were considered before assume that subsequent behaviors \n",
    "# are not the same. For same pairwise (2 behavior), we need to access also the data, when \n",
    "# the behaviors are same\n",
    "\n",
    "class SamePairBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_next(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that will be followed by the same event <second_event>,\n",
    "    separated by <max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>.\n",
    "    \n",
    "    If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account differently to match start and end times\n",
    "    to the correct event. During iteration for one loop, we have to exclude the \n",
    "    fact that the first_event == second_event.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event) \n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    # Check for overlap between the same behaviors (print index, where 'True') and use\n",
    "    # it as a check that there is not this error in the behavior data\n",
    "    #print(sample_id, sample_df.index[sample_df['bw_overlap']].tolist())\n",
    "    \n",
    "    # Note: The overlap statement was removed. This part has to be \n",
    "    # checked if overlapping events are found in the data\n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of first behavior and remember its time.\n",
    "        if row[first_event_start_col]and first_event_start_time is None:\n",
    "            first_event_start_time = row['time']\n",
    "        if row[first_event_end_col] and first_event_end_time is None: \n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col]:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[second_event_end_col]:\n",
    "            second_event_end_time = row['time']\n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time, second_event_end_time):\n",
    "            continue\n",
    "        \n",
    "        #NR\n",
    "        if first_event_start_time == second_event_start_time:\n",
    "            continue\n",
    "        if first_event_end_time == second_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed. During iteration the first_event == second_event. \n",
    "        if (second_event_start_time == first_event_end_time): #NR\n",
    "            continue\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "# Open single sample (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    SamePairBehaviorTransition('17-08-26L1-cl', 'turn', 'turn', 3),\n",
    "    #SamePairBehaviorTransition('17-08-26L6-cl', 'fw', 'fw', 10)\n",
    "]\n",
    "\n",
    "'''\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    SamePairBehaviorTransition(name, 'bw', 'bw', 30) for name in lm_data]\n",
    "'''\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_next(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "\n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "    \n",
    "print(len(found_transitions))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the behavior_transitions were identified with given restriction, \n",
    "in the next paragraph the data will be narrowed down to celltype (or aready in cell_trace_configs)???\n",
    "and filter pattern, on top of the previous transition type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Nadine for single sample \n",
    "\n",
    "# How do I make sure that ctc is the same I use in the transitions? Can we do a class again???\n",
    "# how do I link it to the correct behavior_transition (pre, post, middle)??? \n",
    "# or at least print out whih one I am currently doing\n",
    "# if I print out first or second event start I don\"t know what is my event (first or second)?!?!\n",
    "# for the test 170826L1 I don't have enough data points!?!?!?\n",
    "# How can I print preevent/event/postevent that it does not through an error because it is not defined in all classes\n",
    "# How I get start column of those events, I am interested in?    \n",
    "# How to combine it with cellConfig?\n",
    "\n",
    "transition_events = []     \n",
    "transition_timestamp = []\n",
    "\n",
    "# Extract columns matching our cell type and the optional filter pattern. \n",
    "#Get subset of cells from CellConfig far above.\n",
    "csd = sample_df.filter(regex=ctc.get_filter_regex()) \n",
    "csd.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "csd.reset_index(inplace = True) # Add index and time = column  \n",
    "\n",
    "# Define which behavior_transition is used \n",
    "# ToDo\n",
    "\n",
    "#print(bt.pre_event)\n",
    "#print(bt.event) # Gives the name of the event\n",
    "#print(bt.post_event)\n",
    "\n",
    "for transition_list in found_transitions:\n",
    "    for transition in transition_list:\n",
    "        #print(transition['first_event_start']) # Time_stamp for first_event_start\n",
    "        #print(transition['second_event_start']) # Time_stamp for second_event_start\n",
    "        t = transition['first_event_start'] #this depends on the behavior_transition_type \n",
    "        transition_timestamp.append(t) # Append a list with transitions\n",
    "        #print(transition_timestamp)\n",
    "            \n",
    "event_trans = csd.loc[csd['time'].isin(transition_timestamp)]                \n",
    "        \n",
    "# Gives the timestamp for the event_trans\n",
    "for idx, row in event_trans.iterrows():\n",
    "    print('TP of {} ='.format(bt.event), row['time'])\n",
    "       \n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_trans). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In csd, time was set to index\n",
    "# Add index and time = column\n",
    "#\n",
    "# Set the window range left and right from the event\n",
    "left_half_window_size = 5.0 #in seconds\n",
    "right_half_window_size = 50.0\n",
    "\n",
    "# event_trans defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "for i,row in event_trans.iterrows():\n",
    "    n_behavior += 1\n",
    "    window_start = row['time'] - left_half_window_size\n",
    "    window_end = row['time'] + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end       \n",
    "    events = csd[(csd.time >= window_start) & (csd.time <= window_end)]\n",
    "    #print(events)\n",
    "             \n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    events.loc[:, 'time'] = events['time'] - row['time']\n",
    "    #print(events) #ok\n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    events.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), \n",
    "                     axis = 'columns', inplace = True) \n",
    "    #print(events) \n",
    "    # Rename time collum to time\n",
    "    events.rename(columns={ events.columns[0]: 'time' }, inplace = True)\n",
    "    #print(events)\n",
    "    transition_events.append(events) # Append a list with all event\n",
    "    #print(transition_events)\n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_transitions = transition_events.pop(0)\n",
    "\n",
    "for right_df in transition_events:\n",
    "    all_transitions = pd.merge_ordered(all_transitions, right_df, on=\"time\", how=\"outer\")\n",
    "    #print(all_transitions)\n",
    "# Resets the index as time and drops time column\n",
    "all_transitions.index = all_transitions[\"time\"]\n",
    "del all_transitions[\"time\"]   \n",
    "#print(all_transitions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Nadine for multiple sample \n",
    "\n",
    "# Not-Working\n",
    "\n",
    "transition_events = []     \n",
    "transition_timestamp = []\n",
    "\n",
    "\n",
    "# Extract columns matching our cell type and the optional filter pattern. \n",
    "#Get subset of cells from CellConfig far above. HERE copied from CellTraceConfig\n",
    "# Allows to load all samples with specific filter pattern\n",
    "\n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig(name,'basin', 'stim', 'A1') for name in lm_data]\n",
    "\n",
    "# Allow to load all samples with specific filter pattern\n",
    "# ToDo\n",
    "\n",
    "no_name = [] #List of events, with raw dff data (no interpolation or other \n",
    "                #processing done at this point). Sample_id is added to the cell name. \n",
    "\n",
    "for ctc in cell_trace_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue\n",
    "        \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)   \n",
    "  \n",
    "    \n",
    "    for transition_list in found_transitions:\n",
    "        for transition in transition_list:\n",
    "            #print(transition['first_event_start']) # Time_stamp for first_event_start\n",
    "            #print(transition['second_event_start']) # Time_stamp for second_event_start\n",
    "            t = transition['first_event_start'] #this depends on the behavior_transition_type \n",
    "            transition_timestamp.append(t) # Append a list with transitions\n",
    "            #print(transition_timestamp)\n",
    "\n",
    "            event_trans = csd.loc[csd['time'].isin(transition_timestamp)]                \n",
    "        \n",
    "            # Gives the timestamp for the event_trans\n",
    "    for idx, row in event_trans.iterrows():\n",
    "        print('TP of {} ='.format(bt.event), row['time'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: HERE FOR PLOTTING THE ALIGNED EVENT after behavioral_transition filter\n",
    "\n",
    "# Plotting for multi-events (all_transitions) (raw_dff_data)\n",
    "# If a dataframe with NANs is plotted, use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=1.0, fov=(-5, 30, -0.05, 0.8), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "sub1 = fig.add_subplot(111)\n",
    "all_transitions.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "#ToDo\n",
    "#sub2 = fig.add_subplot(212)\n",
    "#all_transitions_avg_df.plot(ax=sub2, color = 'k', label = ctc.cell_type)\n",
    "#aligned_layout_plot(sub2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(event_df)) #prints header\n",
    "#rounded time is only visual, I still get several 'same' tp\n",
    "#sample_df.round({'time' : 1})\n",
    "   # Round time on 1 or 2nd decimal\n",
    "    # the df.round({'time' : 1}) doesn't work if to many decimals\n",
    "    #decimals = 2    \n",
    "    #timestamp_df['time'] = timestamp_df['time'].apply(lambda x: round(x, decimals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        for k in transition_timestamp:\n",
    "            a = csd.loc[csd['time'].isin([k])]\n",
    "            print(a)  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import numpy.linalg as linalg\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "import pylab as pl\n",
    "%pylab inline\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "from itertools import combinations\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA (Kristin Brandson)\n",
    "\n",
    "#ndims = 1764 #timepoints, 'Y'\n",
    "#nobs = 1090 #cells, 'X'\n",
    "#maxnpcs = np.minimum(ndims,nobs-1) #number of max PCs\n",
    "\n",
    "# subtract off the mean\n",
    "databar = all_df - np.mean(all_df,0)\n",
    "u,d,v = linalg.svd(databar,full_matrices=0)\n",
    "\n",
    "# first eigenvector:\n",
    "v[0,:]\n",
    "# second eigenvector:\n",
    "v[1,:]\n",
    "\n",
    "# third eigenvector:\n",
    "v[2,:]    \n",
    "\n",
    "#print(maxnpcs)\n",
    "#print(databar.shape)\n",
    "#print(u.shape) #orthogonal of matrix\n",
    "#print(d.shape) #orthogonal of matrix\n",
    "#print(v.shape) #v gives me the dimension of the PCs \n",
    "\n",
    "#transpose matrix\n",
    "#vt = np.transpose(v)\n",
    "#print(vt.shape)\n",
    "#print(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
