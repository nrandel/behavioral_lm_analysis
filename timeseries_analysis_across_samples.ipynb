{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: behavior annotation with Chris's fran code are zero-based!!!\n",
    "\n",
    "addition to the code: \n",
    "\n",
    "1) read in a single/ few data sets and read all datasets (for cellConfig and behavior_transition)\n",
    "\n",
    "2) check if I really have solved the timestamp issue\n",
    "\n",
    "3) check if all csv files exists\n",
    "\n",
    "4)!!! ratio to entcounter for less activity in the end\n",
    "\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was build for analysis of neuronal avtivity data in combination with behavioral annotations. \n",
    "\n",
    "Multiple csv-files (behavior, dff, time_stamp, neuronal activity (lm-data)) are combined in a single dataframe per experiment (sample_df). The identity of the sample is kept by introducing a sample-ID (date, number of sample) and an experiment-ID (imaging acquisition type <close and open loop>). Kind of behavior is predefined and extended with 'quiet'. Data of behavior, as well time-data are combined with all lm data. To define the start end end, as well accoiunt for overlapping events of the same behavior, a start, end, and overlap column is added for each behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries and implement error messages (Default should be True)\n",
    "# To make the plot in the notebook and not in an extra window\n",
    "%matplotlib notebook \n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import re\n",
    "\n",
    "error_on_missing_timestamps = False\n",
    "error_on_time_light_mismatch = False\n",
    "error_on_time_behavior_mismatch = False\n",
    "error_on_missing_behaviors = False\n",
    "error_on_invalid_behavior_range = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open multiple .csv from single directory. Define existing behaviors. Define sample_ID and experiment_ID.\n",
    "\n",
    "#directory for behavior data\n",
    "behavior_directory = r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/behavior_csv/' # directory for behavioral annotations\n",
    "behavior_files = glob.glob(os.path.join(behavior_directory, \"*.csv\")) #join pathname with filename\n",
    "\n",
    "# Behavior columns available in CSV files\n",
    "available_behaviors = ('fw', 'bw', 'stim', 'hunch', 'turn', 'other', 'HP', 'left turn', 'right turn')\n",
    "\n",
    "#Regular expression (define the expression filenames are searched for)\n",
    "#'.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "#'+' 1>> occurences, 'L' here character from filename\n",
    "#() outcome here: 2 groups, useful for extraction\n",
    "#[] optional list, eg 1 --> 1\n",
    "#? character non or once \n",
    "\n",
    "#Behavior reg-ex (regular expression)\n",
    "behavior_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-behavior-(.+).csv')\n",
    "\n",
    "# Function: readall_behavior iterates through all csv (sorted) \n",
    "# and appends the files into the list (ls) and returns dictionary\n",
    "def readall_behavior(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample \n",
    "        # (cl (closeloop, RGECO/ Chronos), ol (openloop, RGECO/ Chronos), \n",
    "        # blocks (Raghav: GCaMP/Chrimson))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        # Look for filename_components, which are true for pattern\n",
    "        match = behavior_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        #define filename_components sample_id (first group), and exp_id (sec group)\n",
    "        part_sample_id, _, exp_id = filename_components         \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ';')\n",
    "        df.fillna(0, inplace=True) #replace NaN with zero\n",
    "        df['sample_id'] = sample_id  #add sample_id column\n",
    "        df['exp_id'] = exp_id #add exp_id column\n",
    "        data[sample_id] = df\n",
    "        #Count 'True' for each column ('behavior') in each single behavior.csv)\n",
    "        #print(filename, df[df == 1].count()) \n",
    "        #print(df)\n",
    "    return data\n",
    "\n",
    "behavior_data = readall_behavior(behavior_files)\n",
    "#print(behavior_data['17-08-26L3-cl'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START           11\n",
      "END              0\n",
      "fw            4636\n",
      "bw             859\n",
      "stim           277\n",
      "hunch          431\n",
      "turn          2044\n",
      "other          173\n",
      "HP             729\n",
      "left turn     1037\n",
      "right turn    1008\n",
      "sample_id        0\n",
      "exp_id           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Frequency of each behavior in all imported behavior.csv by using the returned 'ls' from \n",
    "# the function readAll: concatenate the 'behavior_files' (global variable). 'True' for each \n",
    "# column ('behavior_type') in the concatenated file (df_behavior).\n",
    "# Sorting has to be = False (warning message without 'sort')\n",
    "df_behavior = pd.concat(behavior_data.values(), axis = 0, ignore_index = True, sort = False) #add sorting\n",
    "print(df_behavior[df_behavior == 1].count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and merge fluorescence data: Several LM files for the same sample_id exists, but differ in cell_id).\n",
    "# List of LM data with two extra columns: sample_id and cell_id\n",
    "\n",
    "    #Mapping of sampleID vs lists of LM dataframes\n",
    "    #Convert list to a single dataframe\n",
    "    #Map with df_behavior (later done)\n",
    "\n",
    "# Open LM files from different directories\n",
    "lightmicroscope_directories = [r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Basin_traces/', \n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/Handle-like_Traces',\n",
    "                               r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/a00c_traces'\n",
    "                              ] \n",
    "\n",
    "# Iterate through LM data and extend files in a list from within and between directory and \n",
    "# build a list of files from all directories\n",
    "# (Note: append would 'extend' lists and not single files)\n",
    "lightmicroscope_files = []\n",
    "for d in lightmicroscope_directories:\n",
    "    lightmicroscope_files.extend(\n",
    "        glob.glob(os.path.join(d, \"*.csv\"))) #join pathname with filename, \n",
    "                                                                       \n",
    "# Regular expression (define the expression filenames are searched for)\n",
    "# '.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "# '+' 1>> occurences, 'L' here character from filename\n",
    "# () outcome here: 2 groups, useful for extraction\n",
    "\n",
    "# Lightmicroscopic data reg-ex (regular expression)\n",
    "lightmicroscope_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-(.*)-(.*).csv')\n",
    "\n",
    "# Function: readall_lm iterates through all LM_csv (sorted) \n",
    "# and returns a dictionary{key:value} \n",
    "# samples = {sample_id:cell-id}\n",
    "def readall_lm(all_files):\n",
    "    samples = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # Larva ID: YY-MM-DDLX, DETAIL = cell_id\n",
    "        #look for filename_components, which are true for pattern\n",
    "        match = lightmicroscope_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, cell_id, exp_id = filename_components\n",
    "        \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        # Read LM.files \n",
    "        df = pd.read_csv(filename, index_col=None, header=0, delimiter = ',')\n",
    "        # Replace NaN with zero\n",
    "        df.fillna(0, inplace=True)\n",
    "        \n",
    "        # Add cellname to each column as prefix\n",
    "        # lambda is a non defined function (longer version: def lambda(x):)\n",
    "        # Rename of columns after the format cell_id, name) eg: Basin A9\n",
    "        # inplace = True: column names are overwritten (if False: new dataframe)\n",
    "        df.rename(lambda x: '{}_{}'.format(cell_id, x), axis = 'columns', inplace = True)\n",
    "        \n",
    "        # Get the sample_id (key) from the dictionary? to make a list [sample_cells] and \n",
    "        # if sample_id exists, append the list\n",
    "        # if sample_id does not exists, start a new list\n",
    "        # reminder: there can be several cell_id per sample_id\n",
    "        sample_cells = samples.get(sample_id)\n",
    "        if not sample_cells:\n",
    "            samples[sample_id] = sample_cells = {\n",
    "                'data': [],\n",
    "                'exp_id': exp_id,\n",
    "            }\n",
    "        sample_cells['data'].append(df)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "lm_samples = readall_lm(lightmicroscope_files)\n",
    "\n",
    "# New dictionary: lm_data{} to build a single dataframe with all cell_ids combined \n",
    "# for a single sample. Iterate over dict (samples)? and data from same sample in \n",
    "# one dataframe. \n",
    "# df.items iterate over pairs and build a list\n",
    "\n",
    "lm_data = {}\n",
    "\n",
    "# Iterate over all light samples and merge all found files\n",
    "# for each sample into a single data frame (per sample)\n",
    "for sample_id, sample_info in lm_samples.items():\n",
    "    cells_dataframes = sample_info['data']\n",
    "    #check if number of cells >= 1\n",
    "    if not cells_dataframes:\n",
    "        raise ValueError('No cells found for sample {}'.format(sample_id))\n",
    "    #first element in the list\n",
    "    lm_df = None\n",
    "\n",
    "    #iteration through other df\n",
    "    for cdf in cells_dataframes:\n",
    "        if lm_df is None:\n",
    "            lm_df = cdf\n",
    "        else:\n",
    "            if len(lm_df.index) != len(cdf.index):\n",
    "                raise ValueError('Data frame frame to merge has not same row count as target', sample_id)\n",
    "            lm_df = pd.merge(lm_df, cdf, left_index = True, right_index = True)\n",
    "            \n",
    "    lm_df['sample_id'] = sample_id  #add sample_id column\n",
    "    lm_df['exp_id'] = sample_info['exp_id']\n",
    "    lm_data[sample_id] = lm_df\n",
    "#print(list(lm_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import txt-files from of the absolute time/frame from the Ca-imaging (lm-data). \n",
    "# All txt-files have to be transposed, which is a memory intensive step. After the \n",
    "# data are complete, the transposed files should be exported (ToDo). Time-data are \n",
    "# combined with sample-ID and experiment-ID.\n",
    "\n",
    "timelapse_directory =(r'/Users/randeln/Documents/Zlatic_lab/close-loop/Notes/timelapse/') \n",
    "timelapse_files = glob.glob(os.path.join(timelapse_directory, \"*.txt\")) #join pathname with filename\n",
    "\n",
    "\n",
    "# Regular expression (define the expression filenames are searched for)\n",
    "# '.' single character, matched everything, '*' 0>> occurences, '/' path delimiter, '\\d' 0-9 digit,\n",
    "# '+' 1>> occurences, 'L' here character from filename\n",
    "# () outcome here: 2 groups, useful for extraction\n",
    "# [] optional list, eg 1 --> 1\n",
    "# ? character non or once \n",
    "\n",
    "# Behavior reg-ex (regular expression)\n",
    "time_sample_re = re.compile('.*/(\\d\\d-\\d\\d-\\d\\dL\\d+(-\\d+)?)-time-(.+).txt')\n",
    "\n",
    "# Function: readall_timelapse iterates through all txt (sorted) and appends the \n",
    "# files into the dict (data) and returns ls\n",
    "def readall_time(all_files, printit=False):\n",
    "    data = {}\n",
    "    for filename in sorted(all_files):\n",
    "        # Find sample ID, file name pattern: YY-MM-DDLXDETAIL.csv,\n",
    "        # exp_id = DETAIL: several measurements of same sample (cl (closeloop), ol (openloop), blocks (Raghav))\n",
    "        # Larva ID: YY-MM-DDLX\n",
    "        #look for filename_components, which are true for pattern\n",
    "        match = time_sample_re.match(filename)\n",
    "        if not match:\n",
    "            raise ValueError('Unexpected filename format: {}'.format(filename))\n",
    "        filename_components = match.groups()\n",
    "        part_sample_id, _, exp_id = filename_components #define filename_components sample_id (first group), and exp_id (sec group)  \n",
    "        sample_id = \"{}-{}\".format(part_sample_id, exp_id)\n",
    "        \n",
    "        df = pd.read_csv(filename, header=1, index_col=None, delim_whitespace = True)\n",
    "        df = df.T #transposing because read_csv imports as row\n",
    "        df = df.reset_index() #transpose function sets data as index\n",
    "        df.rename(columns={'index':'time'}, inplace=True) #rename reset index column to time\n",
    "        df['time'] = df.time.astype(float)\n",
    "        data[sample_id] = df\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache file found, recomputing\n"
     ]
    }
   ],
   "source": [
    "timelapse_cache = 'timelapse.cache'\n",
    "\n",
    "try:\n",
    "    with open(timelapse_cache, 'r') as timelapse_cache_file:\n",
    "        # TODO\n",
    "        cache_data = timelapse_cache_file.read()\n",
    "        time_data = ast.literal_eval(cache_data)\n",
    "except FileNotFoundError as e:\n",
    "    print('No cache file found, recomputing')\n",
    "    # No cache file found, recompute\n",
    "    time_data = readall_time(timelapse_files)\n",
    "    # Write cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data = {}\n",
    "\n",
    "# Time data are merged into light data and checked if number length of lm = timestamp.  \n",
    "# Due to technical conditions, some time.txt-file have too many or not enough time data compared\n",
    "# to the corresponding LM data. The discrepancy is fixed by either dropping the extra timepoints or \n",
    "# by taking the average of the difference between each timepoint and extend the dataframe. \n",
    "# The first 10 timepoints are not included to account for instability of the microscope in \n",
    "# the beginning due to the moving parts. \n",
    "# Maximal difference between timepoints fyi.\n",
    "\n",
    "for sample_id, sample_df in lm_data.items():\n",
    "    # Add time stamps to data frame of current sample by merging\n",
    "    # The time data frame for the current sample, which is expected\n",
    "    # to match the light data (based on index).\n",
    "    timestamp_df = time_data.get(sample_id)\n",
    "    if timestamp_df is None:\n",
    "        msg = '{}: could not find timestamp data for sample'.format(sample_id)\n",
    "        if error_on_missing_timestamps:\n",
    "            raise ValueError(msg)\n",
    "        # Ignore, if missing data shouldn't cancel the whole process.\n",
    "        print(msg)\n",
    "        continue\n",
    "        \n",
    "    n_timestamps = len(timestamp_df)\n",
    "    n_lightdata = len(sample_df)\n",
    "    \n",
    "    # The timestamp and light recordings are done by different systems.\n",
    "    # This can cause the existence of additional time points/ or missing time points in a\n",
    "    # dataset, which will be filtered out in the merge operation below.\n",
    "    if n_lightdata != n_timestamps:\n",
    "        msg = '{}: time data ({} entries) doesn\\'t match light data ({} entries)'.format(\n",
    "                sample_id, n_timestamps, n_lightdata)\n",
    "        if error_on_time_light_mismatch:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        diffs = np.diff(timestamp_df['time'])[10:] #from 10th row onwards\n",
    "        diffs_avg = diffs.mean(axis=0)\n",
    "        #diff between timedata and lightdata\n",
    "        missing_data = len(sample_df) - len(timestamp_df)\n",
    "        \n",
    "        #add 'diffs_avg' to fill in missing_timedata\n",
    "        if missing_data > 0:\n",
    "            last_valid_index = len(timestamp_df) - 1\n",
    "            last_timestamp = timestamp_df.iloc[last_valid_index]['time']\n",
    "            if pd.isna(last_timestamp):\n",
    "                raise ValueError('Unexpected last valid timestamp for sample {} at index {}'.format(\n",
    "                        sample_id, last_valid_index))\n",
    "            for i in range(0, missing_data):\n",
    "                last_valid_index += 1\n",
    "                timestamp_df.loc[last_valid_index] = timestamp_df.iloc[last_valid_index - 1]['time'] + diffs_avg\n",
    "        elif missing_data < 0:\n",
    "            drop_start = len(timestamp_df) + missing_data\n",
    "            drop_end = len(timestamp_df)\n",
    "            timestamp_df.drop(list(range(drop_start, drop_end)))\n",
    "\n",
    "    # Merge timedata into light data\n",
    "    # Use an 'inner' join/merge to exclude time points that don't have matching light data.\n",
    "    new_sample_df = pd.merge(sample_df, timestamp_df, left_index = True, right_index = True, how='inner')\n",
    "    \n",
    "    # Store newly created data frame for sample (dictionary)\n",
    "    sample_data[sample_id] = new_sample_df\n",
    "    \n",
    "print('Matched {} light data sets with their respective time points'.format(len(sample_data)))\n",
    "\n",
    "# Max.diffs for timestamps\n",
    "diffs = np.diff(timestamp_df['time'])[10:] #from 10th row onwards\n",
    "mx = diffs.max()\n",
    "#print(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine behavior data with light data into a single data frame\n",
    "# per sample ID. To do so, add behavior data to light data frames,\n",
    "# because the light data is already organizes by frame. To accomodate\n",
    "# frame ranges without an behavior data, a column named \"quiet\" is\n",
    "# added which is True in these cases and False otherwise. Additionally,\n",
    "# for each behavior column, a behavior start and end column as well as\n",
    "# an overlap column is added so that parallel and successive behaviors\n",
    "# of the same type can be differentiated.\n",
    "\n",
    "for sample_id, sample_df in sample_data.items():\n",
    "    sample_behavior = behavior_data.get(sample_id)\n",
    "    if sample_behavior is None:\n",
    "        msg = 'Could not find behavior data for sample \"{}\"'.format(sample_id)\n",
    "        if error_on_missing_behaviors:\n",
    "            raise ValueError(msg)\n",
    "        print(msg)\n",
    "        continue\n",
    "\n",
    "    # Add extra columns for behavior\n",
    "    for behavior in available_behaviors:\n",
    "        sample_df[behavior] = False\n",
    "        sample_df['{}_start'.format(behavior)] = False\n",
    "        sample_df['{}_end'.format(behavior)] = False\n",
    "        sample_df['{}_overlap'.format(behavior)] = False\n",
    "    \n",
    "    # Add 'quiet' column. Set it initially to True and mark frames\n",
    "    # with actual behavior as quiet = False.\n",
    "    sample_df['quiet'] = True\n",
    "    \n",
    "    n_light_entries = len(sample_df)\n",
    "\n",
    "    # Iterate over behavior data and add data to target data frame\n",
    "    for i, row in sample_behavior.iterrows():\n",
    "        # Start ane end are 1-based, make them 0-based\n",
    "        start = int(row['START'])\n",
    "        end = int(row['END'])\n",
    "        \n",
    "        if type(row['START']) == str:\n",
    "            print(sample_id)\n",
    "            print(start, end)\n",
    "        \n",
    "        if start >= end:\n",
    "            msg = \"{}: start ({}) needs to be strictly smaller than end ({})\".format(sample_id, start, end)\n",
    "            if error_on_invalid_behavior_range:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "        \n",
    "        # Make sure we capture start/end times that are a fractional number.\n",
    "        if row['START'] - start > 0 or row['END'] - end > 0:\n",
    "            raise ValueError('{}: start and end frame number can\\'t contain fractions'.format(sample_id))\n",
    "            \n",
    "        # Ignore behavior entries with an end frame higher than available light data.\n",
    "        # The behavior data is one-based, which is why a strict larger than test should\n",
    "        # be correct.\n",
    "        if end > n_light_entries:\n",
    "            msg = 'Sample: {} - Behavior row with range {}-{} exceeds light time points ({}): {}'.format(\n",
    "                sample_id, start, end, n_light_entries, row)\n",
    "            if error_on_time_behavior_mismatch:\n",
    "                raise ValueError(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "            \n",
    "        # Find behavior observed in row\n",
    "        observed_behaviors = []\n",
    "        for behavior in available_behaviors:\n",
    "            if row[behavior]:\n",
    "                observed_behaviors.append(behavior)\n",
    "        \n",
    "        # We assume that not more than two behaviors are observed at the same time\n",
    "        if len(observed_behaviors) > 2:\n",
    "            raise ValueError('Found multiple behaviors in row {} of sample {}'.format(i, sample_id))\n",
    "        \n",
    "        # Add observed behavior information to target data frames in all\n",
    "        # rows in behavior range.\n",
    "        for b in observed_behaviors:\n",
    "            # Iterate over frames valid for current behavior. Every valid\n",
    "            # frame is mapped into the canonical (light/cell) data frame,\n",
    "            # which is 0-indexed.\n",
    "            for j in range(start, end + 1):\n",
    "                # Behavior ranges are 1-indexed\n",
    "                current_frame = j - 1\n",
    "                # If the current behavior has already been observed at this frame,\n",
    "                # set overlap to True, because we are about to mark this behavior\n",
    "                # again as observed for this frame.\n",
    "                if sample_df.at[current_frame, b]:\n",
    "                    sample_df.at[current_frame, '{}_overlap'.format(b)] = True\n",
    "                else:\n",
    "                    sample_df.at[current_frame, b] = True\n",
    "                \n",
    "                # Mark this row as not quiet, because we observed\n",
    "                # a behavior in the current frame.\n",
    "                sample_df.at[current_frame, 'quiet'] = False\n",
    "\n",
    "            sample_df.at[start - 1, '{}_start'.format(b)] = True\n",
    "            sample_df.at[end - 1, '{}_end'.format(b)] = True\n",
    "            \n",
    "    # Mark quiet ranges with _start, _end and _overlap. By definion,\n",
    "    # quiet_overlap is always False.\n",
    "    sample_df['quiet_start'] = False\n",
    "    sample_df['quiet_end'] = False\n",
    "    sample_df['quiet_overlap'] = False\n",
    "    last_sample_idx = n_light_entries - 1\n",
    "    for i, row in sample_df.iterrows():\n",
    "        sample_df.at[i, 'quiet_start'] = row['quiet'] and (i == 0 or not sample_df.at[i - 1, 'quiet'])\n",
    "        sample_df.at[i, 'quiet_end'] = row['quiet'] and (i == last_sample_idx or not sample_df.at[i + 1, 'quiet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above generated dataframe per sample (sample_df) including lm data, behavior, time and sample_id/exp_id, and the extended cell-names and extra behavioral columns can be analysed in the following section.\n",
    "\n",
    "Class will be defined, including sample_id, cell_type, event (type), and filter pattern. This \n",
    "allows to extract information throughout all samples about activity pattern (lm-data) \n",
    "of a specific celltype (including sub-type) and event_start (=behavior). \n",
    "\n",
    "For single sample: cell_subset_df \n",
    "        allows visualisation of whole experimantal time, and extract/ visualized information around a specific\n",
    "        event, which are aligned and normalized (event_start = zero). An adjustable time-window around the \n",
    "        event_start of interested is included. \n",
    "    - avg, max, min, stdev and sem can be direct calculated from the data (for whole timeseries)\n",
    "    - But if events are aligned, same problem as with multiple samples\n",
    "        \n",
    "For multiple combined samples (no additional processing): all_events\n",
    "        extract/ visualized information around a specific\n",
    "        event, which are aligned and normalized (event_start = zero). An adjustable time-window around the\n",
    "        event_start of interested is included. \n",
    "\n",
    "    The samples are imaged (Ca-imaging) with different imaging speed, and therefore a direct comparison between the \n",
    "    samples is not possible. Following things have to be considered:\n",
    "    \n",
    "                    ToDo!!!\n",
    "    \n",
    "        - If the data are analysed as raw data (all_events) there are many NaN in the data set, and also the same \n",
    "        timestamp is duplicated for each individual sample\n",
    "        - avg, max, min, stdev and sem can not be direct calculated from the data, because(?) vergessen:(\n",
    "        \n",
    "        - For some analysis (), the samples should be aligned: For now we use interpolation after index. \n",
    "        THAT HAS TO BE CHECKED!!!  \n",
    "        \n",
    "        - alternatives: underlying fitting curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a class with sample_id, cell_type, event_name and filter_pattern\n",
    "class CellTraceConfig:\n",
    "    \n",
    "    def __init__(self, sample_id, cell_type, event_name, filter_pattern=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell_type = cell_type\n",
    "        self.event_name = event_name\n",
    "        self.filter_pattern = filter_pattern\n",
    "        \n",
    "    def get_filter_regex(self):\n",
    "        filter_regex = '^{}_'.format(self.cell_type)\n",
    "        if self.filter_pattern:\n",
    "            filter_regex += '.*{}.*'.format(self.filter_pattern)\n",
    "        return filter_regex\n",
    "    \n",
    "    def get_event_start_col(self):\n",
    "        return '{}_start'.format(self.event_name)\n",
    "\n",
    "    def add_event_time_points_to_plot(self, source_df, plot):\n",
    "        for idx, row in source_df.iterrows():\n",
    "            plot.annotate(self.event_name, xy=(row['time'], 1))\n",
    "            plt.axvline(row['time'], color='k', linestyle='-')  \n",
    "            \n",
    "#Define a class with sample_id, cell_type, event_time and filter_pattern\n",
    "class CellTransConfig:\n",
    "    \n",
    "    def __init__(self, sample_id, cell_type, event_time, filter_pattern=None):\n",
    "        self.sample_id = sample_id\n",
    "        self.cell_type = cell_type\n",
    "        self.event_time = event_time\n",
    "        self.filter_pattern = filter_pattern\n",
    "        \n",
    "    def get_filter_regex(self):\n",
    "        filter_regex = '^{}_'.format(self.cell_type)\n",
    "        if self.filter_pattern:\n",
    "            filter_regex += '.*{}.*'.format(self.filter_pattern)\n",
    "        return filter_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allows to load specific samples (single samples) with specific filter pattern\n",
    "\n",
    "cell_trace_configs = [\n",
    "    #CellTraceConfig('17-08-26L6-cl', 'basin', 'stim'),\n",
    "    CellTraceConfig('17-08-26L5-cl', 'A00c', 'stim', 'mid'),\n",
    "    CellTraceConfig('17-08-26L2-cl', 'A00c', 'stim', 'mid'),\n",
    "    #CellTraceConfig('17-08-23L2-cl', 'A00c', 'stim', 'mid'),\n",
    "    CellTraceConfig('17-08-26L6-cl', 'A00c', 'stim', 'mid'),\n",
    "    #CellTraceConfig('17-08-24L2-1-cl', 'A00c', 'stim'),\n",
    "    #CellTraceConfig('17-08-24L2-2-cl', 'A00c', 'stim', 'mid'),\n",
    "    #CellTraceConfig('17-08-24L5-cl', 'A00c', 'stim', 'mid')\n",
    "]\n",
    "'''\n",
    "# Allows to load all samples with specific filter pattern\n",
    "cell_trace_configs = [\n",
    "    CellTraceConfig(name,'A00c', 'fw') for name in lm_data]\n",
    "'''\n",
    "#put '' [empty string] if you dont want any cell type\n",
    "\n",
    "# Allow to load all samples with specific filter pattern\n",
    "# ToDo\n",
    "\n",
    "all_events = [] #List of events, with raw dff data (no interpolation or other \n",
    "                #processing done at this point). Sample_id is added to the cell name. \n",
    "\n",
    "for ctc in cell_trace_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(ctc.sample_id, cell_subset_df)   \n",
    "    # Get rows where current event starts.\n",
    "    event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()]]\n",
    "    # Gives the timestamp for the event_df (start)\n",
    "    for idx, row in event_df.iterrows():\n",
    "        print('TP of {} ='.format(ctc.event_name), row['time'])\n",
    "        \n",
    "    # Extract for specific time window and align several events. \n",
    "    # Define timepoints pre and post an event (event_df). \n",
    "    # This works for single sample or multiple samples aligned \n",
    "    # Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "    # Add index and time = column\n",
    "\n",
    "    # Set the window range left and right from the event\n",
    "    left_half_window_size = 10.0 #in seconds\n",
    "    right_half_window_size = 50.0\n",
    "\n",
    "    # Event_df defined in pargraph before \n",
    "    windows = []\n",
    "    n_behavior = 0\n",
    "    for i,row in event_df.iterrows():\n",
    "        n_behavior += 1\n",
    "        window_start = row['time'] - left_half_window_size\n",
    "        window_end = row['time'] + right_half_window_size\n",
    "        \n",
    "        # Get subset of rows between window_start and window_end       \n",
    "        event = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "        # Normalizing the data to align on beginning of selected\n",
    "        # behavior (event_df = Zero) by substracting events in window\n",
    "        # around start of event of interest from start of event interest.\n",
    "        # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "        event.loc[:, 'time'] = event['time'] - row['time']\n",
    "\n",
    "        # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "        event.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), \n",
    "                     axis = 'columns', inplace = True) \n",
    "\n",
    "        # Rename time collum to time\n",
    "        event.rename(columns={ event.columns[0]: 'time' }, inplace = True) \n",
    "        all_events.append(event) # Append a list with all event\n",
    "        \n",
    "        #Round (NR)\n",
    "        #decimals = 1    \n",
    "        #event['time'] = event['time'].apply(lambda x: round(x, decimals))\n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_df = all_events.pop(0)\n",
    "for right_df in all_events:\n",
    "    all_df = pd.merge_ordered(all_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column (sollte spaeter kommen)\n",
    "all_df.index = all_df[\"time\"]\n",
    "del all_df[\"time\"]        \n",
    "#print(all_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_df = all_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single sample, over whole experimental time\n",
    "- does not have much meaningfulness\n",
    "\n",
    "single sample, aligned for events\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # Single sample -analysis\n",
    "    # For single sample over the whole experimental time\n",
    "    # Note: multiple sample-comparison need more pre-processing(see below)\n",
    "    # Calculate min, max, avg, stddev, sem from cell_subset_df (defined earlier)\n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    del cell_subset_df['time'] # delete time_column\n",
    "    cell_subset_df.index.name = None # delete index name\n",
    "    cell_avg_df = cell_subset_df.mean(axis=1)\n",
    "    cell_min_df = cell_subset_df.min(axis=1)\n",
    "    cell_max_df = cell_subset_df.max(axis=1)\n",
    "    # Standard deviation (distribution)\n",
    "    cell_std_df = cell_subset_df.std(axis = 1)\n",
    "    #standard error of mean\n",
    "    cell_sem_df = cell_subset_df.sem(axis = 1)\n",
    "    #print(ctc.sample_id, cell_avg_df) #OK\n",
    "\n",
    "\n",
    "# For single or multiple sample, aligned for certain event\n",
    "#Average is wrongly applied, because it avg all events and all cells pro tp\n",
    "# Good! NaN are ignored and the correct avg is calculated\n",
    "all_cell_avg_df = int_all_df.mean(axis=1) # Interpolated data used\n",
    "all_cell_min_df = int_all_df.min(axis=1)\n",
    "all_cell_max_df = int_all_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_cell_std_df = int_all_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_cell_sem_df = int_all_df.sem(axis = 1)\n",
    "#print(all_cell_avg_df) #wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "                        #and not avg over all cells per timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# Plotting - single sample \n",
    "def layout_plot(plot, tick_spacing=100, fov=(0, 2500, 0, 1.2), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "# Get rows where current event is active and draw a vertical \n",
    "# line to indicate the event in the plot\n",
    "event_df = sample_df[sample_df.loc[:,ctc.get_event_start_col()] == 1]\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor(\"white\")\n",
    "\n",
    "# Plot all cells from cell_subset_df over entire time (specified in Cell_Trace_Config).\n",
    "sub1 = fig.add_subplot(211) #211\n",
    "cell_subset_df.plot(ax=sub1)\n",
    "ctc.add_event_time_points_to_plot(event_df, sub1)\n",
    "layout_plot(sub1)\n",
    "\n",
    "# Avg, min, max, std-dev for multiple cells in single sample over whole time\n",
    "sub2 = fig.add_subplot(212)#212\n",
    "ctc.add_event_time_points_to_plot(event_df, sub2)\n",
    "cell_avg_df.plot(ax=sub2, color = 'g', label = ctc.cell_type, linewidth=1)\n",
    "cell_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "cell_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#cell_avg_df.plot.line(yerr=cell_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "#cell_avg_df.plot.line(yerr=cell_sem_df, ax=sub2, color = 'c', alpha = 0.1)\n",
    "layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note: HERE FOR PLOTTING THE ALIGNED EVENT, INDEPENDENT OF PRO AND / OR POST_EVENT\n",
    "# (should be after transition_event)\n",
    "\n",
    "# Plotting for multi-events (all_df) (raw_dff_data)\n",
    "# If a dataframe with NANs is plotted, use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=0.5, fov=(-20, 50, -0.05, 1.9), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "sub1 = fig.add_subplot(211)\n",
    "all_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(212)\n",
    "all_cell_avg_df.plot(ax=sub2, color = 'k', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_cell_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_cell_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "all_cell_avg_df.plot.line(yerr=all_cell_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "#all_cell_avg_df.plot.line(yerr=all_cell_sem_df, ax=sub2, color = 'c', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part extract the information about behavior transition under certain limitation:\n",
    "1) Find second_behavior, and extract information if the defined first_behavior happens within a max_delay.\n",
    "2) Find first_behavior, and extract information if the defined second_behavior happens within a max_delay.\n",
    "3) Find second_behavior, and extract information if the defined first_behavior and third_behavior \n",
    "   happens within a max_delays.\n",
    "4) Find first_behavior, and extract information if the same second_behavior happens within a max_delay. Note: So far no overlap cases detected. Code for overlap cases could not be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_after(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <second_event> that follow the event <first_event>,\n",
    "    separated by <max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>.\n",
    "    \n",
    "    TODO: If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account to match start and end times\n",
    "    to the correct event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for end of first behavior and remember its time.\n",
    "        if row[first_event_start_col]:\n",
    "            first_event_start_time = row['time']\n",
    "        if row[first_event_end_col] and not row[first_event_overlap_col]:\n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col]:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[second_event_end_col]:\n",
    "            second_event_end_time = row['time']\n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time, second_event_end_time):\n",
    "            continue\n",
    "            \n",
    "        #NR\n",
    "        # Define rules for event_start_time and event_end_time\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > second_event_end_time:\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "behavior_transitions = [\n",
    "    PreBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "    #PreBehaviorTransition('17-08-26L6-cl', 'turn', 'bw', 3)\n",
    "]\n",
    "\n",
    "'''\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    PreBehaviorTransition(name,'fw', 'bw', 10) for name in lm_data]\n",
    "\n",
    "'''\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_after(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "    found_transitions.append(transitions)\n",
    "\n",
    "print(len(found_transitions)) #number of data sets not the actual stim\n",
    "print(len(transitions)) #not what I want ToDo\n",
    "print(found_transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, event, post_event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.post_event = post_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_before(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that is followed by the event <second_event>,\n",
    "    separated by <max_delay> time. The end of <second_event> is expected\n",
    "    to happen strictly after the end of <first_event>. The start time\n",
    "    of <second_event> however can overlap with the end time of <first_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The start time of <second_event> can be before, at or after the\n",
    "    end of <first_event>.\n",
    "    \n",
    "    TODO: If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account to match start and end times\n",
    "    to the correct event.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of second behavior and remember its time.\n",
    "        if row[second_event_start_col] and not row[second_event_overlap_col]:\n",
    "            #print(\"{} starts at {}\".format(second_event, row[\"time\"]))\n",
    "            second_event_start_time = row['time']\n",
    "        if row[first_event_end_col]:\n",
    "            #print(\"{} ends at {}\".format(first_event, row[\"time\"]))\n",
    "            first_event_end_time = row['time']\n",
    "        if row[first_event_start_col]:\n",
    "            #print(\"{} starts at {}\".format(first_event, row[\"time\"]))\n",
    "            first_event_start_time = row['time']\n",
    "        for column in sample_df.columns:\n",
    "            if (first_event_start_time is not None and\n",
    "                column.endswith(\"_start\") and\n",
    "                column != first_event_start_col and\n",
    "                column != second_event_start_col and\n",
    "                first_event not in column and\n",
    "                second_event not in column):\n",
    "                if row[column]:\n",
    "                    #print(\"{} ended at {}, but then found {} at {}\".format(first_event, first_event_end_time, column, row[\"time\"]))\n",
    "                    first_event_start_time = None\n",
    "                    first_event_end_time = None\n",
    "                    second_event_start_time = None\n",
    "                    second_event_end_time = None\n",
    "                \n",
    "        \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time):\n",
    "            continue\n",
    "        \n",
    "        #NR\n",
    "        # Define rules for event_start_time and event_end_time\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        \n",
    "        # Test if first_event_start_time = second_event_start_time\n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Open single samples \n",
    "'''\n",
    "behavior_transitions = [\n",
    "    #PostBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "    #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 3),\n",
    "    #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 3),\n",
    "    PostBehaviorTransition('17-08-26L6-cl', 'turn', 'bw', 3)\n",
    "]\n",
    "'''\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    PostBehaviorTransition(name,'bw', 'stim', 3) for name in lm_data]\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_before(bt.sample_id, sample_df, bt.event, bt.post_event, bt.max_delay)\n",
    "    \n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "\n",
    "print(len(found_transitions)) #number of data sets not the actual stim\n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))\n",
    "#print(len(transitions)) \n",
    "print(found_transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Test\n",
    "def test_find_behavior_before():\n",
    "    data_columns = ['time', 'bw_start', 'bw_end', 'bw_overlap', 'fw_start', 'fw_end', 'fw_overlap', 'turn_start', 'turn_end', 'turn_overlap']\n",
    "    data = [\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [2, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [3, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [4, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [5, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    ]\n",
    "    toy_df = pd.DataFrame(data, columns = data_columns)\n",
    "    \n",
    "    behavior_transitions = [\n",
    "        #PostBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "        #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 3),\n",
    "        #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 3),\n",
    "        PostBehaviorTransition('na', 'fw', 'bw', 5)\n",
    "    ]\n",
    "    \n",
    "    found_transitions = []\n",
    "    for bt in behavior_transitions:\n",
    "        sample_df = toy_df\n",
    "        if sample_df is None:\n",
    "            raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "        transitions = find_behavior_before(bt.sample_id, sample_df, bt.event, bt.post_event, bt.max_delay)\n",
    "\n",
    "        if transitions:\n",
    "            found_transitions.append(transitions)\n",
    "\n",
    "\n",
    "    print(len(found_transitions)) #number of data sets not the actual stim\n",
    "    print(len(transitions)) \n",
    "    print(found_transitions)\n",
    "    \n",
    "test_find_behavior_before()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For post_behavior_transition events get event_time and use ceLL_trace_config to filter by celltype and pattern.\n",
    "# The results are merged and interpolated.\n",
    "cell_trans_configs = []\n",
    "all_trans_events = []\n",
    "\n",
    "for sample in found_transitions:\n",
    "    sample_ls_trans = []\n",
    "    for found_transition in sample:\n",
    "        #print(found_transition[\"sample_id\"], found_transition[\"second_event_start\"])\n",
    "        #print(found_transition[\"sample_id\"])\n",
    "        # For all behavior except stimulus\n",
    "        sample_ls_trans.append(found_transition[\"second_event_start\"]) \n",
    "        cell_trans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"A00c\", \n",
    "                                                  found_transition[\"second_event_start\"]))\n",
    "        \n",
    "        # For stimulus as first_event\n",
    "        #sample_ls_trans.append(found_transition[\"first_event_start\"]) \n",
    "        #cell_trans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"basin\", \n",
    "        #                                          found_transition[\"first_event_start\"]))\n",
    "        \n",
    "                        \n",
    "    #Find row of found_transition['second_event_start'] in sample_df\n",
    "    #sample_df = sample_data.get(found_transition[\"sample_id\"])\n",
    "    #trans_df = sample_df.loc[sample_df['time'].isin(sample_ls_trans)]\n",
    "    #trans_data[found_transition[\"sample_id\"]] = trans_df\n",
    "    #print(trans_df, found_transition[\"sample_id\"])\n",
    "\n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_df). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "# Add index and time = column\n",
    "\n",
    "\n",
    "# Set the window range left and right from the event\n",
    "left_half_window_size = 10.0 #in seconds\n",
    "right_half_window_size = 20.0\n",
    "\n",
    "# trans_df defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "\n",
    "for ctc in cell_trans_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)\n",
    "    \n",
    "    n_behavior += 1\n",
    "    window_start = ctc.event_time - left_half_window_size\n",
    "    window_end = ctc.event_time + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end       \n",
    "    trans = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "    #print(trans) #ok\n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    #trans.loc[:, 'time'] = trans['time'] - row['time']\n",
    "    trans.loc[:, 'time'] = trans['time'] - ctc.event_time\n",
    "    #print(trans) #ok\n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    #trans.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), axis = 'columns', inplace = True) \n",
    "\n",
    "    # Rename time collum to time\n",
    "    trans.rename(columns={ trans.columns[0]: 'time' }, inplace = True) \n",
    "    #print(trans) # ok\n",
    "    all_trans_events.append(trans) # Append a list with all event\n",
    "#print(all_trans_events)   \n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_trans_df = all_trans_events.pop(0)\n",
    "for right_df in all_trans_events:\n",
    "    all_trans_df = pd.merge_ordered(all_trans_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column (sollte spaeter kommen)\n",
    "all_trans_df.index = all_trans_df[\"time\"]\n",
    "del all_trans_df[\"time\"]        \n",
    "#print(all_trans_df)\n",
    "\n",
    "#NR\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_Ptrans_df = all_trans_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_trans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and stddev, min, max, sem for post_behavior_transition events\n",
    "all_Ptrans_avg_df = int_all_Ptrans_df.mean(axis=1) # Interpolated data used\n",
    "all_Ptrans_min_df = int_all_Ptrans_df.min(axis=1)\n",
    "all_Ptrans_max_df = int_all_Ptrans_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_Ptrans_std_df = int_all_Ptrans_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_Ptrans_sem_df = int_all_Ptrans_df.sem(axis = 1)\n",
    "#wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "#and not avg over all cells per timepoint\n",
    "\n",
    "# Plotting for multi-events (same_behavioral_transition)\n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=1, fov=(-10, 10, 0.0, 1.0), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "#sub1 = fig.add_subplot(211)\n",
    "#all_trans_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "#aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(111) #212\n",
    "all_Ptrans_avg_df.plot(ax=sub2, color = 'c', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_Ptrans_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Ptrans_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Ptrans_avg_df.plot.line(yerr=all_Ptrans_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "all_Ptrans_avg_df.plot.line(yerr=all_Ptrans_sem_df, ax=sub2, color = 'grey', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class BehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, post_event,\n",
    "                 pre_max_delay=0, post_max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.post_event = post_event\n",
    "        self.pre_max_delay = pre_max_delay\n",
    "        self.post_max_delay = post_max_delay\n",
    "\n",
    "def find_behavior_between(sample_id, sample_df, first_event, second_event,\n",
    "                          third_event, pre_max_delay=0, post_max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <second_event> that a) follows the event <first_event>,\n",
    "    separated by <pre_max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <pre_max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>. And b) the behavior <second_event> is followed by the\n",
    "    event <third_event>, separated by <post_max_delay> time. The end of\n",
    "    <third_event> is expected to happen strictly after the end of <second_event>.\n",
    "    The start time of <third_event> however can overlap with the end time of\n",
    "    <second_event>. In this case, the time difference is negative, and still\n",
    "    smaller than <post_max_delay>. The start time of <third_event> can be before,\n",
    "    at or after the end of <second_event>.\n",
    "    \n",
    "    The start of <first_event> is expected to happen strictly before the start\n",
    "    of <third_event>. Apart from this, both <first_event> and <third_event> can\n",
    "    overlap.\n",
    "    \n",
    "    TODO: handle overlaps when behavior types are the same.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    third_event_start_col = '{}_start'.format(third_event)\n",
    "    third_event_end_col = '{}_end'.format(third_event)\n",
    "    third_event_overlap_col = '{}_overlap'.format(third_event)\n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    third_event_start_time = None\n",
    "    third_event_end_time = None\n",
    "\n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for behaviors and remember its time.\n",
    "        if row[first_event_start_col] and first_event_start_time is None:\n",
    "            first_event_start_time = row['time']\n",
    "            first_event_end_time = None\n",
    "            continue\n",
    "        if row[first_event_end_col] and not row[first_event_overlap_col]:\n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col]:\n",
    "            second_event_start_time = row['time']\n",
    "            second_event_end_time = None\n",
    "        if row[second_event_end_col]:\n",
    "            second_event_end_time = row['time']\n",
    "        if row[third_event_start_col] and not row[third_event_overlap_col]:\n",
    "            third_event_start_time = row['time']\n",
    "   \n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_end_time, second_event_start_time,\n",
    "                    second_event_end_time, third_event_start_time):\n",
    "            continue\n",
    "            \n",
    "        #NR\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > second_event_end_time:\n",
    "            continue\n",
    "        if second_event_start_time > third_event_start_time:\n",
    "            continue\n",
    "            \n",
    "        if abs(first_event_start_time - second_event_start_time) < 0.00001:\n",
    "            print('{}: start time (first) event {} and start time of (second) event {} are the same: {}'.format(\n",
    "                sample_id, first_event, second_event, first_event_start_time))\n",
    "        if abs(second_event_start_time - third_event_start_time) < 0.00001:\n",
    "            print('{}: start time (second) event {} and start time of (third) event {} are the same: {}'.format(\n",
    "                sample_id, second_event, third_event, second_event_start_time))\n",
    "\n",
    "        #print(first_event_start_time, first_event_end_time, second_event_start_time, second_event_end_time, third_event_start_time, third_event_end_time)\n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed.\n",
    "        if (second_event_start_time - first_event_end_time) <= pre_max_delay \\\n",
    "                and (third_event_start_time - second_event_end_time) <= post_max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time,\n",
    "                'third_event_start': third_event_start_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = None\n",
    "        first_event_end_time = None\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "        third_event_start_time = None\n",
    "        third_event_end_time = None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Open single sample (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    BehaviorTransition('17-08-26L1-cl', 'bw', 'turn', 'bw', 10, 10),\n",
    "]\n",
    "\n",
    "\n",
    "# Open all samples (!see CellConfig!) >ToDo\n",
    "behavior_transitions = [\n",
    "    BehaviorTransition(name, 'bw', 'turn', 'bw', 3, 3) for name in lm_data]\n",
    "\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_between(bt.sample_id, sample_df, bt.pre_event,\n",
    "                                        bt.event, bt.post_event, bt.pre_max_delay,\n",
    "                                        bt.post_max_delay)\n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "print(len(found_transitions))\n",
    "print(found_transitions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "23 transitions from bw to turn\n",
      "17 transitions from turn to bw\n",
      "6 transition triples found\n",
      "12 transitions from bw to turn\n",
      "21 transitions from turn to bw\n",
      "1 transition triples found\n",
      "2 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "11 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "14 transitions from bw to turn\n",
      "24 transitions from turn to bw\n",
      "3 transition triples found\n",
      "5 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "14 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "4 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "9 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "4 transitions from bw to turn\n",
      "2 transitions from turn to bw\n",
      "13 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "2 transition triples found\n",
      "4 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "16 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "2 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "13 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "19 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "2 transition triples found\n",
      "18 transitions from bw to turn\n",
      "12 transitions from turn to bw\n",
      "1 transition triples found\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "6 transitions from bw to turn\n",
      "5 transitions from turn to bw\n",
      "2 transition triples found\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "10 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "4 transitions from bw to turn\n",
      "2 transitions from turn to bw\n",
      "1 transition triples found\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "11 transitions from bw to turn\n",
      "10 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "6 transitions from bw to turn\n",
      "9 transitions from turn to bw\n",
      "4 transition triples found\n",
      "13 transitions from bw to turn\n",
      "5 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "20 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "6 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "10 transitions from bw to turn\n",
      "5 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "3 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "2 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "7 transitions from bw to turn\n",
      "8 transitions from turn to bw\n",
      "6 transitions from bw to turn\n",
      "7 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "2 transitions from turn to bw\n",
      "3 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "2 transition triples found\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "4 transitions from turn to bw\n",
      "22 transitions from bw to turn\n",
      "21 transitions from turn to bw\n",
      "7 transition triples found\n",
      "10 transitions from bw to turn\n",
      "11 transitions from turn to bw\n",
      "1 transition triples found\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "0 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "4 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "1 transitions from bw to turn\n",
      "0 transitions from turn to bw\n",
      "2 transitions from bw to turn\n",
      "1 transitions from turn to bw\n",
      "12\n",
      "32\n",
      "[[{'sample_id': '17-08-23L2-cl', 'first_event_start': 71.595, 'first_event_end': 76.319, 'second_event_start': 75.955, 'second_event_end': 77.773, 'third_event_start': 78.136}, {'sample_id': '17-08-23L2-cl', 'first_event_start': 78.136, 'first_event_end': 82.86, 'second_event_start': 83.224, 'second_event_end': 85.768, 'third_event_start': 86.131}, {'sample_id': '17-08-23L2-cl', 'first_event_start': 86.131, 'first_event_end': 89.765, 'second_event_start': 90.129, 'second_event_end': 92.308, 'third_event_start': 92.672}, {'sample_id': '17-08-23L2-cl', 'first_event_start': 192.609, 'first_event_end': 196.971, 'second_event_start': 196.607, 'second_event_end': 199.514, 'third_event_start': 199.878}, {'sample_id': '17-08-23L2-cl', 'first_event_start': 822.761, 'first_event_end': 827.486, 'second_event_start': 826.759, 'second_event_end': 828.939, 'third_event_start': 829.302}, {'sample_id': '17-08-23L2-cl', 'first_event_start': 829.302, 'first_event_end': 833.663, 'second_event_start': 833.3, 'second_event_end': 836.934, 'third_event_start': 837.298}], [{'sample_id': '17-08-23L4-cl', 'first_event_start': 94.273, 'first_event_end': 97.603, 'second_event_start': 97.936, 'second_event_end': 99.935, 'third_event_start': 100.268}], [{'sample_id': '17-08-24L4-cl', 'first_event_start': 1017.817, 'first_event_end': 1021.562, 'second_event_start': 1020.814, 'second_event_end': 1023.436, 'third_event_start': 1023.81}, {'sample_id': '17-08-24L4-cl', 'first_event_start': 1043.29, 'first_event_end': 1045.912, 'second_event_start': 1045.912, 'second_event_end': 1048.16, 'third_event_start': 1048.534}, {'sample_id': '17-08-24L4-cl', 'first_event_start': 1498.44, 'first_event_end': 1501.437, 'second_event_start': 1501.812, 'second_event_end': 1503.311, 'third_event_start': 1503.685}], [{'sample_id': '17-08-27L2-cl', 'first_event_start': 360.253, 'first_event_end': 367.302, 'second_event_start': 366.189, 'second_event_end': 368.415, 'third_event_start': 368.786}, {'sample_id': '17-08-27L2-cl', 'first_event_start': 988.372, 'first_event_end': 998.018, 'second_event_start': 996.163, 'second_event_end': 998.389, 'third_event_start': 998.759}], [{'sample_id': '17-08-31L1-cl', 'first_event_start': 524.308, 'first_event_end': 530.589, 'second_event_start': 530.959, 'second_event_end': 532.437, 'third_event_start': 532.806}, {'sample_id': '17-08-31L1-cl', 'first_event_start': 2053.625, 'first_event_end': 2059.906, 'second_event_start': 2059.536, 'second_event_end': 2062.492, 'third_event_start': 2062.861}], [{'sample_id': '17-08-31L2-cl', 'first_event_start': 402.269, 'first_event_end': 405.508, 'second_event_start': 405.508, 'second_event_end': 408.026, 'third_event_start': 408.386}], [{'sample_id': '17-11-02L3-cl', 'first_event_start': 588.606, 'first_event_end': 591.882, 'second_event_start': 591.518, 'second_event_end': 594.794, 'third_event_start': 595.158}, {'sample_id': '17-11-02L3-cl', 'first_event_start': 595.158, 'first_event_end': 598.434, 'second_event_start': 598.07, 'second_event_end': 601.346, 'third_event_start': 601.71}], [{'sample_id': '17-11-03L5-cl', 'first_event_start': 495.736, 'first_event_end': 500.224, 'second_event_start': 500.632, 'second_event_end': 503.896, 'third_event_start': 503.896}], [{'sample_id': '17-11-05L6-cl', 'first_event_start': 860.508, 'first_event_end': 866.768, 'second_event_start': 866.768, 'second_event_end': 870.081, 'third_event_start': 870.45}, {'sample_id': '17-11-05L6-cl', 'first_event_start': 878.55, 'first_event_end': 882.233, 'second_event_start': 882.601, 'second_event_end': 885.178, 'third_event_start': 885.547}, {'sample_id': '17-11-05L6-cl', 'first_event_start': 1440.069, 'first_event_end': 1442.647, 'second_event_start': 1443.016, 'second_event_end': 1445.224, 'third_event_start': 1445.592}, {'sample_id': '17-11-05L6-cl', 'first_event_start': 1930.524, 'first_event_end': 1934.207, 'second_event_start': 1934.574, 'second_event_end': 1937.52, 'third_event_start': 1937.888}], [{'sample_id': '17-11-29L1-cl', 'first_event_start': 112.821, 'first_event_end': 114.76, 'second_event_start': 115.147, 'second_event_end': 116.31, 'third_event_start': 116.698}, {'sample_id': '17-11-29L1-cl', 'first_event_start': 123.288, 'first_event_end': 125.227, 'second_event_start': 125.614, 'second_event_end': 126.39, 'third_event_start': 126.778}], [{'sample_id': '17-11-29L5-cl', 'first_event_start': 15.123, 'first_event_end': 19.776, 'second_event_start': 19.0, 'second_event_end': 22.877, 'third_event_start': 23.265}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 23.265, 'first_event_end': 26.754, 'second_event_start': 26.754, 'second_event_end': 30.631, 'third_event_start': 31.018}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 31.018, 'first_event_end': 35.283, 'second_event_start': 34.895, 'second_event_end': 38.772, 'third_event_start': 39.159}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 600.146, 'first_event_end': 604.411, 'second_event_start': 604.411, 'second_event_end': 605.187, 'third_event_start': 605.574}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 622.245, 'first_event_end': 626.897, 'second_event_start': 626.897, 'second_event_end': 630.386, 'third_event_start': 630.774}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 1018.076, 'first_event_end': 1022.728, 'second_event_start': 1022.34, 'second_event_end': 1025.442, 'third_event_start': 1025.829}, {'sample_id': '17-11-29L5-cl', 'first_event_start': 1430.19, 'first_event_end': 1433.68, 'second_event_start': 1433.68, 'second_event_end': 1436.393, 'third_event_start': 1436.78}], [{'sample_id': '17-11-29L6-cl', 'first_event_start': 1324.913, 'first_event_end': 1327.6, 'second_event_start': 1327.984, 'second_event_end': 1331.054, 'third_event_start': 1331.437}]]\n"
     ]
    }
   ],
   "source": [
    "#Will\n",
    "\n",
    "###Not working\n",
    "\n",
    "# Open single samples \n",
    "'''\n",
    "first_transitions = [\n",
    "    #PostBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "    #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 3),\n",
    "    #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 3),\n",
    "    PostBehaviorTransition('17-08-26L6-cl', 'turn', 'bw', 3)\n",
    "]\n",
    "second_transitions = [\n",
    "    #PostBehaviorTransition('17-08-26L1-cl', 'turn', 'bw', 11),\n",
    "    #PostBehaviorTransition('17-08-26L2-cl', 'stim', 'fw', 3),\n",
    "    #PostBehaviorTransition('17-08-26L5-cl', 'stim', 'fw', 3),\n",
    "    PostBehaviorTransition('17-08-26L6-cl', 'bw', 'turn', 3)\n",
    "]\n",
    "'''\n",
    "# Open all samples\n",
    "first_transitions = [\n",
    "    PostBehaviorTransition(name,'bw', 'turn', 3) for name in lm_data]\n",
    "    \n",
    "second_transitions = [\n",
    "    PostBehaviorTransition(name,'turn', 'bw', 3) for name in lm_data]    \n",
    "\n",
    "found_transitions = []\n",
    "for first_bt, second_bt in zip(first_transitions, second_transitions):\n",
    "    transitions=[]\n",
    "    assert first_bt.sample_id == second_bt.sample_id, \"{} does not match {}\".format(first_bt.sample_id, second_bt.sample_id)\n",
    "    sample_df = sample_data.get(first_bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    first_transitions = find_behavior_before(first_bt.sample_id, sample_df, first_bt.event, first_bt.post_event, first_bt.max_delay)\n",
    "    second_transitions = find_behavior_before(second_bt.sample_id, sample_df, second_bt.event, second_bt.post_event, second_bt.max_delay)\n",
    "    \n",
    "    print(\"{} transitions from {} to {}\".format(len(first_transitions), first_bt.event, first_bt.post_event))\n",
    "    print(\"{} transitions from {} to {}\".format(len(second_transitions), second_bt.event, second_bt.post_event))\n",
    "    \n",
    "    for ft in first_transitions:\n",
    "        for st in second_transitions:\n",
    "            if abs(ft[\"second_event_start\"] - st[\"first_event_start\"]) < 0.00001:\n",
    "                transitions.append({\n",
    "                    \"sample_id\":ft[\"sample_id\"], \"first_event_start\":ft[\"first_event_start\"], \"first_event_end\":ft[\"first_event_end\"],\n",
    "                    \"second_event_start\": st[\"first_event_start\"], \"second_event_end\": st[\"first_event_end\"],\n",
    "                    \"third_event_start\": st[\"second_event_start\"]\n",
    "                })\n",
    "    if transitions:\n",
    "        print(\"{} transition triples found\".format(len(transitions)))\n",
    "        found_transitions.append(transitions)\n",
    "    \n",
    "\n",
    "\n",
    "print(len(found_transitions)) #number of data sets not the actual stim\n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All behavior_transitions, which were considered before assume that subsequent behaviors \n",
    "# are not the same. For same pairwise (2 behavior), we need to access also the data, when \n",
    "# the behaviors are same\n",
    "\n",
    "class SamePairBehaviorTransition:\n",
    "    \n",
    "    def __init__(self, sample_id, pre_event, event, max_delay=0):\n",
    "        self.sample_id = sample_id\n",
    "        self.pre_event = pre_event\n",
    "        self.event = event\n",
    "        self.max_delay = max_delay\n",
    "\n",
    "def find_behavior_next(sample_id, sample_df, first_event, second_event, max_delay=0):\n",
    "    \"\"\"For the data frame of a single sample <df>, find all behaviors\n",
    "    of type <first_event> that will be followed by the same event <second_event>,\n",
    "    separated by <max_delay> time. The start of <first_event> is expected\n",
    "    to happen strictly before the start of <second_event>. The end time\n",
    "    of <first_event> however can overlap with the start time of <second_event>.\n",
    "    In this case, the time difference is negative, and still smaller than\n",
    "    <max_delay>. The end time of <first_event> can be before, at or after the\n",
    "    end of <second_event>.\n",
    "    \n",
    "    If <first_event> and <second_event> are the same type of behavior,\n",
    "    overlaps have to be taken into account differently to match start and end times\n",
    "    to the correct event. During iteration for one loop, we have to exclude the \n",
    "    fact that the first_event == second_event.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"finding same behaviors only\")\n",
    "    \n",
    "    results = []\n",
    "    first_event_start_col = '{}_start'.format(first_event)\n",
    "    first_event_end_col = '{}_end'.format(first_event)\n",
    "    first_event_overlap_col = '{}_overlap'.format(first_event)\n",
    "    second_event_start_col = '{}_start'.format(second_event)\n",
    "    second_event_end_col = '{}_end'.format(second_event)\n",
    "    second_event_overlap_col = '{}_overlap'.format(second_event) \n",
    "    \n",
    "    first_event_start_time = None\n",
    "    first_event_end_time = None\n",
    "    second_event_start_time = None\n",
    "    second_event_end_time = None\n",
    "    \n",
    "    # Check for overlap between the same behaviors (print index, where 'True') and use\n",
    "    # it as a check that there is not this error in the behavior data\n",
    "    #print(sample_id, sample_df.index[sample_df['bw_overlap']].tolist())\n",
    "    \n",
    "    # Note: The overlap statement was removed. This part has to be \n",
    "    # checked if overlapping events are found in the data\n",
    "    for i, row in sample_df.iterrows():\n",
    "        # Look for start of first behavior and remember its time.\n",
    "        if row[first_event_start_col]and first_event_start_time is None:\n",
    "            first_event_start_time = row['time']\n",
    "        if row[first_event_end_col] and first_event_end_time is None: \n",
    "            first_event_end_time = row['time']\n",
    "        if row[second_event_start_col] and first_event_start_time is not None:\n",
    "            second_event_start_time = row['time']\n",
    "        if row[second_event_end_col] and first_event_end_time is not None:\n",
    "            second_event_end_time = row['time']\n",
    "        for column in sample_df.columns:\n",
    "            if (first_event_start_time is not None and\n",
    "                column.endswith(\"_start\") and\n",
    "                column != first_event_start_col and\n",
    "                column != second_event_start_col and\n",
    "                first_event not in column and\n",
    "                second_event not in column):\n",
    "                if row[column]:\n",
    "                    #print(\"{} ended at {}, but then found {} at {}\".format(first_event, first_event_end_time, column, row[\"time\"]))\n",
    "                    first_event_start_time = None\n",
    "                    first_event_end_time = None\n",
    "                    second_event_start_time = None\n",
    "                    second_event_end_time = None\n",
    "        # As long as we haven't collected all needed time points,\n",
    "        # keep on searching.\n",
    "        if None in (first_event_start_time, first_event_end_time,\n",
    "                    second_event_start_time, second_event_end_time):\n",
    "            continue\n",
    "        \n",
    "        #NR\n",
    "        if first_event_start_time == second_event_start_time:\n",
    "            continue\n",
    "        if first_event_end_time == second_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > first_event_end_time:\n",
    "            continue\n",
    "        if first_event_start_time > second_event_start_time:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Test time between first event end and second event start. If it\n",
    "        # is smaller than <max_delay>, store start of second event as result.\n",
    "        # The first event end time being greater than the second event start\n",
    "        # time, is explicitly allowed. During iteration the first_event == second_event. \n",
    "        if (second_event_start_time == first_event_end_time): #NR\n",
    "            continue\n",
    "        if (second_event_start_time - first_event_end_time) <= max_delay:\n",
    "            results.append({\n",
    "                'sample_id': sample_id,\n",
    "                'first_event_start': first_event_start_time,\n",
    "                'first_event_end': first_event_end_time,\n",
    "                'second_event_start': second_event_start_time,\n",
    "                'second_event_end': second_event_end_time\n",
    "            })\n",
    "        \n",
    "        # Reset behavior tracking variables to find new pattern match.\n",
    "        first_event_start_time = second_event_start_time\n",
    "        first_event_end_time = second_event_end_time\n",
    "        second_event_start_time = None\n",
    "        second_event_end_time = None\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "# Open single sample (!see CellConfig!) >ToDo\n",
    "\n",
    "behavior_transitions = [\n",
    "    #SamePairBehaviorTransition('17-08-26L1-cl', 'turn', 'turn', 2),\n",
    "    SamePairBehaviorTransition('17-08-31L1-cl', 'bw', 'bw', 3),\n",
    "    SamePairBehaviorTransition('17-11-06L3-cl', 'bw', 'bw', 3),\n",
    "    SamePairBehaviorTransition('17-11-29L1-cl', 'bw', 'bw', 3)\n",
    "]\n",
    "\n",
    "'''\n",
    "# Open all samples \n",
    "behavior_transitions = [\n",
    "    SamePairBehaviorTransition(name, 'bw', 'bw', 3) for name in lm_data]\n",
    "'''\n",
    "\n",
    "found_transitions = []\n",
    "for bt in behavior_transitions:\n",
    "    sample_df = sample_data.get(bt.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "    transitions = find_behavior_next(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "\n",
    "    if transitions:\n",
    "        found_transitions.append(transitions)\n",
    "\n",
    "#print(len(transitions))\n",
    "print(len(found_transitions))\n",
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))\n",
    "print(found_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([len(sample_transitions) for sample_transitions in found_transitions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#test\n",
    "def test_find_behavior_next():\n",
    "    data_columns = ['time', 'bw_start', 'bw_end', 'bw_overlap', 'fw_start', 'fw_end', 'fw_overlap', 'turn_start', 'turn_end', 'turn_overlap']\n",
    "    data = [\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [2, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [3, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [4, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [5, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [7, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [8, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [9, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [10, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [11, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [12, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [13, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [14, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    "    toy_df = pd.DataFrame(data, columns = data_columns)\n",
    "\n",
    "    behavior_transitions = [\n",
    "    SamePairBehaviorTransition('na', 'bw', 'bw', 12)]\n",
    "\n",
    "\n",
    "    found_transitions = []\n",
    "    for bt in behavior_transitions:\n",
    "        sample_df = toy_df\n",
    "        if sample_df is None:\n",
    "            raise ValueError('No data found for sample {}'.format(bt.sample_id))\n",
    "        transitions = find_behavior_next(bt.sample_id, sample_df, bt.pre_event, bt.event, bt.max_delay)\n",
    "\n",
    "        if transitions:\n",
    "            found_transitions.append(transitions)\n",
    "\n",
    "    print(len(transitions))\n",
    "    print(len(found_transitions))\n",
    "    print(found_transitions)\n",
    "    \n",
    "test_find_behavior_next()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For same_behavior_transition events get event_time and use cee_trace_config to filter by celltype and pattern.\n",
    "# The results are merged and interpolated.\n",
    "cell_Strans_configs = []\n",
    "all_Strans_events = []\n",
    "\n",
    "for sample in found_transitions:\n",
    "    sample_ls_trans = []\n",
    "    for found_transition in sample:\n",
    "        #print(found_transition[\"sample_id\"], found_transition[\"second_event_start\"])\n",
    "        #print(found_transition[\"sample_id\"])\n",
    "        sample_ls_trans.append(found_transition[\"second_event_start\"])\n",
    "        cell_Strans_configs.append(CellTransConfig(found_transition[\"sample_id\"], \"handle\", \n",
    "                                                  found_transition[\"second_event_start\"]))\n",
    "\n",
    "# Extract for specific time window and align several events. \n",
    "# Define timepoints pre and post an event (event_df). \n",
    "# This works for single sample or multiple samples aligned \n",
    "# Note: In cell_subset_df, time was set to index, because for the previous avg calculation \n",
    "# Add index and time = column\n",
    "\n",
    "# Set the window range left and right from the event\n",
    "left_half_window_size = 20.0 #in seconds\n",
    "right_half_window_size = 30.0\n",
    "\n",
    "# trans_df defined in pargraph before \n",
    "windows = []\n",
    "n_behavior = 0\n",
    "\n",
    "for ctc in cell_Strans_configs:\n",
    "    sample_df = sample_data.get(ctc.sample_id)\n",
    "    if sample_df is None:\n",
    "        raise ValueError('{}: could not find sample data'.format(ctc.sample_id))\n",
    "        continue    \n",
    "    # Extract columns matching our cell type and the optional filter pattern.\n",
    "    # Pandas' filter() operations works on columns for DataFrames by default.\n",
    "    cell_subset_df = sample_df.filter(regex=ctc.get_filter_regex()) #Get subset of cells \n",
    "    cell_subset_df.set_index(sample_df.time, inplace=True) #Set time to index (essential for min/max...)\n",
    "    cell_subset_df.reset_index(inplace = True) # Add index and time = column\n",
    "    #print(cell_subset_df)\n",
    "    \n",
    "    n_behavior += 1\n",
    "    window_start = ctc.event_time - left_half_window_size\n",
    "    window_end = ctc.event_time + right_half_window_size\n",
    "        \n",
    "    # Get subset of rows between window_start and window_end       \n",
    "    trans = cell_subset_df[(cell_subset_df.time >= window_start) & (cell_subset_df.time <= window_end)]\n",
    "    #print(trans) #ok\n",
    "    # Normalizing the data to align on beginning of selected\n",
    "    # behavior (event_df = Zero) by substracting events in window\n",
    "    # around start of event of interest from start of event interest.\n",
    "    # Note: using \":\" in event.loc[] will select \"all rows\" in our window.\n",
    "    #trans.loc[:, 'time'] = trans['time'] - row['time']\n",
    "    trans.loc[:, 'time'] = trans['time'] - ctc.event_time\n",
    "    #print(trans) #ok\n",
    "    # Add sample_id to each column as prefix and n_behavior as suffix to distinguish events within a sample\n",
    "    #trans.rename(lambda x: '{}_{}_{}'.format(ctc.sample_id, x, n_behavior), axis = 'columns', inplace = True) \n",
    "\n",
    "    # Rename time collum to time\n",
    "    trans.rename(columns={ trans.columns[0]: 'time' }, inplace = True) \n",
    "    #print(trans) # ok\n",
    "    all_Strans_events.append(trans) # Append a list with all event\n",
    "#print(all_trans_events)   \n",
    "        \n",
    "        \n",
    "# Removes first event and takes it as left_window in pd.merge_ordered and iterates than through all_events\n",
    "all_trans_df = all_Strans_events.pop(0)\n",
    "for right_df in all_Strans_events:\n",
    "    all_trans_df = pd.merge_ordered(all_trans_df, right_df, on=\"time\", how=\"outer\")\n",
    "\n",
    "# Resets the index as time and drops time column (sollte spaeter kommen)\n",
    "all_trans_df.index = all_trans_df[\"time\"]\n",
    "del all_trans_df[\"time\"]        \n",
    "#print(all_trans_df)\n",
    "\n",
    "# Index intepolation (linear interpolatione not on all_df, because index [=time] is not eaqually distributed)\n",
    "int_all_Strans_df = all_trans_df.interpolate(method='index', axis=0, limit=None, inplace=False, limit_direction='both')\n",
    "#print(int_all_Strans_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and stddev, min, max, sem for same_behavior_transition events\n",
    "all_Strans_avg_df = int_all_Strans_df.mean(axis=1) # Interpolated data used\n",
    "all_Strans_min_df = int_all_Strans_df.min(axis=1)\n",
    "all_Strans_max_df = int_all_Strans_df.max(axis=1)\n",
    "# Standard deviation (distribution)\n",
    "all_Strans_std_df = int_all_Strans_df.std(axis = 1)\n",
    "#standard error of mean\n",
    "all_Strans_sem_df = int_all_Strans_df.sem(axis = 1)\n",
    "#wrong zur haelfte: Want to have avg per celltyp over time point, \n",
    "#and not avg over all cells per timepoint\n",
    "\n",
    "# Plotting for multi-events (same_behavioral_transition)\n",
    "# If a dataframe with NANs is plotted (raw-data = non interpolated), use \n",
    "# marker = '+', or 'o', since the line in the lineplot only connects \n",
    "# consecutive data points\n",
    "def aligned_layout_plot(plot, tick_spacing=1, fov=(-10, 10, 0.1, 0.4), legend=False): \n",
    "    # Set fine x-axis scale\n",
    "    plot.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "    # Set x and y limits and legend (default = False) \n",
    "    plot.axis(fov)\n",
    "    plot.legend().set_visible(legend)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Plot all cells from all_df, aligned at zero for event_start, specified in Cell_Trace_Config.\n",
    "#sub1 = fig.add_subplot(111) #211\n",
    "#all_trans_df.plot(ax=sub1, marker = '*', label = ctc.cell_type)\n",
    "#aligned_layout_plot(sub1)\n",
    "\n",
    "sub2 = fig.add_subplot(111) #212\n",
    "all_Strans_avg_df.plot(ax=sub2, color = 'g', label = ctc.cell_type) #use interpolated df to calculate average...\n",
    "#all_Strans_min_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Strans_max_df.plot(ax=sub2, color = 'r', linewidth=1, alpha = 0.5)\n",
    "#all_Strans_avg_df.plot.line(yerr=all_Strans_std_df, ax=sub2, color = 'r', alpha = 0.1)\n",
    "all_Strans_avg_df.plot.line(yerr=all_Strans_sem_df, ax=sub2, color = 'grey', alpha = 0.1)\n",
    "aligned_layout_plot(sub2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (using merged_ordered + interpolated) from behavioral transitions\n",
    "# HERE: Not aneraged!! --> int_all_Strans_df (aligned to sec_event_start)\n",
    "\n",
    "# Before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(event_df)) #prints header\n",
    "#rounded time is only visual, I still get several 'same' tp\n",
    "#sample_df.round({'time' : 1})\n",
    "   # Round time on 1 or 2nd decimal\n",
    "    # the df.round({'time' : 1}) doesn't work if to many decimals\n",
    "    #decimals = 2    \n",
    "    #timestamp_df['time'] = timestamp_df['time'].apply(lambda x: round(x, decimals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'A' : [3.2, np.nan, np.nan, 5, np.nan, np.nan],\n",
    "     'B' : [np.nan, 4.1, np.nan, np.nan, 6.2, np.nan], \n",
    "     'C' : [np.nan, np.nan, 1.1, np.nan, np.nan, 2.5]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "\n",
    "int_df_linear = df.interpolate(method = 'linear', inplace = False)\n",
    "print(int_df_linear)\n",
    "\n",
    "int_df_index = df.interpolate(method='index', inplace=False)\n",
    "print(int_df_index)\n",
    "#u = df['T'].unique\n",
    "\n",
    "# Resets the index as time and drops time column\n",
    "#df.index = df[\"T\"]\n",
    "#del df[\"T\"] \n",
    "\n",
    "#df.fillna(value=None, method='ffill', axis=None, inplace=False, limit=None, downcast=None) # not what I want\n",
    "#df.interpolate(method='linear', axis=0, limit=None,\n",
    "#                      inplace=False, limit_direction='forward', limit_area=None, downcast=None) \n",
    "\n",
    "\n",
    "# print out only rows where in'T' are duplicates\n",
    "#df[df.duplicated(subset= ['T'], keep=False)]\n",
    "\n",
    "    \n",
    "    \n",
    "#np.nanmean(j, axis=0)\n",
    "#    for j in df.loc[df[\"T\"]== value]:\n",
    "#        np.nanmean(j, axis=0)\n",
    "\n",
    "\n",
    "#df.mean(axis=1, skipna=None)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {'T' : [1,2, 3, 4, 5, 6, 7, 8], 'A' : [3.2, 5, 5.5, 5.3, 9, 8, 8, 3],\n",
    "     'B' : [4.1, 6.2, 6.0, 6.2, 8, 1, 1.5, 3.7], \n",
    "     'C' : [1.1, 2.5, 2.3, 1.2, 0.9, 1.1, 1.8, 1.7]}\n",
    "df1 = pd.DataFrame(data=d)\n",
    "df1\n",
    "\n",
    "d = {'A' : [3.2, 5, 5.5, 5.3, 9, 8, 8, 3],\n",
    "     'B' : [4.1, 6.2, 6.0, 6.2, 8, 1, 1.5, 3.7], \n",
    "     'C' : [1.1, 2.5, 2.3, 1.2, 0.9, 1.1, 1.8, 1.7]}\n",
    "df2 = pd.DataFrame(data=d)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation \n",
    "# inplace  = False, since we want to keep the data sets with raw data as well\n",
    "\n",
    "# Linear Interpolation: According to documentation, \n",
    "# because it assums the index is equally spaced.\n",
    "# ‘Index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "int_df2 = df2.interpolate(method = 'index', inplace = False)\n",
    "print(int_df2)\n",
    "\n",
    "#Note: First 5 values = NAN!!??!! (Method?)\n",
    "\n",
    "#intpol_all_df = all_df.interpolate(method='index', inplace=False)\n",
    "#print(intpol_all_df)\n",
    "ls = [2, 1.2, 3, 8.2]\n",
    "#for i in ls:\n",
    "    #print(i)\n",
    "a =int_df2.loc[int_df2['C'].isin(ls)]\n",
    "print(a)\n",
    "# find row where C = 1.2 #isin\n",
    "#a =int_df2.loc[df['C'].isin(1.2)]\n",
    "#a =int_df2.loc[int_df2['C'] == 1.2]\n",
    "\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data anlysis - TODO\n",
    "# Ask Ann if this kind of interpolation made sens\n",
    "\n",
    "# Dataprocessing\n",
    "# For the next step, we try two methods two normalize the data and get the timestamps \n",
    "# in synchrony between the different samples/events (1.Interpolation of some kind, 2. Binning,\n",
    "# 3) fitting curve)\n",
    "               \n",
    "# Interpolation \n",
    "# inplace  = False, since we want to keep the data sets with raw data as well\n",
    "\n",
    "# Linear Interpolation: According to documentation it is not correct to use, \n",
    "# because it assums the index is equally spaced.\n",
    "\n",
    "# ‘Index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "#Note: First 5 values = NAN!!??!!\n",
    "\n",
    "#intpol_all_df = all_df.interpolate(method='index', inplace=False)\n",
    "#print(intpol_all_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
